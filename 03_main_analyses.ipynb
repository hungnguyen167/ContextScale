{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Data Splits for Ensemble Training\n",
    "\n",
    "This notebook has been updated to create five different train/eval/test splits from the original dataset for ensemble training, ensuring better diversity and more robust uncertainty estimation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4k3dCezyXmXQ"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dPi54JU9XmXR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from utils.functions import group_texts, sentiment_code, topic_code,party_deu, clean_text_loop, copy_weights, sentiment_code_coalition, topic_code_coalition\n",
    "from utils.functions import train_loop, eval_loop, tokenize_function, cmp_scale, scale_func, d2v_reduct, check_weights_similar, compare_architectures, get_architecture_details, recode_tw\n",
    "from utils.models import ContextScalePrediction, corpusIterator, phraseIterator\n",
    "from safetensors.torch import load_file, save_file\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import nltk\n",
    "#nltk.download('stopwords') ## Remove comments and do it once if you haven't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mDclkfIWZcwv",
    "outputId": "769a0b89-1e46-4b79-c424-cb6022cd4971"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX PRO 6000 Blackwell Workstation Edition'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "device = torch.device('cuda')\n",
    "torch.cuda.get_device_name(device=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULX0gK7mg6d3",
    "outputId": "bdbd3c77-e5c0-46f3-cb41-c78bdbbbd4ab"
   },
   "outputs": [],
   "source": [
    "## Pseudo-randomness for reproducibility\n",
    "seed_val = 1234\n",
    "torch.manual_seed(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small test of BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = 'I went to the river bank'\n",
    "sentence_b = 'I went to the bank by the river'\n",
    "tok_a = tokenizer(sentence_a, return_tensors='pt')\n",
    "tok_b = tokenizer(sentence_b, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  146, 1355, 1106, 1103, 2186, 3085,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  146, 1355, 1106, 1103, 3085, 1118, 1103, 2186,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs_a = model(**tok_a)\n",
    "    outputs_b = model(**tok_b)\n",
    "# Extract word embeddings from the last hidden layer\n",
    "last_hidden_states_a = outputs_a.last_hidden_state\n",
    "last_hidden_states_b = outputs_b.last_hidden_state\n",
    "\n",
    "# Extract the word embedding for the first token (CLS token)\n",
    "word_embedding_a = last_hidden_states_a[:, -2, :] ## 0 is the CLS token, river is the last token\n",
    "word_embedding_b = last_hidden_states_b[:, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.64],\n",
       "       [0.64, 1.  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(word_embedding_a.numpy(), word_embedding_b.numpy()).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "b41aechYXmXT"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qB4dt7BNuojT"
   },
   "outputs": [],
   "source": [
    "manifesto = pd.read_csv(os.path.join(\"data\", \"r_outputs\",\"pulled_manifestoes.csv\"), encoding=\"utf-8\", dtype = {2: 'str', 18:'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto = manifesto[(manifesto.cmp_code.notna()) & ~(manifesto.cmp_code.isin(['H']))].reset_index(drop=True)\n",
    "len(manifesto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto['sentiment'] = manifesto['cmp_code'].apply(sentiment_code)\n",
    "manifesto['topic'] = manifesto['cmp_code'].apply(topic_code)\n",
    "manifesto['election'] = manifesto['date'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>eu_code</th>\n",
       "      <th>pos</th>\n",
       "      <th>manifesto_id</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>translation_en</th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>party_code</th>\n",
       "      <th>countryname</th>\n",
       "      <th>abbrev</th>\n",
       "      <th>name</th>\n",
       "      <th>edate</th>\n",
       "      <th>parfam</th>\n",
       "      <th>election</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Agriculture - Protectionism</th>\n",
       "      <th>left</th>\n",
       "      <td>19310</td>\n",
       "      <td>18296</td>\n",
       "      <td>19310</td>\n",
       "      <td>193</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>...</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>17900</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "      <td>19310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2181</td>\n",
       "      <td>2030</td>\n",
       "      <td>2181</td>\n",
       "      <td>9</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>...</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2048</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "      <td>2181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Economics</th>\n",
       "      <th>left</th>\n",
       "      <td>52420</td>\n",
       "      <td>50090</td>\n",
       "      <td>52420</td>\n",
       "      <td>80</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>...</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>48432</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "      <td>52420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>75141</td>\n",
       "      <td>71348</td>\n",
       "      <td>75141</td>\n",
       "      <td>103</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>...</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>71048</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "      <td>75141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>37940</td>\n",
       "      <td>36785</td>\n",
       "      <td>37940</td>\n",
       "      <td>62</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>...</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>36126</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "      <td>37940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Education</th>\n",
       "      <th>left</th>\n",
       "      <td>45505</td>\n",
       "      <td>44167</td>\n",
       "      <td>45505</td>\n",
       "      <td>10</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>...</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>42979</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "      <td>45505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>638</td>\n",
       "      <td>625</td>\n",
       "      <td>638</td>\n",
       "      <td>0</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>...</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>558</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Environment - Growth</th>\n",
       "      <th>left</th>\n",
       "      <td>64336</td>\n",
       "      <td>62320</td>\n",
       "      <td>64336</td>\n",
       "      <td>78</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>...</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>61034</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "      <td>64336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>20756</td>\n",
       "      <td>19608</td>\n",
       "      <td>20756</td>\n",
       "      <td>0</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>...</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>19117</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "      <td>20756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>10884</td>\n",
       "      <td>10336</td>\n",
       "      <td>10884</td>\n",
       "      <td>37</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>...</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10393</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "      <td>10884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">European Integration</th>\n",
       "      <th>left</th>\n",
       "      <td>14003</td>\n",
       "      <td>13783</td>\n",
       "      <td>14003</td>\n",
       "      <td>1</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>...</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>13569</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "      <td>14003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>6090</td>\n",
       "      <td>5968</td>\n",
       "      <td>6090</td>\n",
       "      <td>1</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>...</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>5921</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "      <td>6090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Fabrics of Society</th>\n",
       "      <th>left</th>\n",
       "      <td>36107</td>\n",
       "      <td>34285</td>\n",
       "      <td>36107</td>\n",
       "      <td>11</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>...</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>33565</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "      <td>36107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>18085</td>\n",
       "      <td>17796</td>\n",
       "      <td>18085</td>\n",
       "      <td>19</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>...</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>17532</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "      <td>18085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>52426</td>\n",
       "      <td>52025</td>\n",
       "      <td>52426</td>\n",
       "      <td>57</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>...</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>50476</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "      <td>52426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Immigration</th>\n",
       "      <th>left</th>\n",
       "      <td>12267</td>\n",
       "      <td>11180</td>\n",
       "      <td>12267</td>\n",
       "      <td>28</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>...</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>11760</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "      <td>12267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>17979</td>\n",
       "      <td>17888</td>\n",
       "      <td>17979</td>\n",
       "      <td>9</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>...</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>16947</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "      <td>17979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">International Relations</th>\n",
       "      <th>left</th>\n",
       "      <td>24660</td>\n",
       "      <td>24066</td>\n",
       "      <td>24660</td>\n",
       "      <td>18</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>...</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>23614</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "      <td>24660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>3033</td>\n",
       "      <td>2901</td>\n",
       "      <td>3033</td>\n",
       "      <td>6</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>...</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>2878</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2190</td>\n",
       "      <td>2169</td>\n",
       "      <td>2190</td>\n",
       "      <td>2</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>...</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2059</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Labour and Social Welfare</th>\n",
       "      <th>left</th>\n",
       "      <td>184420</td>\n",
       "      <td>177051</td>\n",
       "      <td>184420</td>\n",
       "      <td>63</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>...</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>172763</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "      <td>184420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>4159</td>\n",
       "      <td>4070</td>\n",
       "      <td>4159</td>\n",
       "      <td>2</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>...</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4074</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "      <td>4159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>6464</td>\n",
       "      <td>6432</td>\n",
       "      <td>6464</td>\n",
       "      <td>1</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>...</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6192</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "      <td>6464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Military</th>\n",
       "      <th>left</th>\n",
       "      <td>4697</td>\n",
       "      <td>4442</td>\n",
       "      <td>4697</td>\n",
       "      <td>9</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>...</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4460</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "      <td>4697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>10369</td>\n",
       "      <td>10309</td>\n",
       "      <td>10369</td>\n",
       "      <td>20</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>...</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>9951</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "      <td>10369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>neutral</th>\n",
       "      <td>75159</td>\n",
       "      <td>71899</td>\n",
       "      <td>75159</td>\n",
       "      <td>48</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>...</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>69565</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "      <td>75159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Political System</th>\n",
       "      <th>left</th>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>0</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>...</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2124</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "      <td>2191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>44193</td>\n",
       "      <td>42953</td>\n",
       "      <td>44193</td>\n",
       "      <td>30</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>...</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>41493</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "      <td>44193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>35720</td>\n",
       "      <td>31305</td>\n",
       "      <td>35720</td>\n",
       "      <td>264</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>...</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>34236</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "      <td>35720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text  text_en  cmp_code  eu_code  \\\n",
       "topic                       sentiment                                       \n",
       "Agriculture - Protectionism left        19310    18296     19310      193   \n",
       "                            right        2181     2030      2181        9   \n",
       "Economics                   left        52420    50090     52420       80   \n",
       "                            neutral     75141    71348     75141      103   \n",
       "                            right       37940    36785     37940       62   \n",
       "Education                   left        45505    44167     45505       10   \n",
       "                            right         638      625       638        0   \n",
       "Environment - Growth        left        64336    62320     64336       78   \n",
       "                            neutral     20756    19608     20756        0   \n",
       "                            right       10884    10336     10884       37   \n",
       "European Integration        left        14003    13783     14003        1   \n",
       "                            right        6090     5968      6090        1   \n",
       "Fabrics of Society          left        36107    34285     36107       11   \n",
       "                            neutral     18085    17796     18085       19   \n",
       "                            right       52426    52025     52426       57   \n",
       "Immigration                 left        12267    11180     12267       28   \n",
       "                            right       17979    17888     17979        9   \n",
       "International Relations     left        24660    24066     24660       18   \n",
       "                            neutral      3033     2901      3033        6   \n",
       "                            right        2190     2169      2190        2   \n",
       "Labour and Social Welfare   left       184420   177051    184420       63   \n",
       "                            neutral      4159     4070      4159        2   \n",
       "                            right        6464     6432      6464        1   \n",
       "Military                    left         4697     4442      4697        9   \n",
       "                            right       10369    10309     10369       20   \n",
       "Other                       neutral     75159    71899     75159       48   \n",
       "Political System            left         2191     2191      2191        0   \n",
       "                            neutral     44193    42953     44193       30   \n",
       "                            right       35720    31305     35720      264   \n",
       "\n",
       "                                          pos  manifesto_id   party    date  \\\n",
       "topic                       sentiment                                         \n",
       "Agriculture - Protectionism left        19310         19310   19310   19310   \n",
       "                            right        2181          2181    2181    2181   \n",
       "Economics                   left        52420         52420   52420   52420   \n",
       "                            neutral     75141         75141   75141   75141   \n",
       "                            right       37940         37940   37940   37940   \n",
       "Education                   left        45505         45505   45505   45505   \n",
       "                            right         638           638     638     638   \n",
       "Environment - Growth        left        64336         64336   64336   64336   \n",
       "                            neutral     20756         20756   20756   20756   \n",
       "                            right       10884         10884   10884   10884   \n",
       "European Integration        left        14003         14003   14003   14003   \n",
       "                            right        6090          6090    6090    6090   \n",
       "Fabrics of Society          left        36107         36107   36107   36107   \n",
       "                            neutral     18085         18085   18085   18085   \n",
       "                            right       52426         52426   52426   52426   \n",
       "Immigration                 left        12267         12267   12267   12267   \n",
       "                            right       17979         17979   17979   17979   \n",
       "International Relations     left        24660         24660   24660   24660   \n",
       "                            neutral      3033          3033    3033    3033   \n",
       "                            right        2190          2190    2190    2190   \n",
       "Labour and Social Welfare   left       184420        184420  184420  184420   \n",
       "                            neutral      4159          4159    4159    4159   \n",
       "                            right        6464          6464    6464    6464   \n",
       "Military                    left         4697          4697    4697    4697   \n",
       "                            right       10369         10369   10369   10369   \n",
       "Other                       neutral     75159         75159   75159   75159   \n",
       "Political System            left         2191          2191    2191    2191   \n",
       "                            neutral     44193         44193   44193   44193   \n",
       "                            right       35720         35720   35720   35720   \n",
       "\n",
       "                                       language  source  ...  translation_en  \\\n",
       "topic                       sentiment                    ...                   \n",
       "Agriculture - Protectionism left          19310   19310  ...           19310   \n",
       "                            right          2181    2181  ...            2181   \n",
       "Economics                   left          52420   52420  ...           52420   \n",
       "                            neutral       75141   75141  ...           75141   \n",
       "                            right         37940   37940  ...           37940   \n",
       "Education                   left          45505   45505  ...           45505   \n",
       "                            right           638     638  ...             638   \n",
       "Environment - Growth        left          64336   64336  ...           64336   \n",
       "                            neutral       20756   20756  ...           20756   \n",
       "                            right         10884   10884  ...           10884   \n",
       "European Integration        left          14003   14003  ...           14003   \n",
       "                            right          6090    6090  ...            6090   \n",
       "Fabrics of Society          left          36107   36107  ...           36107   \n",
       "                            neutral       18085   18085  ...           18085   \n",
       "                            right         52426   52426  ...           52426   \n",
       "Immigration                 left          12267   12267  ...           12267   \n",
       "                            right         17979   17979  ...           17979   \n",
       "International Relations     left          24660   24660  ...           24660   \n",
       "                            neutral        3033    3033  ...            3033   \n",
       "                            right          2190    2190  ...            2190   \n",
       "Labour and Social Welfare   left         184420  184420  ...          184420   \n",
       "                            neutral        4159    4159  ...            4159   \n",
       "                            right          6464    6464  ...            6464   \n",
       "Military                    left           4697    4697  ...            4697   \n",
       "                            right         10369   10369  ...           10369   \n",
       "Other                       neutral       75159   75159  ...           75159   \n",
       "Political System            left           2191    2191  ...            2191   \n",
       "                            neutral       44193   44193  ...           44193   \n",
       "                            right         35720   35720  ...           35720   \n",
       "\n",
       "                                           id  country  party_code  \\\n",
       "topic                       sentiment                                \n",
       "Agriculture - Protectionism left        19310    19310       19310   \n",
       "                            right        2181     2181        2181   \n",
       "Economics                   left        52420    52420       52420   \n",
       "                            neutral     75141    75141       75141   \n",
       "                            right       37940    37940       37940   \n",
       "Education                   left        45505    45505       45505   \n",
       "                            right         638      638         638   \n",
       "Environment - Growth        left        64336    64336       64336   \n",
       "                            neutral     20756    20756       20756   \n",
       "                            right       10884    10884       10884   \n",
       "European Integration        left        14003    14003       14003   \n",
       "                            right        6090     6090        6090   \n",
       "Fabrics of Society          left        36107    36107       36107   \n",
       "                            neutral     18085    18085       18085   \n",
       "                            right       52426    52426       52426   \n",
       "Immigration                 left        12267    12267       12267   \n",
       "                            right       17979    17979       17979   \n",
       "International Relations     left        24660    24660       24660   \n",
       "                            neutral      3033     3033        3033   \n",
       "                            right        2190     2190        2190   \n",
       "Labour and Social Welfare   left       184420   184420      184420   \n",
       "                            neutral      4159     4159        4159   \n",
       "                            right        6464     6464        6464   \n",
       "Military                    left         4697     4697        4697   \n",
       "                            right       10369    10369       10369   \n",
       "Other                       neutral     75159    75159       75159   \n",
       "Political System            left         2191     2191        2191   \n",
       "                            neutral     44193    44193       44193   \n",
       "                            right       35720    35720       35720   \n",
       "\n",
       "                                       countryname  abbrev    name   edate  \\\n",
       "topic                       sentiment                                        \n",
       "Agriculture - Protectionism left             19310   17900   19310   19310   \n",
       "                            right             2181    2048    2181    2181   \n",
       "Economics                   left             52420   48432   52420   52420   \n",
       "                            neutral          75141   71048   75141   75141   \n",
       "                            right            37940   36126   37940   37940   \n",
       "Education                   left             45505   42979   45505   45505   \n",
       "                            right              638     558     638     638   \n",
       "Environment - Growth        left             64336   61034   64336   64336   \n",
       "                            neutral          20756   19117   20756   20756   \n",
       "                            right            10884   10393   10884   10884   \n",
       "European Integration        left             14003   13569   14003   14003   \n",
       "                            right             6090    5921    6090    6090   \n",
       "Fabrics of Society          left             36107   33565   36107   36107   \n",
       "                            neutral          18085   17532   18085   18085   \n",
       "                            right            52426   50476   52426   52426   \n",
       "Immigration                 left             12267   11760   12267   12267   \n",
       "                            right            17979   16947   17979   17979   \n",
       "International Relations     left             24660   23614   24660   24660   \n",
       "                            neutral           3033    2878    3033    3033   \n",
       "                            right             2190    2059    2190    2190   \n",
       "Labour and Social Welfare   left            184420  172763  184420  184420   \n",
       "                            neutral           4159    4074    4159    4159   \n",
       "                            right             6464    6192    6464    6464   \n",
       "Military                    left              4697    4460    4697    4697   \n",
       "                            right            10369    9951   10369   10369   \n",
       "Other                       neutral          75159   69565   75159   75159   \n",
       "Political System            left              2191    2124    2191    2191   \n",
       "                            neutral          44193   41493   44193   44193   \n",
       "                            right            35720   34236   35720   35720   \n",
       "\n",
       "                                       parfam  election  \n",
       "topic                       sentiment                    \n",
       "Agriculture - Protectionism left        19310     19310  \n",
       "                            right        2181      2181  \n",
       "Economics                   left        52420     52420  \n",
       "                            neutral     75141     75141  \n",
       "                            right       37940     37940  \n",
       "Education                   left        45505     45505  \n",
       "                            right         638       638  \n",
       "Environment - Growth        left        64336     64336  \n",
       "                            neutral     20756     20756  \n",
       "                            right       10884     10884  \n",
       "European Integration        left        14003     14003  \n",
       "                            right        6090      6090  \n",
       "Fabrics of Society          left        36107     36107  \n",
       "                            neutral     18085     18085  \n",
       "                            right       52426     52426  \n",
       "Immigration                 left        12267     12267  \n",
       "                            right       17979     17979  \n",
       "International Relations     left        24660     24660  \n",
       "                            neutral      3033      3033  \n",
       "                            right        2190      2190  \n",
       "Labour and Social Welfare   left       184420    184420  \n",
       "                            neutral      4159      4159  \n",
       "                            right        6464      6464  \n",
       "Military                    left         4697      4697  \n",
       "                            right       10369     10369  \n",
       "Other                       neutral     75159     75159  \n",
       "Political System            left         2191      2191  \n",
       "                            neutral     44193     44193  \n",
       "                            right       35720     35720  \n",
       "\n",
       "[29 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto.groupby(['topic','sentiment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_result = manifesto.groupby(['topic', 'sentiment', 'cmp_code']).size().reset_index(name='count')\n",
    "grouped_result.to_csv('data/temps/categorization_table.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>eu_code</th>\n",
       "      <th>pos</th>\n",
       "      <th>manifesto_id</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>party_code</th>\n",
       "      <th>countryname</th>\n",
       "      <th>abbrev</th>\n",
       "      <th>name</th>\n",
       "      <th>edate</th>\n",
       "      <th>parfam</th>\n",
       "      <th>topic</th>\n",
       "      <th>election</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>459916</td>\n",
       "      <td>441871</td>\n",
       "      <td>459916</td>\n",
       "      <td>491</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>...</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>432200</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "      <td>459916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>240526</td>\n",
       "      <td>230575</td>\n",
       "      <td>240526</td>\n",
       "      <td>208</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>...</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>225707</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "      <td>240526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>182881</td>\n",
       "      <td>175872</td>\n",
       "      <td>182881</td>\n",
       "      <td>462</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>...</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>174907</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "      <td>182881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             text  text_en  cmp_code  eu_code     pos  manifesto_id   party  \\\n",
       "sentiment                                                                     \n",
       "left       459916   441871    459916      491  459916        459916  459916   \n",
       "neutral    240526   230575    240526      208  240526        240526  240526   \n",
       "right      182881   175872    182881      462  182881        182881  182881   \n",
       "\n",
       "             date  language  source  ...      id  country  party_code  \\\n",
       "sentiment                            ...                                \n",
       "left       459916    459916  459916  ...  459916   459916      459916   \n",
       "neutral    240526    240526  240526  ...  240526   240526      240526   \n",
       "right      182881    182881  182881  ...  182881   182881      182881   \n",
       "\n",
       "           countryname  abbrev    name   edate  parfam   topic  election  \n",
       "sentiment                                                                 \n",
       "left            459916  432200  459916  459916  459916  459916    459916  \n",
       "neutral         240526  225707  240526  240526  240526  240526    240526  \n",
       "right           182881  174907  182881  182881  182881  182881    182881  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7056048580190937"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manifesto[manifesto.topic==\"Military\"])/len(manifesto)*100 ## minority group: 1.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = manifesto['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length (word) is: 18.07444502180969\n",
      "Std length (word) is: 11.17055383287234\n",
      "Min length (word) is: 1\n",
      "Max length (word) is: 269\n"
     ]
    }
   ],
   "source": [
    "from statistics import stdev, mean\n",
    "## Before\n",
    "seq_len = [len(i.split()) for i in texts]\n",
    "seq_len_mean = mean(seq_len)\n",
    "seq_len_std = stdev(seq_len)\n",
    "seq_len_max = max(seq_len)\n",
    "seq_len_min = min(seq_len)\n",
    "print('Mean length (word) is: {}'.format(seq_len_mean))\n",
    "print('Std length (word) is: {}'.format(seq_len_std))\n",
    "print('Min length (word) is: {}'.format(seq_len_min))\n",
    "print('Max length (word) is: {}'.format(seq_len_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = group_texts(manifesto, \n",
    "                      ['countryname','election','party','cmp_code'], 'text', \n",
    "                      max_group_factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped = pd.DataFrame(results)\n",
    "manifesto_regrouped = manifesto_regrouped.explode('text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = manifesto_regrouped['labels'].str.split(';', expand=True)\n",
    "manifesto_regrouped = pd.concat([manifesto_regrouped, df_cols], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.columns = ['text', 'idx', 'country','election', 'party', 'cmp_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>idx</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statt dessen soll ein Freiwilligen-Milizheer g...</td>\n",
       "      <td>Austria;1999;42110;104</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weg von der Sicherheit durch RÃ¼stung, Unsere S...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Militarisierung der EU bringt mehr RÃ¼stung...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                     idx  \\\n",
       "0  Statt dessen soll ein Freiwilligen-Milizheer g...  Austria;1999;42110;104   \n",
       "1  Weg von der Sicherheit durch RÃ¼stung, Unsere S...  Austria;1999;42110;105   \n",
       "2  Die Militarisierung der EU bringt mehr RÃ¼stung...  Austria;1999;42110;105   \n",
       "3  Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...  Austria;1999;42110;105   \n",
       "4  Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...  Austria;1999;42110;105   \n",
       "\n",
       "   country election  party cmp_code  \n",
       "0  Austria     1999  42110      104  \n",
       "1  Austria     1999  42110      105  \n",
       "2  Austria     1999  42110      105  \n",
       "3  Austria     1999  42110      105  \n",
       "4  Austria     1999  42110      105  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.loc[:,'sentiment'] = manifesto_regrouped['cmp_code'].apply(sentiment_code)\n",
    "manifesto_regrouped.loc[:,'topic'] = manifesto_regrouped['cmp_code'].apply(topic_code)\n",
    "manifesto_regrouped = manifesto_regrouped.drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>idx</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Agriculture - Protectionism</th>\n",
       "      <th>left</th>\n",
       "      <td>7381</td>\n",
       "      <td>7381</td>\n",
       "      <td>7381</td>\n",
       "      <td>7381</td>\n",
       "      <td>7381</td>\n",
       "      <td>7381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>935</td>\n",
       "      <td>935</td>\n",
       "      <td>935</td>\n",
       "      <td>935</td>\n",
       "      <td>935</td>\n",
       "      <td>935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Economics</th>\n",
       "      <th>left</th>\n",
       "      <td>20294</td>\n",
       "      <td>20294</td>\n",
       "      <td>20294</td>\n",
       "      <td>20294</td>\n",
       "      <td>20294</td>\n",
       "      <td>20294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>29459</td>\n",
       "      <td>29459</td>\n",
       "      <td>29459</td>\n",
       "      <td>29459</td>\n",
       "      <td>29459</td>\n",
       "      <td>29459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>14528</td>\n",
       "      <td>14528</td>\n",
       "      <td>14528</td>\n",
       "      <td>14528</td>\n",
       "      <td>14528</td>\n",
       "      <td>14528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Education</th>\n",
       "      <th>left</th>\n",
       "      <td>17314</td>\n",
       "      <td>17314</td>\n",
       "      <td>17314</td>\n",
       "      <td>17314</td>\n",
       "      <td>17314</td>\n",
       "      <td>17314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Environment - Growth</th>\n",
       "      <th>left</th>\n",
       "      <td>24621</td>\n",
       "      <td>24621</td>\n",
       "      <td>24621</td>\n",
       "      <td>24621</td>\n",
       "      <td>24621</td>\n",
       "      <td>24621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>7635</td>\n",
       "      <td>7635</td>\n",
       "      <td>7635</td>\n",
       "      <td>7635</td>\n",
       "      <td>7635</td>\n",
       "      <td>7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>4203</td>\n",
       "      <td>4203</td>\n",
       "      <td>4203</td>\n",
       "      <td>4203</td>\n",
       "      <td>4203</td>\n",
       "      <td>4203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">European Integration</th>\n",
       "      <th>left</th>\n",
       "      <td>5481</td>\n",
       "      <td>5481</td>\n",
       "      <td>5481</td>\n",
       "      <td>5481</td>\n",
       "      <td>5481</td>\n",
       "      <td>5481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Fabrics of Society</th>\n",
       "      <th>left</th>\n",
       "      <td>14002</td>\n",
       "      <td>14002</td>\n",
       "      <td>14002</td>\n",
       "      <td>14002</td>\n",
       "      <td>14002</td>\n",
       "      <td>14002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>7041</td>\n",
       "      <td>7041</td>\n",
       "      <td>7041</td>\n",
       "      <td>7041</td>\n",
       "      <td>7041</td>\n",
       "      <td>7041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>19632</td>\n",
       "      <td>19632</td>\n",
       "      <td>19632</td>\n",
       "      <td>19632</td>\n",
       "      <td>19632</td>\n",
       "      <td>19632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Immigration</th>\n",
       "      <th>left</th>\n",
       "      <td>4817</td>\n",
       "      <td>4817</td>\n",
       "      <td>4817</td>\n",
       "      <td>4817</td>\n",
       "      <td>4817</td>\n",
       "      <td>4817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">International Relations</th>\n",
       "      <th>left</th>\n",
       "      <td>9051</td>\n",
       "      <td>9051</td>\n",
       "      <td>9051</td>\n",
       "      <td>9051</td>\n",
       "      <td>9051</td>\n",
       "      <td>9051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1160</td>\n",
       "      <td>1160</td>\n",
       "      <td>1160</td>\n",
       "      <td>1160</td>\n",
       "      <td>1160</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Labour and Social Welfare</th>\n",
       "      <th>left</th>\n",
       "      <td>70547</td>\n",
       "      <td>70547</td>\n",
       "      <td>70547</td>\n",
       "      <td>70547</td>\n",
       "      <td>70547</td>\n",
       "      <td>70547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1530</td>\n",
       "      <td>1530</td>\n",
       "      <td>1530</td>\n",
       "      <td>1530</td>\n",
       "      <td>1530</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2570</td>\n",
       "      <td>2570</td>\n",
       "      <td>2570</td>\n",
       "      <td>2570</td>\n",
       "      <td>2570</td>\n",
       "      <td>2570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Military</th>\n",
       "      <th>left</th>\n",
       "      <td>1798</td>\n",
       "      <td>1798</td>\n",
       "      <td>1798</td>\n",
       "      <td>1798</td>\n",
       "      <td>1798</td>\n",
       "      <td>1798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>3800</td>\n",
       "      <td>3800</td>\n",
       "      <td>3800</td>\n",
       "      <td>3800</td>\n",
       "      <td>3800</td>\n",
       "      <td>3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>neutral</th>\n",
       "      <td>28030</td>\n",
       "      <td>28030</td>\n",
       "      <td>28030</td>\n",
       "      <td>28030</td>\n",
       "      <td>28030</td>\n",
       "      <td>28030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Political System</th>\n",
       "      <th>left</th>\n",
       "      <td>934</td>\n",
       "      <td>934</td>\n",
       "      <td>934</td>\n",
       "      <td>934</td>\n",
       "      <td>934</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>16566</td>\n",
       "      <td>16566</td>\n",
       "      <td>16566</td>\n",
       "      <td>16566</td>\n",
       "      <td>16566</td>\n",
       "      <td>16566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>13801</td>\n",
       "      <td>13801</td>\n",
       "      <td>13801</td>\n",
       "      <td>13801</td>\n",
       "      <td>13801</td>\n",
       "      <td>13801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text    idx  country  election  party  \\\n",
       "topic                       sentiment                                           \n",
       "Agriculture - Protectionism left        7381   7381     7381      7381   7381   \n",
       "                            right        935    935      935       935    935   \n",
       "Economics                   left       20294  20294    20294     20294  20294   \n",
       "                            neutral    29459  29459    29459     29459  29459   \n",
       "                            right      14528  14528    14528     14528  14528   \n",
       "Education                   left       17314  17314    17314     17314  17314   \n",
       "                            right        257    257      257       257    257   \n",
       "Environment - Growth        left       24621  24621    24621     24621  24621   \n",
       "                            neutral     7635   7635     7635      7635   7635   \n",
       "                            right       4203   4203     4203      4203   4203   \n",
       "European Integration        left        5481   5481     5481      5481   5481   \n",
       "                            right       2428   2428     2428      2428   2428   \n",
       "Fabrics of Society          left       14002  14002    14002     14002  14002   \n",
       "                            neutral     7041   7041     7041      7041   7041   \n",
       "                            right      19632  19632    19632     19632  19632   \n",
       "Immigration                 left        4817   4817     4817      4817   4817   \n",
       "                            right       6699   6699     6699      6699   6699   \n",
       "International Relations     left        9051   9051     9051      9051   9051   \n",
       "                            neutral     1160   1160     1160      1160   1160   \n",
       "                            right        898    898      898       898    898   \n",
       "Labour and Social Welfare   left       70547  70547    70547     70547  70547   \n",
       "                            neutral     1530   1530     1530      1530   1530   \n",
       "                            right       2570   2570     2570      2570   2570   \n",
       "Military                    left        1798   1798     1798      1798   1798   \n",
       "                            right       3800   3800     3800      3800   3800   \n",
       "Other                       neutral    28030  28030    28030     28030  28030   \n",
       "Political System            left         934    934      934       934    934   \n",
       "                            neutral    16566  16566    16566     16566  16566   \n",
       "                            right      13801  13801    13801     13801  13801   \n",
       "\n",
       "                                       cmp_code  \n",
       "topic                       sentiment            \n",
       "Agriculture - Protectionism left           7381  \n",
       "                            right           935  \n",
       "Economics                   left          20294  \n",
       "                            neutral       29459  \n",
       "                            right         14528  \n",
       "Education                   left          17314  \n",
       "                            right           257  \n",
       "Environment - Growth        left          24621  \n",
       "                            neutral        7635  \n",
       "                            right          4203  \n",
       "European Integration        left           5481  \n",
       "                            right          2428  \n",
       "Fabrics of Society          left          14002  \n",
       "                            neutral        7041  \n",
       "                            right         19632  \n",
       "Immigration                 left           4817  \n",
       "                            right          6699  \n",
       "International Relations     left           9051  \n",
       "                            neutral        1160  \n",
       "                            right           898  \n",
       "Labour and Social Welfare   left          70547  \n",
       "                            neutral        1530  \n",
       "                            right          2570  \n",
       "Military                    left           1798  \n",
       "                            right          3800  \n",
       "Other                       neutral       28030  \n",
       "Political System            left            934  \n",
       "                            neutral       16566  \n",
       "                            right         13801  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.groupby(['topic','sentiment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length (word) is: 74.3312271051415\n",
      "Std length (word) is: 36.82022801372765\n",
      "Min length (word) is: 1\n",
      "Max length (word) is: 374\n"
     ]
    }
   ],
   "source": [
    "texts = manifesto_regrouped['text'].tolist()\n",
    "from statistics import stdev, mean\n",
    "## Before\n",
    "seq_len = [len(i.split()) for i in texts]\n",
    "seq_len_mean = mean(seq_len)\n",
    "seq_len_std = stdev(seq_len)\n",
    "seq_len_max = max(seq_len)\n",
    "seq_len_min = min(seq_len)\n",
    "print('Mean length (word) is: {}'.format(seq_len_mean))\n",
    "print('Std length (word) is: {}'.format(seq_len_std))\n",
    "print('Min length (word) is: {}'.format(seq_len_min))\n",
    "print('Max length (word) is: {}'.format(seq_len_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.to_csv('data/temps/manifesto_regrouped.csv', encoding='utf-8', index=False)\n",
    "manifesto.to_csv('data/temps/manifesto.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto = pd.read_csv('data/temps/manifesto.csv', encoding='utf-8', dtype={2:'str',18: 'str'})\n",
    "manifesto_regrouped = pd.read_csv('data/temps/manifesto_regrouped.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_reduced = manifesto_regrouped[['topic','sentiment','text']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_reduced['topic_sentiment'] = manifesto_reduced['topic'] + '_' + manifesto_reduced['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52fecd9496e4b91b87630b5154ef6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/337412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8def9b059fd742ac81cbc1e5a073285d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/337412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cc7c155f654a27abe6bb02dfe60413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/337412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manifesto_dataset = Dataset.from_pandas(manifesto_reduced)\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('topic')\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('sentiment')\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('topic_sentiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save class labels\n",
    "import pickle\n",
    "topic_labels = manifesto_dataset.features['topic'].names\n",
    "file_path = 'data/temps/topic_labels'\n",
    "with open(file_path, 'wb') as fp:\n",
    "    pickle.dump(topic_labels, fp)\n",
    "\n",
    "sentiment_labels = manifesto_dataset.features['sentiment'].names\n",
    "file_path = 'data/temps/sentiment_labels'\n",
    "with open(file_path, 'wb') as fp:\n",
    "    pickle.dump(sentiment_labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = manifesto_dataset.train_test_split(test_size=0.1, stratify_by_column='topic_sentiment', seed=seed_val)\n",
    "train_eval = train_test['train'].train_test_split(test_size=0.3, stratify_by_column='topic_sentiment', seed=seed_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 212569\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 33742\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 91101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_datasets = DatasetDict({\n",
    "    'train': train_eval['train'],\n",
    "    'test': train_test['test'],\n",
    "    'eval': train_eval['test']\n",
    "})\n",
    "manifesto_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e2907139a44d4092355578554c3d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/212569 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c464b82ac7474b74b6e001a56bdbd519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1846949f9156448e87a3236acfd3204d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['topic', 'sentiment', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = manifesto_datasets.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text', 'topic_sentiment'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=16, shuffle=False, collate_fn = data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['eval'], batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion_sent = nn.CrossEntropyLoss()\n",
    "criterion_topic =  nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.751638, elapsed=12.38s, remaining=1556.61s.\n",
      "Batch 200: loss=3.698838, elapsed=23.59s, remaining=1485.39s.\n",
      "Batch 300: loss=3.630751, elapsed=34.56s, remaining=1441.65s.\n",
      "Batch 400: loss=3.565981, elapsed=45.33s, remaining=1410.88s.\n",
      "Batch 500: loss=3.504886, elapsed=55.74s, remaining=1377.90s.\n",
      "Batch 600: loss=3.434784, elapsed=66.58s, remaining=1359.77s.\n",
      "Batch 700: loss=3.356035, elapsed=77.50s, remaining=1346.71s.\n",
      "Batch 800: loss=3.267085, elapsed=88.44s, remaining=1334.20s.\n",
      "Batch 900: loss=3.179063, elapsed=99.50s, remaining=1322.91s.\n",
      "Batch 1000: loss=3.092085, elapsed=110.39s, remaining=1310.51s.\n",
      "Batch 1100: loss=3.009968, elapsed=121.31s, remaining=1298.75s.\n",
      "Batch 1200: loss=2.938461, elapsed=132.15s, remaining=1286.04s.\n",
      "Batch 1300: loss=2.872399, elapsed=142.86s, remaining=1272.81s.\n",
      "Batch 1400: loss=2.807210, elapsed=153.71s, remaining=1260.81s.\n",
      "Batch 1500: loss=2.754112, elapsed=164.21s, remaining=1246.37s.\n",
      "Batch 1600: loss=2.702598, elapsed=175.21s, remaining=1235.65s.\n",
      "Batch 1700: loss=2.654283, elapsed=186.09s, remaining=1224.27s.\n",
      "Batch 1800: loss=2.607766, elapsed=197.17s, remaining=1213.97s.\n",
      "Batch 1900: loss=2.565347, elapsed=207.97s, remaining=1202.61s.\n",
      "Batch 2000: loss=2.526054, elapsed=218.56s, remaining=1189.90s.\n",
      "Batch 2100: loss=2.487752, elapsed=229.64s, remaining=1179.90s.\n",
      "Batch 2200: loss=2.452389, elapsed=240.55s, remaining=1168.95s.\n",
      "Batch 2300: loss=2.418823, elapsed=251.15s, remaining=1156.30s.\n",
      "Batch 2400: loss=2.387216, elapsed=262.18s, remaining=1145.98s.\n",
      "Batch 2500: loss=2.356465, elapsed=273.29s, remaining=1135.98s.\n",
      "Batch 2600: loss=2.326544, elapsed=284.16s, remaining=1124.63s.\n",
      "Batch 2700: loss=2.300500, elapsed=295.05s, remaining=1113.61s.\n",
      "Batch 2800: loss=2.274177, elapsed=305.86s, remaining=1102.15s.\n",
      "Batch 2900: loss=2.251562, elapsed=316.86s, remaining=1091.45s.\n",
      "Batch 3000: loss=2.229775, elapsed=327.65s, remaining=1080.02s.\n",
      "Batch 3100: loss=2.207413, elapsed=338.78s, remaining=1069.84s.\n",
      "Batch 3200: loss=2.186642, elapsed=349.54s, remaining=1058.50s.\n",
      "Batch 3300: loss=2.165677, elapsed=360.11s, remaining=1046.64s.\n",
      "Batch 3400: loss=2.147561, elapsed=370.58s, remaining=1034.54s.\n",
      "Batch 3500: loss=2.130107, elapsed=380.93s, remaining=1022.18s.\n",
      "Batch 3600: loss=2.110620, elapsed=391.56s, remaining=1010.63s.\n",
      "Batch 3700: loss=2.093969, elapsed=402.13s, remaining=999.04s.\n",
      "Batch 3800: loss=2.076977, elapsed=412.65s, remaining=987.36s.\n",
      "Batch 3900: loss=2.059899, elapsed=423.02s, remaining=975.40s.\n",
      "Batch 4000: loss=2.046047, elapsed=433.65s, remaining=964.13s.\n",
      "Batch 4100: loss=2.030547, elapsed=444.02s, remaining=952.33s.\n",
      "Batch 4200: loss=2.016885, elapsed=454.60s, remaining=941.01s.\n",
      "Batch 4300: loss=2.003245, elapsed=465.19s, remaining=929.71s.\n",
      "Batch 4400: loss=1.990099, elapsed=475.84s, remaining=918.54s.\n",
      "Batch 4500: loss=1.977794, elapsed=486.84s, remaining=908.09s.\n",
      "Batch 4600: loss=1.965846, elapsed=497.40s, remaining=896.75s.\n",
      "Batch 4700: loss=1.954403, elapsed=508.03s, remaining=885.58s.\n",
      "Batch 4800: loss=1.942456, elapsed=518.62s, remaining=874.40s.\n",
      "Batch 4900: loss=1.931495, elapsed=529.09s, remaining=863.14s.\n",
      "Batch 5000: loss=1.920695, elapsed=539.37s, remaining=851.34s.\n",
      "Batch 5100: loss=1.911001, elapsed=549.78s, remaining=840.10s.\n",
      "Batch 5200: loss=1.901332, elapsed=560.26s, remaining=829.06s.\n",
      "Batch 5300: loss=1.891611, elapsed=570.69s, remaining=817.84s.\n",
      "Batch 5400: loss=1.880918, elapsed=581.22s, remaining=806.77s.\n",
      "Batch 5500: loss=1.870913, elapsed=591.64s, remaining=795.38s.\n",
      "Batch 5600: loss=1.862149, elapsed=602.23s, remaining=784.29s.\n",
      "Batch 5700: loss=1.854470, elapsed=612.78s, remaining=773.25s.\n",
      "Batch 5800: loss=1.845274, elapsed=623.34s, remaining=762.34s.\n",
      "Batch 5900: loss=1.838946, elapsed=633.87s, remaining=751.40s.\n",
      "Batch 6000: loss=1.830990, elapsed=644.29s, remaining=740.17s.\n",
      "Batch 6100: loss=1.823663, elapsed=654.61s, remaining=728.92s.\n",
      "Batch 6200: loss=1.815550, elapsed=665.34s, remaining=718.23s.\n",
      "Batch 6300: loss=1.808339, elapsed=675.94s, remaining=707.40s.\n",
      "Batch 6400: loss=1.800968, elapsed=686.55s, remaining=696.64s.\n",
      "Batch 6500: loss=1.795088, elapsed=697.39s, remaining=686.13s.\n",
      "Batch 6600: loss=1.788605, elapsed=708.03s, remaining=675.24s.\n",
      "Batch 6700: loss=1.782267, elapsed=718.45s, remaining=664.44s.\n",
      "Batch 6800: loss=1.775466, elapsed=729.09s, remaining=653.58s.\n",
      "Batch 6900: loss=1.769486, elapsed=739.29s, remaining=642.46s.\n",
      "Batch 7000: loss=1.764081, elapsed=749.85s, remaining=631.51s.\n",
      "Batch 7100: loss=1.757868, elapsed=760.33s, remaining=620.67s.\n",
      "Batch 7200: loss=1.751949, elapsed=770.78s, remaining=609.72s.\n",
      "Batch 7300: loss=1.746284, elapsed=780.95s, remaining=598.57s.\n",
      "Batch 7400: loss=1.740422, elapsed=791.61s, remaining=587.87s.\n",
      "Batch 7500: loss=1.733765, elapsed=802.30s, remaining=577.21s.\n",
      "Batch 7600: loss=1.727971, elapsed=812.67s, remaining=566.34s.\n",
      "Batch 7700: loss=1.721925, elapsed=822.92s, remaining=555.40s.\n",
      "Batch 7800: loss=1.716892, elapsed=833.31s, remaining=544.46s.\n",
      "Batch 7900: loss=1.711460, elapsed=843.69s, remaining=533.60s.\n",
      "Batch 8000: loss=1.706157, elapsed=854.06s, remaining=522.70s.\n",
      "Batch 8100: loss=1.701159, elapsed=864.39s, remaining=511.81s.\n",
      "Batch 8200: loss=1.695627, elapsed=874.92s, remaining=501.06s.\n",
      "Batch 8300: loss=1.690547, elapsed=885.47s, remaining=490.35s.\n",
      "Batch 8400: loss=1.686285, elapsed=896.04s, remaining=479.73s.\n",
      "Batch 8500: loss=1.682314, elapsed=906.91s, remaining=469.07s.\n",
      "Batch 8600: loss=1.677553, elapsed=917.36s, remaining=458.25s.\n",
      "Batch 8700: loss=1.672696, elapsed=927.94s, remaining=447.57s.\n",
      "Batch 8800: loss=1.667783, elapsed=938.46s, remaining=436.82s.\n",
      "Batch 8900: loss=1.663115, elapsed=949.08s, remaining=426.21s.\n",
      "Batch 9000: loss=1.658526, elapsed=959.39s, remaining=415.34s.\n",
      "Batch 9100: loss=1.653869, elapsed=969.70s, remaining=404.53s.\n",
      "Batch 9200: loss=1.649528, elapsed=980.02s, remaining=393.71s.\n",
      "Batch 9300: loss=1.645032, elapsed=990.48s, remaining=383.03s.\n",
      "Batch 9400: loss=1.639860, elapsed=1001.11s, remaining=372.36s.\n",
      "Batch 9500: loss=1.636323, elapsed=1011.92s, remaining=361.76s.\n",
      "Batch 9600: loss=1.631650, elapsed=1022.76s, remaining=351.26s.\n",
      "Batch 9700: loss=1.628364, elapsed=1033.02s, remaining=340.50s.\n",
      "Batch 9800: loss=1.624696, elapsed=1043.84s, remaining=329.92s.\n",
      "Batch 9900: loss=1.620513, elapsed=1054.56s, remaining=319.31s.\n",
      "Batch 10000: loss=1.616145, elapsed=1065.29s, remaining=308.74s.\n",
      "Batch 10100: loss=1.612550, elapsed=1075.66s, remaining=297.97s.\n",
      "Batch 10200: loss=1.608241, elapsed=1086.14s, remaining=287.29s.\n",
      "Batch 10300: loss=1.603836, elapsed=1096.74s, remaining=276.61s.\n",
      "Batch 10400: loss=1.600465, elapsed=1107.35s, remaining=265.94s.\n",
      "Batch 10500: loss=1.596189, elapsed=1118.12s, remaining=255.28s.\n",
      "Batch 10600: loss=1.591901, elapsed=1129.06s, remaining=244.67s.\n",
      "Batch 10700: loss=1.588681, elapsed=1139.65s, remaining=234.00s.\n",
      "Batch 10800: loss=1.584771, elapsed=1150.13s, remaining=223.29s.\n",
      "Batch 10900: loss=1.580664, elapsed=1160.50s, remaining=212.58s.\n",
      "Batch 11000: loss=1.577424, elapsed=1170.79s, remaining=201.86s.\n",
      "Batch 11100: loss=1.573821, elapsed=1181.32s, remaining=191.18s.\n",
      "Batch 11200: loss=1.570290, elapsed=1191.90s, remaining=180.57s.\n",
      "Batch 11300: loss=1.566823, elapsed=1202.56s, remaining=169.93s.\n",
      "Batch 11400: loss=1.563173, elapsed=1212.85s, remaining=159.32s.\n",
      "Batch 11500: loss=1.560154, elapsed=1223.14s, remaining=148.61s.\n",
      "Batch 11600: loss=1.557607, elapsed=1233.60s, remaining=137.95s.\n",
      "Batch 11700: loss=1.554258, elapsed=1243.99s, remaining=127.29s.\n",
      "Batch 11800: loss=1.551129, elapsed=1254.18s, remaining=116.66s.\n",
      "Batch 11900: loss=1.547953, elapsed=1264.60s, remaining=106.02s.\n",
      "Batch 12000: loss=1.545144, elapsed=1274.84s, remaining=95.42s.\n",
      "Batch 12100: loss=1.542151, elapsed=1285.22s, remaining=84.83s.\n",
      "Batch 12200: loss=1.538436, elapsed=1295.82s, remaining=74.21s.\n",
      "Batch 12300: loss=1.535675, elapsed=1306.03s, remaining=63.62s.\n",
      "Batch 12400: loss=1.532415, elapsed=1316.48s, remaining=52.99s.\n",
      "Batch 12500: loss=1.529451, elapsed=1326.90s, remaining=42.37s.\n",
      "Batch 12600: loss=1.526501, elapsed=1337.44s, remaining=31.66s.\n",
      "Batch 12700: loss=1.523326, elapsed=1348.05s, remaining=21.07s.\n",
      "Batch 12800: loss=1.520694, elapsed=1358.65s, remaining=10.52s.\n",
      "Batch 12900: loss=1.517658, elapsed=1369.62s, remaining=-0.05s.\n",
      "Batch 13000: loss=1.514947, elapsed=1380.03s, remaining=-10.60s.\n",
      "Batch 13100: loss=1.512128, elapsed=1390.84s, remaining=-21.19s.\n",
      "Batch 13200: loss=1.509531, elapsed=1401.71s, remaining=-31.84s.\n",
      "\n",
      "Training epoch took: 1410.84s\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 1.105576 \n",
      "\n",
      "Accuracy - Sentiment: 80.1%, Avg loss: 1.105576 \n",
      "\n",
      "Accuracy - Topic: 79.7%, Avg loss: 1.105576 \n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=1.099284, elapsed=10.85s, remaining=1373.32s.\n",
      "Batch 200: loss=1.047196, elapsed=21.56s, remaining=1363.83s.\n",
      "Batch 300: loss=1.053464, elapsed=32.32s, remaining=1352.37s.\n",
      "Batch 400: loss=1.045932, elapsed=42.87s, remaining=1335.98s.\n",
      "Batch 500: loss=1.047987, elapsed=53.65s, remaining=1327.45s.\n",
      "Batch 600: loss=1.053051, elapsed=64.28s, remaining=1314.29s.\n",
      "Batch 700: loss=1.048535, elapsed=75.00s, remaining=1304.22s.\n",
      "Batch 800: loss=1.051369, elapsed=85.73s, remaining=1294.40s.\n",
      "Batch 900: loss=1.051098, elapsed=96.78s, remaining=1289.37s.\n",
      "Batch 1000: loss=1.050842, elapsed=107.55s, remaining=1279.10s.\n",
      "Batch 1100: loss=1.042542, elapsed=118.39s, remaining=1269.84s.\n",
      "Batch 1200: loss=1.040426, elapsed=129.01s, remaining=1258.08s.\n",
      "Batch 1300: loss=1.039076, elapsed=139.79s, remaining=1247.06s.\n",
      "Batch 1400: loss=1.036100, elapsed=150.72s, remaining=1238.62s.\n",
      "Batch 1500: loss=1.033606, elapsed=161.65s, remaining=1228.98s.\n",
      "Batch 1600: loss=1.028392, elapsed=172.44s, remaining=1218.36s.\n",
      "Batch 1700: loss=1.027353, elapsed=183.13s, remaining=1207.14s.\n",
      "Batch 1800: loss=1.027447, elapsed=194.07s, remaining=1197.32s.\n",
      "Batch 1900: loss=1.028300, elapsed=205.03s, remaining=1187.47s.\n",
      "Batch 2000: loss=1.029299, elapsed=215.59s, remaining=1175.50s.\n",
      "Batch 2100: loss=1.024904, elapsed=226.04s, remaining=1162.60s.\n",
      "Batch 2200: loss=1.024950, elapsed=236.72s, remaining=1151.82s.\n",
      "Batch 2300: loss=1.024427, elapsed=247.19s, remaining=1139.65s.\n",
      "Batch 2400: loss=1.023326, elapsed=258.19s, remaining=1130.55s.\n",
      "Batch 2500: loss=1.025246, elapsed=268.63s, remaining=1118.34s.\n",
      "Batch 2600: loss=1.025032, elapsed=279.15s, remaining=1106.58s.\n",
      "Batch 2700: loss=1.026692, elapsed=289.70s, remaining=1095.11s.\n",
      "Batch 2800: loss=1.027299, elapsed=300.00s, remaining=1082.93s.\n",
      "Batch 2900: loss=1.025632, elapsed=310.35s, remaining=1071.35s.\n",
      "Batch 3000: loss=1.024458, elapsed=320.73s, remaining=1059.69s.\n",
      "Batch 3100: loss=1.021082, elapsed=331.23s, remaining=1048.05s.\n",
      "Batch 3200: loss=1.021764, elapsed=341.88s, remaining=1037.69s.\n",
      "Batch 3300: loss=1.020072, elapsed=352.28s, remaining=1026.11s.\n",
      "Batch 3400: loss=1.019334, elapsed=362.66s, remaining=1014.63s.\n",
      "Batch 3500: loss=1.018045, elapsed=373.65s, remaining=1004.70s.\n",
      "Batch 3600: loss=1.018112, elapsed=384.20s, remaining=993.84s.\n",
      "Batch 3700: loss=1.017992, elapsed=394.80s, remaining=982.82s.\n",
      "Batch 3800: loss=1.018463, elapsed=405.16s, remaining=971.31s.\n",
      "Batch 3900: loss=1.017535, elapsed=415.84s, remaining=960.66s.\n",
      "Batch 4000: loss=1.016106, elapsed=426.32s, remaining=949.38s.\n",
      "Batch 4100: loss=1.015232, elapsed=436.75s, remaining=938.23s.\n",
      "Batch 4200: loss=1.015385, elapsed=447.32s, remaining=927.37s.\n",
      "Batch 4300: loss=1.016234, elapsed=457.83s, remaining=916.50s.\n",
      "Batch 4400: loss=1.016380, elapsed=468.29s, remaining=905.46s.\n",
      "Batch 4500: loss=1.015736, elapsed=478.78s, remaining=894.58s.\n",
      "Batch 4600: loss=1.014366, elapsed=489.11s, remaining=883.44s.\n",
      "Batch 4700: loss=1.013933, elapsed=499.54s, remaining=872.49s.\n",
      "Batch 4800: loss=1.011874, elapsed=509.97s, remaining=861.72s.\n",
      "Batch 4900: loss=1.009948, elapsed=520.55s, remaining=851.00s.\n",
      "Batch 5000: loss=1.008869, elapsed=531.04s, remaining=840.09s.\n",
      "Batch 5100: loss=1.010440, elapsed=541.57s, remaining=829.28s.\n",
      "Batch 5200: loss=1.010800, elapsed=552.09s, remaining=818.39s.\n",
      "Batch 5300: loss=1.009365, elapsed=562.47s, remaining=807.36s.\n",
      "Batch 5400: loss=1.008722, elapsed=573.05s, remaining=796.81s.\n",
      "Batch 5500: loss=1.007700, elapsed=583.47s, remaining=786.16s.\n",
      "Batch 5600: loss=1.007611, elapsed=593.66s, remaining=774.89s.\n",
      "Batch 5700: loss=1.007675, elapsed=603.93s, remaining=763.58s.\n",
      "Batch 5800: loss=1.006805, elapsed=614.23s, remaining=752.62s.\n",
      "Batch 5900: loss=1.006563, elapsed=624.83s, remaining=742.01s.\n",
      "Batch 6000: loss=1.006697, elapsed=635.36s, remaining=731.37s.\n",
      "Batch 6100: loss=1.005522, elapsed=645.80s, remaining=720.57s.\n",
      "Batch 6200: loss=1.004371, elapsed=656.29s, remaining=710.10s.\n",
      "Batch 6300: loss=1.004197, elapsed=666.69s, remaining=699.37s.\n",
      "Batch 6400: loss=1.003354, elapsed=677.13s, remaining=688.72s.\n",
      "Batch 6500: loss=1.002658, elapsed=687.55s, remaining=677.79s.\n",
      "Batch 6600: loss=1.001684, elapsed=697.96s, remaining=667.14s.\n",
      "Batch 6700: loss=1.000866, elapsed=708.23s, remaining=656.13s.\n",
      "Batch 6800: loss=1.001280, elapsed=718.77s, remaining=645.58s.\n",
      "Batch 6900: loss=1.000611, elapsed=729.22s, remaining=634.88s.\n",
      "Batch 7000: loss=0.999316, elapsed=739.48s, remaining=624.14s.\n",
      "Batch 7100: loss=0.998729, elapsed=750.01s, remaining=613.58s.\n",
      "Batch 7200: loss=0.998322, elapsed=760.52s, remaining=602.81s.\n",
      "Batch 7300: loss=0.998305, elapsed=771.11s, remaining=592.27s.\n",
      "Batch 7400: loss=0.997335, elapsed=781.46s, remaining=581.60s.\n",
      "Batch 7500: loss=0.996680, elapsed=792.00s, remaining=571.06s.\n",
      "Batch 7600: loss=0.995918, elapsed=802.41s, remaining=560.26s.\n",
      "Batch 7700: loss=0.995567, elapsed=812.96s, remaining=549.76s.\n",
      "Batch 7800: loss=0.995066, elapsed=823.57s, remaining=539.22s.\n",
      "Batch 7900: loss=0.993834, elapsed=834.00s, remaining=528.66s.\n",
      "Batch 8000: loss=0.993558, elapsed=844.25s, remaining=517.93s.\n",
      "Batch 8100: loss=0.992762, elapsed=854.58s, remaining=507.16s.\n",
      "Batch 8200: loss=0.992809, elapsed=865.00s, remaining=496.53s.\n",
      "Batch 8300: loss=0.992210, elapsed=875.25s, remaining=485.77s.\n",
      "Batch 8400: loss=0.991283, elapsed=885.65s, remaining=475.13s.\n",
      "Batch 8500: loss=0.990345, elapsed=896.26s, remaining=464.60s.\n",
      "Batch 8600: loss=0.989586, elapsed=906.71s, remaining=454.01s.\n",
      "Batch 8700: loss=0.989054, elapsed=917.34s, remaining=443.53s.\n",
      "Batch 8800: loss=0.988002, elapsed=927.53s, remaining=432.70s.\n",
      "Batch 8900: loss=0.987336, elapsed=937.83s, remaining=422.10s.\n",
      "Batch 9000: loss=0.986944, elapsed=948.23s, remaining=411.49s.\n",
      "Batch 9100: loss=0.985704, elapsed=958.52s, remaining=400.90s.\n",
      "Batch 9200: loss=0.984448, elapsed=968.82s, remaining=390.22s.\n",
      "Batch 9300: loss=0.983878, elapsed=979.44s, remaining=379.76s.\n",
      "Batch 9400: loss=0.983572, elapsed=990.10s, remaining=369.36s.\n",
      "Batch 9500: loss=0.982665, elapsed=1000.63s, remaining=358.82s.\n",
      "Batch 9600: loss=0.981791, elapsed=1011.21s, remaining=348.33s.\n",
      "Batch 9700: loss=0.981316, elapsed=1021.47s, remaining=337.68s.\n",
      "Batch 9800: loss=0.981169, elapsed=1032.16s, remaining=327.19s.\n",
      "Batch 9900: loss=0.980553, elapsed=1042.69s, remaining=316.71s.\n",
      "Batch 10000: loss=0.980278, elapsed=1053.00s, remaining=306.15s.\n",
      "Batch 10100: loss=0.979682, elapsed=1063.58s, remaining=295.67s.\n",
      "Batch 10200: loss=0.978914, elapsed=1074.08s, remaining=285.24s.\n",
      "Batch 10300: loss=0.978843, elapsed=1084.45s, remaining=274.65s.\n",
      "Batch 10400: loss=0.978540, elapsed=1094.94s, remaining=264.18s.\n",
      "Batch 10500: loss=0.977492, elapsed=1105.23s, remaining=253.56s.\n",
      "Batch 10600: loss=0.976817, elapsed=1115.70s, remaining=243.05s.\n",
      "Batch 10700: loss=0.976212, elapsed=1126.23s, remaining=232.57s.\n",
      "Batch 10800: loss=0.975774, elapsed=1136.69s, remaining=222.05s.\n",
      "Batch 10900: loss=0.975026, elapsed=1147.15s, remaining=211.54s.\n",
      "Batch 11000: loss=0.974617, elapsed=1157.40s, remaining=200.97s.\n",
      "Batch 11100: loss=0.973986, elapsed=1167.66s, remaining=190.46s.\n",
      "Batch 11200: loss=0.973225, elapsed=1177.65s, remaining=179.78s.\n",
      "Batch 11300: loss=0.972960, elapsed=1187.84s, remaining=169.22s.\n",
      "Batch 11400: loss=0.971815, elapsed=1198.18s, remaining=158.70s.\n",
      "Batch 11500: loss=0.971167, elapsed=1208.46s, remaining=148.14s.\n",
      "Batch 11600: loss=0.970572, elapsed=1218.72s, remaining=137.61s.\n",
      "Batch 11700: loss=0.970027, elapsed=1229.01s, remaining=127.13s.\n",
      "Batch 11800: loss=0.969694, elapsed=1239.20s, remaining=116.59s.\n",
      "Batch 11900: loss=0.969421, elapsed=1250.01s, remaining=106.12s.\n",
      "Batch 12000: loss=0.969156, elapsed=1260.60s, remaining=95.59s.\n",
      "Batch 12100: loss=0.968659, elapsed=1271.40s, remaining=85.09s.\n",
      "Batch 12200: loss=0.967978, elapsed=1281.82s, remaining=74.58s.\n",
      "Batch 12300: loss=0.967504, elapsed=1292.38s, remaining=64.07s.\n",
      "Batch 12400: loss=0.966753, elapsed=1302.93s, remaining=53.47s.\n",
      "Batch 12500: loss=0.966167, elapsed=1313.53s, remaining=42.85s.\n",
      "Batch 12600: loss=0.965614, elapsed=1324.36s, remaining=32.25s.\n",
      "Batch 12700: loss=0.965089, elapsed=1335.20s, remaining=21.65s.\n",
      "Batch 12800: loss=0.964741, elapsed=1345.98s, remaining=11.03s.\n",
      "Batch 12900: loss=0.963967, elapsed=1356.84s, remaining=0.40s.\n",
      "Batch 13000: loss=0.963335, elapsed=1367.67s, remaining=-10.21s.\n",
      "Batch 13100: loss=0.962707, elapsed=1378.27s, remaining=-20.83s.\n",
      "Batch 13200: loss=0.962312, elapsed=1389.14s, remaining=-31.45s.\n",
      "\n",
      "Training epoch took: 1398.14s\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.947132 \n",
      "\n",
      "Accuracy - Sentiment: 83.3%, Avg loss: 0.947132 \n",
      "\n",
      "Accuracy - Topic: 82.5%, Avg loss: 0.947132 \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.710670, elapsed=10.82s, remaining=1357.19s.\n",
      "Batch 200: loss=0.711007, elapsed=21.58s, remaining=1349.39s.\n",
      "Batch 300: loss=0.721914, elapsed=32.38s, remaining=1340.14s.\n",
      "Batch 400: loss=0.729197, elapsed=43.00s, remaining=1327.66s.\n",
      "Batch 500: loss=0.734731, elapsed=53.32s, remaining=1309.46s.\n",
      "Batch 600: loss=0.731853, elapsed=63.81s, remaining=1297.38s.\n",
      "Batch 700: loss=0.729429, elapsed=74.53s, remaining=1289.83s.\n",
      "Batch 800: loss=0.735027, elapsed=85.00s, remaining=1277.58s.\n",
      "Batch 900: loss=0.732785, elapsed=95.52s, remaining=1266.53s.\n",
      "Batch 1000: loss=0.734151, elapsed=105.96s, remaining=1254.55s.\n",
      "Batch 1100: loss=0.739939, elapsed=116.22s, remaining=1240.88s.\n",
      "Batch 1200: loss=0.736631, elapsed=126.68s, remaining=1229.81s.\n",
      "Batch 1300: loss=0.734905, elapsed=137.40s, remaining=1220.15s.\n",
      "Batch 1400: loss=0.730180, elapsed=148.25s, remaining=1211.54s.\n",
      "Batch 1500: loss=0.731442, elapsed=158.77s, remaining=1200.62s.\n",
      "Batch 1600: loss=0.736901, elapsed=169.18s, remaining=1188.97s.\n",
      "Batch 1700: loss=0.738129, elapsed=179.83s, remaining=1179.14s.\n",
      "Batch 1800: loss=0.737114, elapsed=190.35s, remaining=1168.46s.\n",
      "Batch 1900: loss=0.738661, elapsed=200.88s, remaining=1157.77s.\n",
      "Batch 2000: loss=0.738405, elapsed=211.43s, remaining=1147.35s.\n",
      "Batch 2100: loss=0.736703, elapsed=222.04s, remaining=1137.15s.\n",
      "Batch 2200: loss=0.736312, elapsed=232.28s, remaining=1125.21s.\n",
      "Batch 2300: loss=0.735650, elapsed=242.85s, remaining=1114.84s.\n",
      "Batch 2400: loss=0.738006, elapsed=253.52s, remaining=1104.95s.\n",
      "Batch 2500: loss=0.737913, elapsed=263.92s, remaining=1093.86s.\n",
      "Batch 2600: loss=0.737291, elapsed=274.26s, remaining=1082.62s.\n",
      "Batch 2700: loss=0.739592, elapsed=284.88s, remaining=1072.46s.\n",
      "Batch 2800: loss=0.739260, elapsed=295.33s, remaining=1061.72s.\n",
      "Batch 2900: loss=0.738080, elapsed=305.86s, remaining=1051.22s.\n",
      "Batch 3000: loss=0.737156, elapsed=316.43s, remaining=1040.86s.\n",
      "Batch 3100: loss=0.736910, elapsed=326.90s, remaining=1030.18s.\n",
      "Batch 3200: loss=0.735768, elapsed=337.46s, remaining=1019.82s.\n",
      "Batch 3300: loss=0.734771, elapsed=347.94s, remaining=1009.10s.\n",
      "Batch 3400: loss=0.733848, elapsed=358.64s, remaining=999.10s.\n",
      "Batch 3500: loss=0.734465, elapsed=369.08s, remaining=988.37s.\n",
      "Batch 3600: loss=0.733317, elapsed=379.53s, remaining=977.66s.\n",
      "Batch 3700: loss=0.732653, elapsed=390.00s, remaining=967.04s.\n",
      "Batch 3800: loss=0.731352, elapsed=400.48s, remaining=956.43s.\n",
      "Batch 3900: loss=0.732054, elapsed=410.80s, remaining=945.42s.\n",
      "Batch 4000: loss=0.731504, elapsed=421.16s, remaining=934.60s.\n",
      "Batch 4100: loss=0.732419, elapsed=431.66s, remaining=924.05s.\n",
      "Batch 4200: loss=0.733405, elapsed=442.18s, remaining=913.55s.\n",
      "Batch 4300: loss=0.734249, elapsed=452.90s, remaining=903.46s.\n",
      "Batch 4400: loss=0.733644, elapsed=463.36s, remaining=892.85s.\n",
      "Batch 4500: loss=0.733915, elapsed=473.76s, remaining=882.10s.\n",
      "Batch 4600: loss=0.734396, elapsed=484.06s, remaining=871.21s.\n",
      "Batch 4700: loss=0.733783, elapsed=494.49s, remaining=860.58s.\n",
      "Batch 4800: loss=0.733473, elapsed=504.93s, remaining=849.96s.\n",
      "Batch 4900: loss=0.733038, elapsed=515.56s, remaining=839.67s.\n",
      "Batch 5000: loss=0.732098, elapsed=526.17s, remaining=829.35s.\n",
      "Batch 5100: loss=0.734071, elapsed=536.66s, remaining=818.81s.\n",
      "Batch 5200: loss=0.733712, elapsed=547.09s, remaining=808.19s.\n",
      "Batch 5300: loss=0.733087, elapsed=557.54s, remaining=797.61s.\n",
      "Batch 5400: loss=0.733092, elapsed=568.15s, remaining=787.27s.\n",
      "Batch 5500: loss=0.732760, elapsed=578.91s, remaining=777.10s.\n",
      "Batch 5600: loss=0.732739, elapsed=589.62s, remaining=766.85s.\n",
      "Batch 5700: loss=0.733158, elapsed=600.18s, remaining=756.41s.\n",
      "Batch 5800: loss=0.732151, elapsed=610.57s, remaining=745.70s.\n",
      "Batch 5900: loss=0.731777, elapsed=621.14s, remaining=735.26s.\n",
      "Batch 6000: loss=0.730849, elapsed=631.52s, remaining=724.60s.\n",
      "Batch 6100: loss=0.731597, elapsed=641.90s, remaining=713.90s.\n",
      "Batch 6200: loss=0.730512, elapsed=652.24s, remaining=703.21s.\n",
      "Batch 6300: loss=0.731175, elapsed=662.85s, remaining=692.82s.\n",
      "Batch 6400: loss=0.731166, elapsed=673.41s, remaining=682.35s.\n",
      "Batch 6500: loss=0.730172, elapsed=684.02s, remaining=671.93s.\n",
      "Batch 6600: loss=0.729222, elapsed=694.79s, remaining=661.66s.\n",
      "Batch 6700: loss=0.728261, elapsed=705.34s, remaining=651.18s.\n",
      "Batch 6800: loss=0.727759, elapsed=715.82s, remaining=640.65s.\n",
      "Batch 6900: loss=0.727132, elapsed=726.45s, remaining=630.26s.\n",
      "Batch 7000: loss=0.726434, elapsed=736.90s, remaining=619.68s.\n",
      "Batch 7100: loss=0.726849, elapsed=747.24s, remaining=609.02s.\n",
      "Batch 7200: loss=0.726625, elapsed=757.77s, remaining=598.51s.\n",
      "Batch 7300: loss=0.726787, elapsed=768.34s, remaining=588.05s.\n",
      "Batch 7400: loss=0.725909, elapsed=778.60s, remaining=577.34s.\n",
      "Batch 7500: loss=0.725595, elapsed=788.94s, remaining=566.71s.\n",
      "Batch 7600: loss=0.725650, elapsed=799.44s, remaining=556.20s.\n",
      "Batch 7700: loss=0.725426, elapsed=810.07s, remaining=545.77s.\n",
      "Batch 7800: loss=0.724773, elapsed=820.68s, remaining=535.34s.\n",
      "Batch 7900: loss=0.724512, elapsed=830.86s, remaining=524.61s.\n",
      "Batch 8000: loss=0.724107, elapsed=841.39s, remaining=514.12s.\n",
      "Batch 8100: loss=0.723630, elapsed=851.71s, remaining=503.51s.\n",
      "Batch 8200: loss=0.723309, elapsed=861.93s, remaining=492.85s.\n",
      "Batch 8300: loss=0.723137, elapsed=872.33s, remaining=482.29s.\n",
      "Batch 8400: loss=0.722474, elapsed=882.65s, remaining=471.68s.\n",
      "Batch 8500: loss=0.723118, elapsed=893.13s, remaining=461.19s.\n",
      "Batch 8600: loss=0.723453, elapsed=903.76s, remaining=450.76s.\n",
      "Batch 8700: loss=0.722970, elapsed=914.07s, remaining=440.16s.\n",
      "Batch 8800: loss=0.722406, elapsed=924.45s, remaining=429.61s.\n",
      "Batch 8900: loss=0.722125, elapsed=934.85s, remaining=419.07s.\n",
      "Batch 9000: loss=0.722018, elapsed=945.12s, remaining=408.47s.\n",
      "Batch 9100: loss=0.722455, elapsed=955.68s, remaining=398.01s.\n",
      "Batch 9200: loss=0.722042, elapsed=966.49s, remaining=387.67s.\n",
      "Batch 9300: loss=0.721817, elapsed=977.04s, remaining=377.20s.\n",
      "Batch 9400: loss=0.721404, elapsed=987.37s, remaining=366.63s.\n",
      "Batch 9500: loss=0.720857, elapsed=997.86s, remaining=356.13s.\n",
      "Batch 9600: loss=0.720944, elapsed=1008.18s, remaining=345.58s.\n",
      "Batch 9700: loss=0.720374, elapsed=1018.60s, remaining=335.07s.\n",
      "Batch 9800: loss=0.720523, elapsed=1029.05s, remaining=324.57s.\n",
      "Batch 9900: loss=0.720257, elapsed=1039.61s, remaining=314.10s.\n",
      "Batch 10000: loss=0.719684, elapsed=1049.98s, remaining=303.58s.\n",
      "Batch 10100: loss=0.720234, elapsed=1060.61s, remaining=293.13s.\n",
      "Batch 10200: loss=0.719822, elapsed=1071.24s, remaining=282.66s.\n",
      "Batch 10300: loss=0.719623, elapsed=1081.50s, remaining=272.11s.\n",
      "Batch 10400: loss=0.719595, elapsed=1092.07s, remaining=261.64s.\n",
      "Batch 10500: loss=0.719018, elapsed=1102.46s, remaining=251.13s.\n",
      "Batch 10600: loss=0.718976, elapsed=1113.13s, remaining=240.68s.\n",
      "Batch 10700: loss=0.718885, elapsed=1123.65s, remaining=230.18s.\n",
      "Batch 10800: loss=0.718748, elapsed=1134.20s, remaining=219.70s.\n",
      "Batch 10900: loss=0.718340, elapsed=1144.70s, remaining=209.21s.\n",
      "Batch 11000: loss=0.717669, elapsed=1155.33s, remaining=198.75s.\n",
      "Batch 11100: loss=0.717075, elapsed=1165.73s, remaining=188.24s.\n",
      "Batch 11200: loss=0.716670, elapsed=1176.44s, remaining=177.78s.\n",
      "Batch 11300: loss=0.716502, elapsed=1186.90s, remaining=167.27s.\n",
      "Batch 11400: loss=0.716023, elapsed=1197.27s, remaining=156.76s.\n",
      "Batch 11500: loss=0.715539, elapsed=1207.76s, remaining=146.26s.\n",
      "Batch 11600: loss=0.715244, elapsed=1218.39s, remaining=135.79s.\n",
      "Batch 11700: loss=0.715873, elapsed=1228.91s, remaining=125.29s.\n",
      "Batch 11800: loss=0.715715, elapsed=1239.26s, remaining=114.78s.\n",
      "Batch 11900: loss=0.716107, elapsed=1249.91s, remaining=104.29s.\n",
      "Batch 12000: loss=0.715432, elapsed=1260.37s, remaining=93.80s.\n",
      "Batch 12100: loss=0.715353, elapsed=1270.55s, remaining=83.27s.\n",
      "Batch 12200: loss=0.715335, elapsed=1281.00s, remaining=72.77s.\n",
      "Batch 12300: loss=0.714571, elapsed=1291.08s, remaining=62.24s.\n",
      "Batch 12400: loss=0.714378, elapsed=1301.25s, remaining=51.74s.\n",
      "Batch 12500: loss=0.714181, elapsed=1311.90s, remaining=41.25s.\n",
      "Batch 12600: loss=0.713900, elapsed=1322.45s, remaining=30.76s.\n",
      "Batch 12700: loss=0.713192, elapsed=1332.82s, remaining=20.27s.\n",
      "Batch 12800: loss=0.713306, elapsed=1343.23s, remaining=9.78s.\n",
      "Batch 12900: loss=0.713111, elapsed=1353.61s, remaining=-0.71s.\n",
      "Batch 13000: loss=0.712682, elapsed=1364.24s, remaining=-11.19s.\n",
      "Batch 13100: loss=0.712485, elapsed=1374.72s, remaining=-21.67s.\n",
      "Batch 13200: loss=0.712204, elapsed=1385.00s, remaining=-32.16s.\n",
      "\n",
      "Training epoch took: 1393.96s\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.898516 \n",
      "\n",
      "Accuracy - Sentiment: 85.2%, Avg loss: 0.898516 \n",
      "\n",
      "Accuracy - Topic: 84.6%, Avg loss: 0.898516 \n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.538779, elapsed=10.69s, remaining=1352.81s.\n",
      "Batch 200: loss=0.512404, elapsed=21.27s, remaining=1343.33s.\n",
      "Batch 300: loss=0.536012, elapsed=31.81s, remaining=1330.82s.\n",
      "Batch 400: loss=0.534026, elapsed=42.22s, remaining=1315.65s.\n",
      "Batch 500: loss=0.538114, elapsed=52.73s, remaining=1305.01s.\n",
      "Batch 600: loss=0.539428, elapsed=63.29s, remaining=1295.22s.\n",
      "Batch 700: loss=0.540988, elapsed=73.69s, remaining=1282.24s.\n",
      "Batch 800: loss=0.538239, elapsed=84.24s, remaining=1272.24s.\n",
      "Batch 900: loss=0.536390, elapsed=94.76s, remaining=1261.85s.\n",
      "Batch 1000: loss=0.530014, elapsed=105.60s, remaining=1255.29s.\n",
      "Batch 1100: loss=0.526752, elapsed=116.12s, remaining=1244.57s.\n",
      "Batch 1200: loss=0.529231, elapsed=126.72s, remaining=1234.50s.\n",
      "Batch 1300: loss=0.528329, elapsed=137.28s, remaining=1224.08s.\n",
      "Batch 1400: loss=0.528232, elapsed=147.81s, remaining=1213.43s.\n",
      "Batch 1500: loss=0.525470, elapsed=158.30s, remaining=1202.44s.\n",
      "Batch 1600: loss=0.528597, elapsed=168.84s, remaining=1191.78s.\n",
      "Batch 1700: loss=0.530312, elapsed=179.18s, remaining=1179.94s.\n",
      "Batch 1800: loss=0.528425, elapsed=190.05s, remaining=1171.51s.\n",
      "Batch 1900: loss=0.530300, elapsed=200.49s, remaining=1160.26s.\n",
      "Batch 2000: loss=0.528415, elapsed=211.00s, remaining=1149.58s.\n",
      "Batch 2100: loss=0.527334, elapsed=221.41s, remaining=1138.26s.\n",
      "Batch 2200: loss=0.527680, elapsed=231.98s, remaining=1127.91s.\n",
      "Batch 2300: loss=0.524852, elapsed=242.42s, remaining=1116.91s.\n",
      "Batch 2400: loss=0.524333, elapsed=252.98s, remaining=1106.45s.\n",
      "Batch 2500: loss=0.521479, elapsed=263.73s, remaining=1096.83s.\n",
      "Batch 2600: loss=0.524368, elapsed=274.30s, remaining=1086.40s.\n",
      "Batch 2700: loss=0.524928, elapsed=284.90s, remaining=1076.03s.\n",
      "Batch 2800: loss=0.525915, elapsed=295.41s, remaining=1065.42s.\n",
      "Batch 2900: loss=0.525348, elapsed=305.80s, remaining=1054.24s.\n",
      "Batch 3000: loss=0.526233, elapsed=316.19s, remaining=1043.21s.\n",
      "Batch 3100: loss=0.527600, elapsed=326.59s, remaining=1032.24s.\n",
      "Batch 3200: loss=0.527302, elapsed=336.98s, remaining=1021.28s.\n",
      "Batch 3300: loss=0.526916, elapsed=347.39s, remaining=1010.41s.\n",
      "Batch 3400: loss=0.527588, elapsed=357.80s, remaining=999.56s.\n",
      "Batch 3500: loss=0.529698, elapsed=368.29s, remaining=988.97s.\n",
      "Batch 3600: loss=0.529362, elapsed=379.14s, remaining=979.32s.\n",
      "Batch 3700: loss=0.530034, elapsed=389.57s, remaining=968.55s.\n",
      "Batch 3800: loss=0.528670, elapsed=400.04s, remaining=957.92s.\n",
      "Batch 3900: loss=0.528837, elapsed=410.53s, remaining=947.33s.\n",
      "Batch 4000: loss=0.528404, elapsed=421.02s, remaining=936.70s.\n",
      "Batch 4100: loss=0.528746, elapsed=431.58s, remaining=926.23s.\n",
      "Batch 4200: loss=0.528693, elapsed=442.21s, remaining=915.94s.\n",
      "Batch 4300: loss=0.528874, elapsed=452.64s, remaining=905.21s.\n",
      "Batch 4400: loss=0.528513, elapsed=463.00s, remaining=894.33s.\n",
      "Batch 4500: loss=0.528162, elapsed=473.67s, remaining=884.08s.\n",
      "Batch 4600: loss=0.527600, elapsed=484.18s, remaining=873.54s.\n",
      "Batch 4700: loss=0.527101, elapsed=494.94s, remaining=863.46s.\n",
      "Batch 4800: loss=0.526951, elapsed=505.31s, remaining=852.63s.\n",
      "Batch 4900: loss=0.527621, elapsed=516.06s, remaining=842.49s.\n",
      "Batch 5000: loss=0.527835, elapsed=526.28s, remaining=831.44s.\n",
      "Batch 5100: loss=0.527623, elapsed=536.90s, remaining=821.08s.\n",
      "Batch 5200: loss=0.526762, elapsed=547.42s, remaining=810.56s.\n",
      "Batch 5300: loss=0.526706, elapsed=557.75s, remaining=799.75s.\n",
      "Batch 5400: loss=0.525208, elapsed=568.27s, remaining=789.21s.\n",
      "Batch 5500: loss=0.525002, elapsed=578.70s, remaining=778.58s.\n",
      "Batch 5600: loss=0.525926, elapsed=589.11s, remaining=767.91s.\n",
      "Batch 5700: loss=0.526502, elapsed=599.59s, remaining=757.35s.\n",
      "Batch 5800: loss=0.525830, elapsed=610.17s, remaining=746.92s.\n",
      "Batch 5900: loss=0.526064, elapsed=620.71s, remaining=736.43s.\n",
      "Batch 6000: loss=0.525895, elapsed=631.15s, remaining=725.83s.\n",
      "Batch 6100: loss=0.526513, elapsed=641.75s, remaining=715.40s.\n",
      "Batch 6200: loss=0.526182, elapsed=652.27s, remaining=704.90s.\n",
      "Batch 6300: loss=0.525440, elapsed=662.62s, remaining=694.18s.\n",
      "Batch 6400: loss=0.525104, elapsed=673.11s, remaining=683.63s.\n",
      "Batch 6500: loss=0.525361, elapsed=683.62s, remaining=673.11s.\n",
      "Batch 6600: loss=0.525263, elapsed=694.01s, remaining=662.46s.\n",
      "Batch 6700: loss=0.525277, elapsed=704.65s, remaining=652.08s.\n",
      "Batch 6800: loss=0.524435, elapsed=715.13s, remaining=641.53s.\n",
      "Batch 6900: loss=0.523937, elapsed=725.64s, remaining=631.01s.\n",
      "Batch 7000: loss=0.523816, elapsed=736.17s, remaining=620.50s.\n",
      "Batch 7100: loss=0.523585, elapsed=746.51s, remaining=609.85s.\n",
      "Batch 7200: loss=0.524331, elapsed=756.84s, remaining=599.17s.\n",
      "Batch 7300: loss=0.524359, elapsed=767.08s, remaining=588.44s.\n",
      "Batch 7400: loss=0.524189, elapsed=777.54s, remaining=577.90s.\n",
      "Batch 7500: loss=0.524491, elapsed=788.11s, remaining=567.45s.\n",
      "Batch 7600: loss=0.524844, elapsed=798.58s, remaining=556.92s.\n",
      "Batch 7700: loss=0.525659, elapsed=809.05s, remaining=546.38s.\n",
      "Batch 7800: loss=0.525485, elapsed=819.54s, remaining=535.88s.\n",
      "Batch 7900: loss=0.525362, elapsed=830.03s, remaining=525.36s.\n",
      "Batch 8000: loss=0.524943, elapsed=840.61s, remaining=514.90s.\n",
      "Batch 8100: loss=0.524721, elapsed=850.94s, remaining=504.29s.\n",
      "Batch 8200: loss=0.524536, elapsed=861.39s, remaining=493.74s.\n",
      "Batch 8300: loss=0.523871, elapsed=872.04s, remaining=483.33s.\n",
      "Batch 8400: loss=0.523510, elapsed=882.70s, remaining=472.93s.\n",
      "Batch 8500: loss=0.523886, elapsed=893.41s, remaining=462.53s.\n",
      "Batch 8600: loss=0.523917, elapsed=904.12s, remaining=452.11s.\n",
      "Batch 8700: loss=0.523763, elapsed=914.65s, remaining=441.62s.\n",
      "Batch 8800: loss=0.523821, elapsed=925.13s, remaining=431.09s.\n",
      "Batch 8900: loss=0.523882, elapsed=935.48s, remaining=420.50s.\n",
      "Batch 9000: loss=0.523942, elapsed=946.28s, remaining=410.12s.\n",
      "Batch 9100: loss=0.524208, elapsed=956.59s, remaining=399.53s.\n",
      "Batch 9200: loss=0.523414, elapsed=966.97s, remaining=388.97s.\n",
      "Batch 9300: loss=0.523529, elapsed=977.36s, remaining=378.41s.\n",
      "Batch 9400: loss=0.523187, elapsed=987.84s, remaining=367.87s.\n",
      "Batch 9500: loss=0.523078, elapsed=998.12s, remaining=357.28s.\n",
      "Batch 9600: loss=0.522852, elapsed=1008.57s, remaining=346.76s.\n",
      "Batch 9700: loss=0.522938, elapsed=1018.85s, remaining=336.17s.\n",
      "Batch 9800: loss=0.522738, elapsed=1029.32s, remaining=325.67s.\n",
      "Batch 9900: loss=0.522740, elapsed=1039.82s, remaining=315.15s.\n",
      "Batch 10000: loss=0.522020, elapsed=1050.31s, remaining=304.64s.\n",
      "Batch 10100: loss=0.522250, elapsed=1060.76s, remaining=294.12s.\n",
      "Batch 10200: loss=0.521965, elapsed=1070.92s, remaining=283.54s.\n",
      "Batch 10300: loss=0.521977, elapsed=1081.54s, remaining=273.08s.\n",
      "Batch 10400: loss=0.521642, elapsed=1091.92s, remaining=262.55s.\n",
      "Batch 10500: loss=0.522164, elapsed=1102.18s, remaining=251.99s.\n",
      "Batch 10600: loss=0.522286, elapsed=1112.72s, remaining=241.50s.\n",
      "Batch 10700: loss=0.521915, elapsed=1123.16s, remaining=231.00s.\n",
      "Batch 10800: loss=0.521617, elapsed=1133.70s, remaining=220.52s.\n",
      "Batch 10900: loss=0.521748, elapsed=1143.90s, remaining=209.97s.\n",
      "Batch 11000: loss=0.521292, elapsed=1154.29s, remaining=199.46s.\n",
      "Batch 11100: loss=0.521388, elapsed=1164.74s, remaining=188.96s.\n",
      "Batch 11200: loss=0.521352, elapsed=1175.27s, remaining=178.47s.\n",
      "Batch 11300: loss=0.520702, elapsed=1185.97s, remaining=168.00s.\n",
      "Batch 11400: loss=0.520596, elapsed=1196.35s, remaining=157.48s.\n",
      "Batch 11500: loss=0.520372, elapsed=1206.98s, remaining=147.00s.\n",
      "Batch 11600: loss=0.520389, elapsed=1217.20s, remaining=136.48s.\n",
      "Batch 11700: loss=0.520121, elapsed=1227.42s, remaining=125.95s.\n",
      "Batch 11800: loss=0.520170, elapsed=1237.75s, remaining=115.44s.\n",
      "Batch 11900: loss=0.519788, elapsed=1248.34s, remaining=104.97s.\n",
      "Batch 12000: loss=0.519574, elapsed=1259.08s, remaining=94.50s.\n",
      "Batch 12100: loss=0.519986, elapsed=1269.56s, remaining=84.01s.\n",
      "Batch 12200: loss=0.519401, elapsed=1280.31s, remaining=73.53s.\n",
      "Batch 12300: loss=0.518818, elapsed=1290.56s, remaining=63.02s.\n",
      "Batch 12400: loss=0.518548, elapsed=1301.13s, remaining=52.53s.\n",
      "Batch 12500: loss=0.517816, elapsed=1311.50s, remaining=42.03s.\n",
      "Batch 12600: loss=0.517518, elapsed=1322.00s, remaining=31.54s.\n",
      "Batch 12700: loss=0.517125, elapsed=1332.72s, remaining=21.00s.\n",
      "Batch 12800: loss=0.517347, elapsed=1343.47s, remaining=10.46s.\n",
      "Batch 12900: loss=0.517275, elapsed=1353.89s, remaining=-0.03s.\n",
      "Batch 13000: loss=0.517364, elapsed=1364.51s, remaining=-10.53s.\n",
      "Batch 13100: loss=0.517180, elapsed=1375.20s, remaining=-21.04s.\n",
      "Batch 13200: loss=0.517016, elapsed=1385.44s, remaining=-31.52s.\n",
      "\n",
      "Training epoch took: 1394.41s\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.879045 \n",
      "\n",
      "Accuracy - Sentiment: 86.6%, Avg loss: 0.879045 \n",
      "\n",
      "Accuracy - Topic: 85.6%, Avg loss: 0.879045 \n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.387196, elapsed=10.44s, remaining=1321.91s.\n",
      "Batch 200: loss=0.377614, elapsed=21.21s, remaining=1339.32s.\n",
      "Batch 300: loss=0.376677, elapsed=31.50s, remaining=1317.99s.\n",
      "Batch 400: loss=0.375254, elapsed=41.84s, remaining=1303.54s.\n",
      "Batch 500: loss=0.379572, elapsed=52.36s, remaining=1295.42s.\n",
      "Batch 600: loss=0.384923, elapsed=62.93s, remaining=1287.68s.\n",
      "Batch 700: loss=0.388770, elapsed=73.34s, remaining=1276.22s.\n",
      "Batch 800: loss=0.384580, elapsed=83.87s, remaining=1266.89s.\n",
      "Batch 900: loss=0.385324, elapsed=94.42s, remaining=1257.55s.\n",
      "Batch 1000: loss=0.390581, elapsed=104.72s, remaining=1244.81s.\n",
      "Batch 1100: loss=0.393375, elapsed=115.27s, remaining=1235.28s.\n",
      "Batch 1200: loss=0.391923, elapsed=125.84s, remaining=1225.79s.\n",
      "Batch 1300: loss=0.395575, elapsed=136.42s, remaining=1216.31s.\n",
      "Batch 1400: loss=0.393995, elapsed=146.66s, remaining=1203.85s.\n",
      "Batch 1500: loss=0.390489, elapsed=157.18s, remaining=1193.71s.\n",
      "Batch 1600: loss=0.389809, elapsed=167.76s, remaining=1184.04s.\n",
      "Batch 1700: loss=0.390747, elapsed=178.06s, remaining=1172.36s.\n",
      "Batch 1800: loss=0.388991, elapsed=188.56s, remaining=1162.04s.\n",
      "Batch 1900: loss=0.389556, elapsed=198.94s, remaining=1151.04s.\n",
      "Batch 2000: loss=0.388610, elapsed=209.18s, remaining=1139.32s.\n",
      "Batch 2100: loss=0.387074, elapsed=219.64s, remaining=1128.90s.\n",
      "Batch 2200: loss=0.385779, elapsed=230.04s, remaining=1118.22s.\n",
      "Batch 2300: loss=0.384985, elapsed=240.74s, remaining=1108.96s.\n",
      "Batch 2400: loss=0.385427, elapsed=251.25s, remaining=1098.77s.\n",
      "Batch 2500: loss=0.385151, elapsed=261.60s, remaining=1087.79s.\n",
      "Batch 2600: loss=0.383214, elapsed=271.87s, remaining=1076.57s.\n",
      "Batch 2700: loss=0.383228, elapsed=282.50s, remaining=1066.79s.\n",
      "Batch 2800: loss=0.384066, elapsed=293.23s, remaining=1057.35s.\n",
      "Batch 2900: loss=0.384419, elapsed=303.88s, remaining=1047.51s.\n",
      "Batch 3000: loss=0.385754, elapsed=314.34s, remaining=1037.03s.\n",
      "Batch 3100: loss=0.386447, elapsed=324.73s, remaining=1026.27s.\n",
      "Batch 3200: loss=0.388214, elapsed=335.29s, remaining=1016.12s.\n",
      "Batch 3300: loss=0.387579, elapsed=345.63s, remaining=1005.26s.\n",
      "Batch 3400: loss=0.387234, elapsed=356.12s, remaining=994.81s.\n",
      "Batch 3500: loss=0.385880, elapsed=366.79s, remaining=984.88s.\n",
      "Batch 3600: loss=0.385278, elapsed=377.00s, remaining=973.71s.\n",
      "Batch 3700: loss=0.385538, elapsed=387.61s, remaining=963.60s.\n",
      "Batch 3800: loss=0.386055, elapsed=397.88s, remaining=952.64s.\n",
      "Batch 3900: loss=0.384715, elapsed=408.24s, remaining=941.89s.\n",
      "Batch 4000: loss=0.384660, elapsed=418.72s, remaining=931.49s.\n",
      "Batch 4100: loss=0.384074, elapsed=429.30s, remaining=921.25s.\n",
      "Batch 4200: loss=0.383437, elapsed=439.88s, remaining=911.05s.\n",
      "Batch 4300: loss=0.382763, elapsed=450.43s, remaining=900.74s.\n",
      "Batch 4400: loss=0.382920, elapsed=460.85s, remaining=890.17s.\n",
      "Batch 4500: loss=0.382929, elapsed=471.09s, remaining=879.26s.\n",
      "Batch 4600: loss=0.383852, elapsed=481.52s, remaining=868.72s.\n",
      "Batch 4700: loss=0.384790, elapsed=491.96s, remaining=858.20s.\n",
      "Batch 4800: loss=0.385620, elapsed=502.35s, remaining=847.56s.\n",
      "Batch 4900: loss=0.384942, elapsed=513.04s, remaining=837.51s.\n",
      "Batch 5000: loss=0.384462, elapsed=523.65s, remaining=827.27s.\n",
      "Batch 5100: loss=0.383448, elapsed=533.90s, remaining=816.47s.\n",
      "Batch 5200: loss=0.383441, elapsed=544.70s, remaining=806.51s.\n",
      "Batch 5300: loss=0.383006, elapsed=555.17s, remaining=796.03s.\n",
      "Batch 5400: loss=0.382892, elapsed=565.76s, remaining=785.69s.\n",
      "Batch 5500: loss=0.381781, elapsed=576.50s, remaining=775.58s.\n",
      "Batch 5600: loss=0.381469, elapsed=586.82s, remaining=764.89s.\n",
      "Batch 5700: loss=0.381447, elapsed=597.17s, remaining=754.23s.\n",
      "Batch 5800: loss=0.381546, elapsed=607.75s, remaining=743.90s.\n",
      "Batch 5900: loss=0.381182, elapsed=618.15s, remaining=733.32s.\n",
      "Batch 6000: loss=0.381096, elapsed=628.67s, remaining=722.90s.\n",
      "Batch 6100: loss=0.380947, elapsed=639.23s, remaining=712.51s.\n",
      "Batch 6200: loss=0.380956, elapsed=649.65s, remaining=701.98s.\n",
      "Batch 6300: loss=0.380584, elapsed=660.34s, remaining=691.75s.\n",
      "Batch 6400: loss=0.380756, elapsed=670.76s, remaining=681.20s.\n",
      "Batch 6500: loss=0.380895, elapsed=681.13s, remaining=670.61s.\n",
      "Batch 6600: loss=0.380875, elapsed=691.67s, remaining=660.20s.\n",
      "Batch 6700: loss=0.381257, elapsed=702.20s, remaining=649.77s.\n",
      "Batch 6800: loss=0.382328, elapsed=712.55s, remaining=639.18s.\n",
      "Batch 6900: loss=0.381955, elapsed=723.12s, remaining=628.79s.\n",
      "Batch 7000: loss=0.381155, elapsed=733.54s, remaining=618.26s.\n",
      "Batch 7100: loss=0.381112, elapsed=744.04s, remaining=607.79s.\n",
      "Batch 7200: loss=0.380774, elapsed=754.69s, remaining=597.42s.\n",
      "Batch 7300: loss=0.380017, elapsed=765.26s, remaining=587.02s.\n",
      "Batch 7400: loss=0.380033, elapsed=775.92s, remaining=576.69s.\n",
      "Batch 7500: loss=0.379737, elapsed=786.34s, remaining=566.16s.\n",
      "Batch 7600: loss=0.380179, elapsed=796.92s, remaining=555.75s.\n",
      "Batch 7700: loss=0.379532, elapsed=807.54s, remaining=545.38s.\n",
      "Batch 7800: loss=0.379713, elapsed=818.00s, remaining=534.88s.\n",
      "Batch 7900: loss=0.380399, elapsed=828.73s, remaining=524.56s.\n",
      "Batch 8000: loss=0.380129, elapsed=839.36s, remaining=514.16s.\n",
      "Batch 8100: loss=0.379968, elapsed=850.03s, remaining=503.78s.\n",
      "Batch 8200: loss=0.380259, elapsed=860.43s, remaining=493.19s.\n",
      "Batch 8300: loss=0.380265, elapsed=870.99s, remaining=482.74s.\n",
      "Batch 8400: loss=0.380158, elapsed=881.26s, remaining=472.12s.\n",
      "Batch 8500: loss=0.380707, elapsed=891.45s, remaining=461.47s.\n",
      "Batch 8600: loss=0.380665, elapsed=901.72s, remaining=450.85s.\n",
      "Batch 8700: loss=0.380412, elapsed=912.45s, remaining=440.50s.\n",
      "Batch 8800: loss=0.380547, elapsed=923.07s, remaining=430.07s.\n",
      "Batch 8900: loss=0.380596, elapsed=933.38s, remaining=419.50s.\n",
      "Batch 9000: loss=0.379957, elapsed=943.74s, remaining=408.95s.\n",
      "Batch 9100: loss=0.379260, elapsed=954.15s, remaining=398.45s.\n",
      "Batch 9200: loss=0.379278, elapsed=964.63s, remaining=387.96s.\n",
      "Batch 9300: loss=0.379250, elapsed=975.13s, remaining=377.47s.\n",
      "Batch 9400: loss=0.379209, elapsed=985.61s, remaining=366.98s.\n",
      "Batch 9500: loss=0.378681, elapsed=996.00s, remaining=356.47s.\n",
      "Batch 9600: loss=0.378834, elapsed=1006.52s, remaining=345.99s.\n",
      "Batch 9700: loss=0.378597, elapsed=1017.10s, remaining=335.54s.\n",
      "Batch 9800: loss=0.378744, elapsed=1027.60s, remaining=325.06s.\n",
      "Batch 9900: loss=0.378595, elapsed=1037.94s, remaining=314.53s.\n",
      "Batch 10000: loss=0.378453, elapsed=1048.24s, remaining=303.98s.\n",
      "Batch 10100: loss=0.378399, elapsed=1058.75s, remaining=293.51s.\n",
      "Batch 10200: loss=0.378516, elapsed=1069.12s, remaining=282.99s.\n",
      "Batch 10300: loss=0.378427, elapsed=1079.78s, remaining=272.55s.\n",
      "Batch 10400: loss=0.378275, elapsed=1090.16s, remaining=262.03s.\n",
      "Batch 10500: loss=0.378557, elapsed=1100.62s, remaining=251.55s.\n",
      "Batch 10600: loss=0.378125, elapsed=1111.27s, remaining=241.10s.\n",
      "Batch 10700: loss=0.377824, elapsed=1121.70s, remaining=230.60s.\n",
      "Batch 10800: loss=0.377311, elapsed=1132.38s, remaining=220.17s.\n",
      "Batch 10900: loss=0.377210, elapsed=1142.85s, remaining=209.67s.\n",
      "Batch 11000: loss=0.376859, elapsed=1153.33s, remaining=199.19s.\n",
      "Batch 11100: loss=0.376701, elapsed=1163.71s, remaining=188.69s.\n",
      "Batch 11200: loss=0.376368, elapsed=1174.12s, remaining=178.19s.\n",
      "Batch 11300: loss=0.376468, elapsed=1184.57s, remaining=167.72s.\n",
      "Batch 11400: loss=0.376278, elapsed=1195.10s, remaining=157.22s.\n",
      "Batch 11500: loss=0.375952, elapsed=1205.65s, remaining=146.75s.\n",
      "Batch 11600: loss=0.375669, elapsed=1216.04s, remaining=136.25s.\n",
      "Batch 11700: loss=0.375728, elapsed=1226.53s, remaining=125.77s.\n",
      "Batch 11800: loss=0.375858, elapsed=1237.30s, remaining=115.31s.\n",
      "Batch 11900: loss=0.375852, elapsed=1247.75s, remaining=104.82s.\n",
      "Batch 12000: loss=0.375625, elapsed=1258.37s, remaining=94.35s.\n",
      "Batch 12100: loss=0.375226, elapsed=1268.71s, remaining=83.85s.\n",
      "Batch 12200: loss=0.375054, elapsed=1278.90s, remaining=73.34s.\n",
      "Batch 12300: loss=0.374970, elapsed=1289.28s, remaining=62.86s.\n",
      "Batch 12400: loss=0.374992, elapsed=1299.78s, remaining=52.38s.\n",
      "Batch 12500: loss=0.374871, elapsed=1310.27s, remaining=41.91s.\n",
      "Batch 12600: loss=0.374866, elapsed=1320.81s, remaining=31.42s.\n",
      "Batch 12700: loss=0.374982, elapsed=1331.57s, remaining=20.95s.\n",
      "Batch 12800: loss=0.375187, elapsed=1342.12s, remaining=10.47s.\n",
      "Batch 12900: loss=0.375285, elapsed=1352.67s, remaining=-0.02s.\n",
      "Batch 13000: loss=0.374836, elapsed=1362.90s, remaining=-10.50s.\n",
      "Batch 13100: loss=0.374842, elapsed=1373.58s, remaining=-20.99s.\n",
      "Batch 13200: loss=0.374591, elapsed=1383.93s, remaining=-31.47s.\n",
      "\n",
      "Training epoch took: 1392.77s\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.932391 \n",
      "\n",
      "Accuracy - Sentiment: 87.0%, Avg loss: 0.932391 \n",
      "\n",
      "Accuracy - Topic: 86.1%, Avg loss: 0.932391 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    timing_log = train_loop(train_dataloader, model,optimizer, scheduler, device, criterion_sent, criterion_topic, sentiment_var='sentiment',\n",
    "               topic_var='topic', timing_log=True)\n",
    "    eval_loop(eval_dataloader, model, device, criterion_sent, criterion_topic, sentiment_var='sentiment', topic_var='topic')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/manifesto_ContextScalePrediction_base/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 45.66s, Estimated remaining time: 50.64s\n",
      "Elapsed time: 91.96s, Estimated remaining time: 5.01s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_base = scale_func(test_dataloader, \n",
    "               model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  precision  recall  accuracy\n",
       "0   0.83       0.82    0.84      0.84\n",
       "1   0.86       0.86    0.86      0.86\n",
       "2   0.90       0.90    0.90      0.90\n",
       "3   0.89       0.88    0.90      0.90\n",
       "4   0.86       0.86    0.86      0.86\n",
       "5   0.83       0.83    0.83      0.83\n",
       "6   0.79       0.79    0.80      0.80\n",
       "7   0.87       0.86    0.87      0.87\n",
       "8   0.90       0.89    0.90      0.90\n",
       "9   0.90       0.91    0.89      0.89\n",
       "10  0.82       0.85    0.80      0.80\n",
       "11  0.83       0.84    0.82      0.82"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_base['res_table_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_base['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  precision  recall  accuracy\n",
       "0  0.91       0.90    0.92      0.92\n",
       "1  0.84       0.85    0.83      0.83\n",
       "2  0.82       0.83    0.81      0.81"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_base['res_table_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_base['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_base'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_base, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_base['res_table_sentiment'].to_csv('results/classification results/base_sentiment.csv', index=False)\n",
    "outputs_base['res_table_topic'].to_csv('results/classification results/base_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with simple flow of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_simple_flow=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion_sent = nn.CrossEntropyLoss()\n",
    "criterion_topic =  nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.744288, elapsed=10.74s, remaining=1360.48s.\n",
      "Batch 200: loss=3.730988, elapsed=21.25s, remaining=1343.60s.\n",
      "Batch 300: loss=3.673736, elapsed=31.78s, remaining=1331.60s.\n",
      "Batch 400: loss=3.594197, elapsed=42.23s, remaining=1317.30s.\n",
      "Batch 500: loss=3.517949, elapsed=52.50s, remaining=1300.04s.\n",
      "Batch 600: loss=3.440967, elapsed=62.89s, remaining=1287.82s.\n",
      "Batch 700: loss=3.344678, elapsed=73.20s, remaining=1274.38s.\n",
      "Batch 800: loss=3.247745, elapsed=83.57s, remaining=1262.78s.\n",
      "Batch 900: loss=3.152653, elapsed=94.15s, remaining=1254.37s.\n",
      "Batch 1000: loss=3.059615, elapsed=104.69s, remaining=1245.10s.\n",
      "Batch 1100: loss=2.976745, elapsed=115.22s, remaining=1235.47s.\n",
      "Batch 1200: loss=2.903974, elapsed=125.73s, remaining=1225.41s.\n",
      "Batch 1300: loss=2.833787, elapsed=136.33s, remaining=1216.14s.\n",
      "Batch 1400: loss=2.772936, elapsed=146.71s, remaining=1204.79s.\n",
      "Batch 1500: loss=2.717446, elapsed=157.29s, remaining=1195.18s.\n",
      "Batch 1600: loss=2.665250, elapsed=167.56s, remaining=1183.20s.\n",
      "Batch 1700: loss=2.619457, elapsed=178.00s, remaining=1172.57s.\n",
      "Batch 1800: loss=2.574827, elapsed=188.48s, remaining=1162.15s.\n",
      "Batch 1900: loss=2.531278, elapsed=198.80s, remaining=1150.81s.\n",
      "Batch 2000: loss=2.492716, elapsed=209.23s, remaining=1140.29s.\n",
      "Batch 2100: loss=2.454792, elapsed=219.86s, remaining=1130.71s.\n",
      "Batch 2200: loss=2.420473, elapsed=230.43s, remaining=1120.78s.\n",
      "Batch 2300: loss=2.387114, elapsed=240.95s, remaining=1110.62s.\n",
      "Batch 2400: loss=2.355715, elapsed=251.39s, remaining=1099.95s.\n",
      "Batch 2500: loss=2.327806, elapsed=261.84s, remaining=1089.38s.\n",
      "Batch 2600: loss=2.299806, elapsed=272.41s, remaining=1079.35s.\n",
      "Batch 2700: loss=2.273654, elapsed=282.83s, remaining=1068.69s.\n",
      "Batch 2800: loss=2.251333, elapsed=293.33s, remaining=1058.33s.\n",
      "Batch 2900: loss=2.228965, elapsed=303.96s, remaining=1048.39s.\n",
      "Batch 3000: loss=2.207362, elapsed=314.35s, remaining=1037.62s.\n",
      "Batch 3100: loss=2.183356, elapsed=324.90s, remaining=1027.37s.\n",
      "Batch 3200: loss=2.163678, elapsed=335.22s, remaining=1016.42s.\n",
      "Batch 3300: loss=2.142606, elapsed=345.71s, remaining=1005.98s.\n",
      "Batch 3400: loss=2.122381, elapsed=356.12s, remaining=995.28s.\n",
      "Batch 3500: loss=2.105213, elapsed=366.62s, remaining=984.90s.\n",
      "Batch 3600: loss=2.086222, elapsed=377.21s, remaining=974.73s.\n",
      "Batch 3700: loss=2.070500, elapsed=387.45s, remaining=963.65s.\n",
      "Batch 3800: loss=2.054549, elapsed=397.79s, remaining=952.85s.\n",
      "Batch 3900: loss=2.040443, elapsed=408.36s, remaining=942.65s.\n",
      "Batch 4000: loss=2.026612, elapsed=418.94s, remaining=932.44s.\n",
      "Batch 4100: loss=2.012752, elapsed=429.34s, remaining=921.84s.\n",
      "Batch 4200: loss=1.997733, elapsed=439.89s, remaining=911.55s.\n",
      "Batch 4300: loss=1.985679, elapsed=450.48s, remaining=901.32s.\n",
      "Batch 4400: loss=1.973687, elapsed=460.84s, remaining=890.62s.\n",
      "Batch 4500: loss=1.961988, elapsed=471.31s, remaining=880.13s.\n",
      "Batch 4600: loss=1.950153, elapsed=481.89s, remaining=869.83s.\n",
      "Batch 4700: loss=1.939200, elapsed=492.32s, remaining=859.30s.\n",
      "Batch 4800: loss=1.928758, elapsed=502.92s, remaining=849.03s.\n",
      "Batch 4900: loss=1.919196, elapsed=513.57s, remaining=838.86s.\n",
      "Batch 5000: loss=1.909537, elapsed=523.89s, remaining=828.13s.\n",
      "Batch 5100: loss=1.900720, elapsed=534.26s, remaining=817.49s.\n",
      "Batch 5200: loss=1.890826, elapsed=545.09s, remaining=807.56s.\n",
      "Batch 5300: loss=1.880919, elapsed=555.56s, remaining=797.06s.\n",
      "Batch 5400: loss=1.873404, elapsed=565.98s, remaining=786.51s.\n",
      "Batch 5500: loss=1.865294, elapsed=576.43s, remaining=775.98s.\n",
      "Batch 5600: loss=1.856628, elapsed=586.70s, remaining=765.22s.\n",
      "Batch 5700: loss=1.847276, elapsed=597.20s, remaining=754.78s.\n",
      "Batch 5800: loss=1.839414, elapsed=607.64s, remaining=744.26s.\n",
      "Batch 5900: loss=1.831129, elapsed=618.12s, remaining=733.79s.\n",
      "Batch 6000: loss=1.825106, elapsed=628.76s, remaining=723.49s.\n",
      "Batch 6100: loss=1.818414, elapsed=638.99s, remaining=712.75s.\n",
      "Batch 6200: loss=1.811004, elapsed=649.54s, remaining=702.36s.\n",
      "Batch 6300: loss=1.803888, elapsed=660.02s, remaining=691.90s.\n",
      "Batch 6400: loss=1.797867, elapsed=670.70s, remaining=681.62s.\n",
      "Batch 6500: loss=1.790132, elapsed=681.09s, remaining=671.06s.\n",
      "Batch 6600: loss=1.783190, elapsed=691.59s, remaining=660.59s.\n",
      "Batch 6700: loss=1.777378, elapsed=701.78s, remaining=649.86s.\n",
      "Batch 6800: loss=1.770972, elapsed=712.11s, remaining=639.24s.\n",
      "Batch 6900: loss=1.763492, elapsed=722.60s, remaining=628.79s.\n",
      "Batch 7000: loss=1.756894, elapsed=732.97s, remaining=618.25s.\n",
      "Batch 7100: loss=1.750671, elapsed=743.74s, remaining=608.02s.\n",
      "Batch 7200: loss=1.745194, elapsed=753.95s, remaining=597.33s.\n",
      "Batch 7300: loss=1.739354, elapsed=764.52s, remaining=586.95s.\n",
      "Batch 7400: loss=1.733107, elapsed=774.79s, remaining=576.32s.\n",
      "Batch 7500: loss=1.726796, elapsed=785.16s, remaining=565.77s.\n",
      "Batch 7600: loss=1.721493, elapsed=795.68s, remaining=555.34s.\n",
      "Batch 7700: loss=1.715633, elapsed=806.12s, remaining=544.84s.\n",
      "Batch 7800: loss=1.710764, elapsed=816.78s, remaining=534.52s.\n",
      "Batch 7900: loss=1.705837, elapsed=827.30s, remaining=524.08s.\n",
      "Batch 8000: loss=1.700713, elapsed=837.67s, remaining=513.54s.\n",
      "Batch 8100: loss=1.695489, elapsed=848.08s, remaining=503.04s.\n",
      "Batch 8200: loss=1.690310, elapsed=858.52s, remaining=492.57s.\n",
      "Batch 8300: loss=1.685455, elapsed=868.98s, remaining=482.10s.\n",
      "Batch 8400: loss=1.680271, elapsed=879.41s, remaining=471.61s.\n",
      "Batch 8500: loss=1.675267, elapsed=890.02s, remaining=461.22s.\n",
      "Batch 8600: loss=1.670894, elapsed=900.37s, remaining=450.68s.\n",
      "Batch 8700: loss=1.666836, elapsed=910.81s, remaining=440.20s.\n",
      "Batch 8800: loss=1.662358, elapsed=921.34s, remaining=429.77s.\n",
      "Batch 8900: loss=1.657766, elapsed=931.73s, remaining=419.27s.\n",
      "Batch 9000: loss=1.653414, elapsed=942.29s, remaining=408.83s.\n",
      "Batch 9100: loss=1.648760, elapsed=952.91s, remaining=398.42s.\n",
      "Batch 9200: loss=1.644143, elapsed=963.17s, remaining=387.86s.\n",
      "Batch 9300: loss=1.639076, elapsed=973.58s, remaining=377.36s.\n",
      "Batch 9400: loss=1.633987, elapsed=984.05s, remaining=366.89s.\n",
      "Batch 9500: loss=1.629985, elapsed=994.22s, remaining=356.32s.\n",
      "Batch 9600: loss=1.625648, elapsed=1004.65s, remaining=345.85s.\n",
      "Batch 9700: loss=1.621937, elapsed=1015.19s, remaining=335.42s.\n",
      "Batch 9800: loss=1.617787, elapsed=1025.73s, remaining=324.97s.\n",
      "Batch 9900: loss=1.613355, elapsed=1036.26s, remaining=314.52s.\n",
      "Batch 10000: loss=1.609302, elapsed=1047.00s, remaining=304.15s.\n",
      "Batch 10100: loss=1.605233, elapsed=1057.54s, remaining=293.69s.\n",
      "Batch 10200: loss=1.601236, elapsed=1067.85s, remaining=283.18s.\n",
      "Batch 10300: loss=1.597325, elapsed=1078.32s, remaining=272.71s.\n",
      "Batch 10400: loss=1.592677, elapsed=1089.09s, remaining=262.33s.\n",
      "Batch 10500: loss=1.589066, elapsed=1099.51s, remaining=251.85s.\n",
      "Batch 10600: loss=1.584858, elapsed=1109.71s, remaining=241.32s.\n",
      "Batch 10700: loss=1.581220, elapsed=1119.91s, remaining=230.79s.\n",
      "Batch 10800: loss=1.577701, elapsed=1130.46s, remaining=220.35s.\n",
      "Batch 10900: loss=1.574327, elapsed=1140.70s, remaining=209.84s.\n",
      "Batch 11000: loss=1.570811, elapsed=1151.17s, remaining=199.38s.\n",
      "Batch 11100: loss=1.567154, elapsed=1161.56s, remaining=188.91s.\n",
      "Batch 11200: loss=1.563796, elapsed=1172.13s, remaining=178.47s.\n",
      "Batch 11300: loss=1.560677, elapsed=1182.64s, remaining=168.01s.\n",
      "Batch 11400: loss=1.557879, elapsed=1193.09s, remaining=157.54s.\n",
      "Batch 11500: loss=1.554537, elapsed=1203.81s, remaining=147.12s.\n",
      "Batch 11600: loss=1.551737, elapsed=1214.48s, remaining=136.68s.\n",
      "Batch 11700: loss=1.548267, elapsed=1224.85s, remaining=126.20s.\n",
      "Batch 11800: loss=1.544685, elapsed=1235.46s, remaining=115.75s.\n",
      "Batch 11900: loss=1.542013, elapsed=1245.81s, remaining=105.27s.\n",
      "Batch 12000: loss=1.538905, elapsed=1256.40s, remaining=94.81s.\n",
      "Batch 12100: loss=1.535878, elapsed=1266.76s, remaining=84.33s.\n",
      "Batch 12200: loss=1.532273, elapsed=1277.22s, remaining=73.86s.\n",
      "Batch 12300: loss=1.528603, elapsed=1287.61s, remaining=63.39s.\n",
      "Batch 12400: loss=1.525737, elapsed=1298.06s, remaining=52.93s.\n",
      "Batch 12500: loss=1.523210, elapsed=1308.77s, remaining=42.47s.\n",
      "Batch 12600: loss=1.519682, elapsed=1319.19s, remaining=32.00s.\n",
      "Batch 12700: loss=1.516885, elapsed=1329.67s, remaining=21.52s.\n",
      "Batch 12800: loss=1.514311, elapsed=1339.84s, remaining=11.05s.\n",
      "Batch 12900: loss=1.511441, elapsed=1350.39s, remaining=0.58s.\n",
      "Batch 13000: loss=1.509298, elapsed=1360.95s, remaining=-9.89s.\n",
      "Batch 13100: loss=1.506901, elapsed=1371.54s, remaining=-20.36s.\n",
      "Batch 13200: loss=1.504231, elapsed=1382.14s, remaining=-30.83s.\n",
      "\n",
      "Training epoch took: 1391.03s\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 1.106851 \n",
      "\n",
      "Accuracy - Sentiment: 80.1%, Avg loss: 1.106851 \n",
      "\n",
      "Accuracy - Topic: 79.8%, Avg loss: 1.106851 \n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=1.042761, elapsed=10.24s, remaining=1297.31s.\n",
      "Batch 200: loss=1.015639, elapsed=20.66s, remaining=1305.40s.\n",
      "Batch 300: loss=1.008279, elapsed=31.15s, remaining=1303.86s.\n",
      "Batch 400: loss=0.999968, elapsed=41.61s, remaining=1297.24s.\n",
      "Batch 500: loss=0.994860, elapsed=52.02s, remaining=1288.05s.\n",
      "Batch 600: loss=0.995588, elapsed=62.42s, remaining=1277.90s.\n",
      "Batch 700: loss=0.996456, elapsed=72.54s, remaining=1262.84s.\n",
      "Batch 800: loss=0.998242, elapsed=82.92s, remaining=1252.88s.\n",
      "Batch 900: loss=0.997721, elapsed=93.50s, remaining=1245.67s.\n",
      "Batch 1000: loss=0.988746, elapsed=103.83s, remaining=1234.88s.\n",
      "Batch 1100: loss=0.989804, elapsed=114.23s, remaining=1224.85s.\n",
      "Batch 1200: loss=0.987173, elapsed=124.82s, remaining=1216.59s.\n",
      "Batch 1300: loss=0.994180, elapsed=135.25s, remaining=1206.59s.\n",
      "Batch 1400: loss=0.994964, elapsed=145.38s, remaining=1193.83s.\n",
      "Batch 1500: loss=0.995851, elapsed=155.95s, remaining=1184.88s.\n",
      "Batch 1600: loss=0.998564, elapsed=166.46s, remaining=1175.38s.\n",
      "Batch 1700: loss=1.002935, elapsed=177.25s, remaining=1167.63s.\n",
      "Batch 1800: loss=1.001976, elapsed=187.87s, remaining=1158.47s.\n",
      "Batch 1900: loss=1.006899, elapsed=198.48s, remaining=1149.10s.\n",
      "Batch 2000: loss=1.007146, elapsed=208.71s, remaining=1137.50s.\n",
      "Batch 2100: loss=1.006086, elapsed=219.23s, remaining=1127.51s.\n",
      "Batch 2200: loss=1.006904, elapsed=229.69s, remaining=1117.26s.\n",
      "Batch 2300: loss=1.009274, elapsed=240.12s, remaining=1106.78s.\n",
      "Batch 2400: loss=1.012025, elapsed=250.35s, remaining=1095.41s.\n",
      "Batch 2500: loss=1.011297, elapsed=260.62s, remaining=1084.38s.\n",
      "Batch 2600: loss=1.010348, elapsed=271.17s, remaining=1074.50s.\n",
      "Batch 2700: loss=1.007309, elapsed=281.65s, remaining=1064.27s.\n",
      "Batch 2800: loss=1.005625, elapsed=292.10s, remaining=1053.89s.\n",
      "Batch 2900: loss=1.003611, elapsed=302.33s, remaining=1042.75s.\n",
      "Batch 3000: loss=1.003218, elapsed=312.95s, remaining=1032.98s.\n",
      "Batch 3100: loss=1.003027, elapsed=323.35s, remaining=1022.44s.\n",
      "Batch 3200: loss=0.999544, elapsed=333.84s, remaining=1012.22s.\n",
      "Batch 3300: loss=1.001977, elapsed=344.37s, remaining=1002.08s.\n",
      "Batch 3400: loss=1.004147, elapsed=354.70s, remaining=991.31s.\n",
      "Batch 3500: loss=1.005051, elapsed=365.18s, remaining=981.04s.\n",
      "Batch 3600: loss=1.002903, elapsed=375.47s, remaining=970.25s.\n",
      "Batch 3700: loss=1.002117, elapsed=385.79s, remaining=959.54s.\n",
      "Batch 3800: loss=1.002256, elapsed=396.44s, remaining=949.66s.\n",
      "Batch 3900: loss=1.002342, elapsed=407.02s, remaining=939.57s.\n",
      "Batch 4000: loss=1.001953, elapsed=417.57s, remaining=929.38s.\n",
      "Batch 4100: loss=1.000494, elapsed=428.06s, remaining=919.07s.\n",
      "Batch 4200: loss=1.000089, elapsed=438.62s, remaining=908.88s.\n",
      "Batch 4300: loss=1.000122, elapsed=449.11s, remaining=898.57s.\n",
      "Batch 4400: loss=1.000777, elapsed=459.54s, remaining=888.10s.\n",
      "Batch 4500: loss=1.000127, elapsed=470.10s, remaining=877.89s.\n",
      "Batch 4600: loss=0.999963, elapsed=480.38s, remaining=867.14s.\n",
      "Batch 4700: loss=0.999604, elapsed=490.75s, remaining=856.59s.\n",
      "Batch 4800: loss=0.999159, elapsed=501.00s, remaining=845.82s.\n",
      "Batch 4900: loss=1.000015, elapsed=511.43s, remaining=835.36s.\n",
      "Batch 5000: loss=0.999558, elapsed=521.84s, remaining=824.91s.\n",
      "Batch 5100: loss=0.999515, elapsed=532.44s, remaining=814.75s.\n",
      "Batch 5200: loss=0.999069, elapsed=543.08s, remaining=804.50s.\n",
      "Batch 5300: loss=0.999246, elapsed=553.47s, remaining=793.98s.\n",
      "Batch 5400: loss=0.997891, elapsed=563.76s, remaining=783.32s.\n",
      "Batch 5500: loss=0.996002, elapsed=574.05s, remaining=772.67s.\n",
      "Batch 5600: loss=0.995535, elapsed=584.46s, remaining=762.20s.\n",
      "Batch 5700: loss=0.995743, elapsed=595.04s, remaining=751.95s.\n",
      "Batch 5800: loss=0.995452, elapsed=605.66s, remaining=741.74s.\n",
      "Batch 5900: loss=0.995278, elapsed=616.10s, remaining=731.30s.\n",
      "Batch 6000: loss=0.993609, elapsed=626.62s, remaining=720.97s.\n",
      "Batch 6100: loss=0.992065, elapsed=637.18s, remaining=710.64s.\n",
      "Batch 6200: loss=0.991953, elapsed=647.80s, remaining=700.40s.\n",
      "Batch 6300: loss=0.991058, elapsed=658.21s, remaining=689.91s.\n",
      "Batch 6400: loss=0.990117, elapsed=668.35s, remaining=679.15s.\n",
      "Batch 6500: loss=0.989707, elapsed=678.91s, remaining=668.84s.\n",
      "Batch 6600: loss=0.989614, elapsed=689.61s, remaining=658.64s.\n",
      "Batch 6700: loss=0.990724, elapsed=700.14s, remaining=648.28s.\n",
      "Batch 6800: loss=0.990450, elapsed=710.67s, remaining=637.92s.\n",
      "Batch 6900: loss=0.990675, elapsed=721.29s, remaining=627.62s.\n",
      "Batch 7000: loss=0.990830, elapsed=731.62s, remaining=617.07s.\n",
      "Batch 7100: loss=0.990840, elapsed=742.13s, remaining=606.67s.\n",
      "Batch 7200: loss=0.990118, elapsed=752.64s, remaining=596.28s.\n",
      "Batch 7300: loss=0.989422, elapsed=762.98s, remaining=585.74s.\n",
      "Batch 7400: loss=0.989276, elapsed=773.45s, remaining=575.30s.\n",
      "Batch 7500: loss=0.988946, elapsed=783.96s, remaining=564.88s.\n",
      "Batch 7600: loss=0.987501, elapsed=794.46s, remaining=554.48s.\n",
      "Batch 7700: loss=0.986751, elapsed=804.92s, remaining=544.04s.\n",
      "Batch 7800: loss=0.987574, elapsed=815.41s, remaining=533.61s.\n",
      "Batch 7900: loss=0.986309, elapsed=825.74s, remaining=523.06s.\n",
      "Batch 8000: loss=0.985597, elapsed=836.26s, remaining=512.66s.\n",
      "Batch 8100: loss=0.985354, elapsed=846.73s, remaining=502.21s.\n",
      "Batch 8200: loss=0.984170, elapsed=857.34s, remaining=491.85s.\n",
      "Batch 8300: loss=0.984323, elapsed=868.08s, remaining=481.56s.\n",
      "Batch 8400: loss=0.983986, elapsed=878.57s, remaining=471.12s.\n",
      "Batch 8500: loss=0.984082, elapsed=889.22s, remaining=460.76s.\n",
      "Batch 8600: loss=0.983553, elapsed=899.58s, remaining=450.26s.\n",
      "Batch 8700: loss=0.982595, elapsed=910.16s, remaining=439.87s.\n",
      "Batch 8800: loss=0.982031, elapsed=920.46s, remaining=429.32s.\n",
      "Batch 8900: loss=0.981763, elapsed=930.82s, remaining=418.81s.\n",
      "Batch 9000: loss=0.981925, elapsed=941.36s, remaining=408.41s.\n",
      "Batch 9100: loss=0.981793, elapsed=951.93s, remaining=397.99s.\n",
      "Batch 9200: loss=0.981046, elapsed=962.48s, remaining=387.57s.\n",
      "Batch 9300: loss=0.981144, elapsed=972.76s, remaining=377.05s.\n",
      "Batch 9400: loss=0.980645, elapsed=983.36s, remaining=366.65s.\n",
      "Batch 9500: loss=0.980213, elapsed=993.58s, remaining=356.10s.\n",
      "Batch 9600: loss=0.978927, elapsed=1004.08s, remaining=345.66s.\n",
      "Batch 9700: loss=0.978608, elapsed=1014.49s, remaining=335.18s.\n",
      "Batch 9800: loss=0.977877, elapsed=1024.99s, remaining=324.73s.\n",
      "Batch 9900: loss=0.977921, elapsed=1035.47s, remaining=314.29s.\n",
      "Batch 10000: loss=0.977309, elapsed=1045.90s, remaining=303.82s.\n",
      "Batch 10100: loss=0.977195, elapsed=1056.53s, remaining=293.41s.\n",
      "Batch 10200: loss=0.976433, elapsed=1066.92s, remaining=282.93s.\n",
      "Batch 10300: loss=0.976240, elapsed=1077.37s, remaining=272.46s.\n",
      "Batch 10400: loss=0.975747, elapsed=1088.04s, remaining=262.05s.\n",
      "Batch 10500: loss=0.975236, elapsed=1098.48s, remaining=251.59s.\n",
      "Batch 10600: loss=0.974616, elapsed=1109.05s, remaining=241.16s.\n",
      "Batch 10700: loss=0.973735, elapsed=1119.48s, remaining=230.69s.\n",
      "Batch 10800: loss=0.972950, elapsed=1130.12s, remaining=220.27s.\n",
      "Batch 10900: loss=0.972623, elapsed=1140.67s, remaining=209.82s.\n",
      "Batch 11000: loss=0.972401, elapsed=1151.18s, remaining=199.38s.\n",
      "Batch 11100: loss=0.971737, elapsed=1161.60s, remaining=188.90s.\n",
      "Batch 11200: loss=0.971763, elapsed=1172.31s, remaining=178.48s.\n",
      "Batch 11300: loss=0.971668, elapsed=1182.72s, remaining=168.00s.\n",
      "Batch 11400: loss=0.970981, elapsed=1193.24s, remaining=157.54s.\n",
      "Batch 11500: loss=0.971155, elapsed=1203.69s, remaining=147.07s.\n",
      "Batch 11600: loss=0.970680, elapsed=1214.27s, remaining=136.63s.\n",
      "Batch 11700: loss=0.970268, elapsed=1224.89s, remaining=126.19s.\n",
      "Batch 11800: loss=0.970357, elapsed=1235.37s, remaining=115.72s.\n",
      "Batch 11900: loss=0.970528, elapsed=1245.72s, remaining=105.25s.\n",
      "Batch 12000: loss=0.970040, elapsed=1256.25s, remaining=94.78s.\n",
      "Batch 12100: loss=0.969774, elapsed=1266.76s, remaining=84.33s.\n",
      "Batch 12200: loss=0.969171, elapsed=1277.24s, remaining=73.86s.\n",
      "Batch 12300: loss=0.968649, elapsed=1287.76s, remaining=63.40s.\n",
      "Batch 12400: loss=0.967535, elapsed=1298.18s, remaining=52.93s.\n",
      "Batch 12500: loss=0.967009, elapsed=1308.67s, remaining=42.46s.\n",
      "Batch 12600: loss=0.966942, elapsed=1319.14s, remaining=31.99s.\n",
      "Batch 12700: loss=0.966433, elapsed=1329.68s, remaining=21.53s.\n",
      "Batch 12800: loss=0.965717, elapsed=1340.36s, remaining=11.07s.\n",
      "Batch 12900: loss=0.965163, elapsed=1350.77s, remaining=0.60s.\n",
      "Batch 13000: loss=0.965227, elapsed=1361.08s, remaining=-9.87s.\n",
      "Batch 13100: loss=0.965173, elapsed=1371.67s, remaining=-20.34s.\n",
      "Batch 13200: loss=0.964477, elapsed=1382.54s, remaining=-30.81s.\n",
      "\n",
      "Training epoch took: 1391.47s\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.962095 \n",
      "\n",
      "Accuracy - Sentiment: 82.8%, Avg loss: 0.962095 \n",
      "\n",
      "Accuracy - Topic: 82.5%, Avg loss: 0.962095 \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.783456, elapsed=10.49s, remaining=1327.75s.\n",
      "Batch 200: loss=0.764701, elapsed=21.23s, remaining=1341.60s.\n",
      "Batch 300: loss=0.739926, elapsed=31.42s, remaining=1315.51s.\n",
      "Batch 400: loss=0.743335, elapsed=41.95s, remaining=1308.03s.\n",
      "Batch 500: loss=0.732111, elapsed=52.43s, remaining=1297.86s.\n",
      "Batch 600: loss=0.731825, elapsed=62.80s, remaining=1285.48s.\n",
      "Batch 700: loss=0.734425, elapsed=73.34s, remaining=1276.62s.\n",
      "Batch 800: loss=0.732929, elapsed=84.09s, remaining=1270.79s.\n",
      "Batch 900: loss=0.732980, elapsed=94.98s, remaining=1265.76s.\n",
      "Batch 1000: loss=0.734589, elapsed=105.30s, remaining=1252.55s.\n",
      "Batch 1100: loss=0.737197, elapsed=115.82s, remaining=1242.01s.\n",
      "Batch 1200: loss=0.735962, elapsed=126.13s, remaining=1229.48s.\n",
      "Batch 1300: loss=0.733293, elapsed=136.80s, remaining=1220.37s.\n",
      "Batch 1400: loss=0.734519, elapsed=147.07s, remaining=1207.86s.\n",
      "Batch 1500: loss=0.734003, elapsed=157.48s, remaining=1196.69s.\n",
      "Batch 1600: loss=0.736005, elapsed=167.85s, remaining=1185.28s.\n",
      "Batch 1700: loss=0.736356, elapsed=178.41s, remaining=1175.31s.\n",
      "Batch 1800: loss=0.737127, elapsed=188.75s, remaining=1163.82s.\n",
      "Batch 1900: loss=0.734622, elapsed=199.45s, remaining=1154.73s.\n",
      "Batch 2000: loss=0.732682, elapsed=209.81s, remaining=1143.60s.\n",
      "Batch 2100: loss=0.732982, elapsed=220.17s, remaining=1132.44s.\n",
      "Batch 2200: loss=0.734018, elapsed=230.72s, remaining=1122.35s.\n",
      "Batch 2300: loss=0.733224, elapsed=241.09s, remaining=1111.34s.\n",
      "Batch 2400: loss=0.732339, elapsed=251.37s, remaining=1100.00s.\n",
      "Batch 2500: loss=0.728824, elapsed=261.76s, remaining=1089.15s.\n",
      "Batch 2600: loss=0.729681, elapsed=272.18s, remaining=1078.49s.\n",
      "Batch 2700: loss=0.728849, elapsed=282.65s, remaining=1068.09s.\n",
      "Batch 2800: loss=0.728901, elapsed=293.06s, remaining=1057.42s.\n",
      "Batch 2900: loss=0.729540, elapsed=303.54s, remaining=1047.04s.\n",
      "Batch 3000: loss=0.729297, elapsed=314.13s, remaining=1036.94s.\n",
      "Batch 3100: loss=0.728663, elapsed=324.43s, remaining=1025.96s.\n",
      "Batch 3200: loss=0.728917, elapsed=335.00s, remaining=1015.82s.\n",
      "Batch 3300: loss=0.728745, elapsed=345.64s, remaining=1005.91s.\n",
      "Batch 3400: loss=0.728305, elapsed=356.26s, remaining=995.86s.\n",
      "Batch 3500: loss=0.728255, elapsed=366.82s, remaining=985.61s.\n",
      "Batch 3600: loss=0.727888, elapsed=377.16s, remaining=974.76s.\n",
      "Batch 3700: loss=0.727433, elapsed=387.77s, remaining=964.62s.\n",
      "Batch 3800: loss=0.728375, elapsed=398.19s, remaining=954.03s.\n",
      "Batch 3900: loss=0.726951, elapsed=408.76s, remaining=943.77s.\n",
      "Batch 4000: loss=0.725304, elapsed=419.10s, remaining=932.98s.\n",
      "Batch 4100: loss=0.725526, elapsed=429.59s, remaining=922.52s.\n",
      "Batch 4200: loss=0.725864, elapsed=439.90s, remaining=911.68s.\n",
      "Batch 4300: loss=0.725897, elapsed=450.48s, remaining=901.44s.\n",
      "Batch 4400: loss=0.726617, elapsed=460.90s, remaining=890.89s.\n",
      "Batch 4500: loss=0.725490, elapsed=471.25s, remaining=880.19s.\n",
      "Batch 4600: loss=0.726281, elapsed=481.83s, remaining=869.93s.\n",
      "Batch 4700: loss=0.726752, elapsed=492.20s, remaining=859.26s.\n",
      "Batch 4800: loss=0.726669, elapsed=502.87s, remaining=849.13s.\n",
      "Batch 4900: loss=0.724682, elapsed=513.14s, remaining=838.31s.\n",
      "Batch 5000: loss=0.725535, elapsed=523.67s, remaining=827.91s.\n",
      "Batch 5100: loss=0.724879, elapsed=534.15s, remaining=817.45s.\n",
      "Batch 5200: loss=0.724708, elapsed=544.45s, remaining=806.73s.\n",
      "Batch 5300: loss=0.724335, elapsed=555.12s, remaining=796.55s.\n",
      "Batch 5400: loss=0.723956, elapsed=565.84s, remaining=786.46s.\n",
      "Batch 5500: loss=0.724322, elapsed=576.22s, remaining=775.86s.\n",
      "Batch 5600: loss=0.724733, elapsed=586.59s, remaining=765.24s.\n",
      "Batch 5700: loss=0.725054, elapsed=596.96s, remaining=754.62s.\n",
      "Batch 5800: loss=0.724802, elapsed=607.27s, remaining=743.95s.\n",
      "Batch 5900: loss=0.725312, elapsed=617.78s, remaining=733.55s.\n",
      "Batch 6000: loss=0.725314, elapsed=628.34s, remaining=723.19s.\n",
      "Batch 6100: loss=0.725231, elapsed=638.71s, remaining=712.62s.\n",
      "Batch 6200: loss=0.724859, elapsed=649.24s, remaining=702.23s.\n",
      "Batch 6300: loss=0.724467, elapsed=659.74s, remaining=691.79s.\n",
      "Batch 6400: loss=0.724449, elapsed=670.21s, remaining=681.34s.\n",
      "Batch 6500: loss=0.723093, elapsed=680.53s, remaining=670.71s.\n",
      "Batch 6600: loss=0.722703, elapsed=690.96s, remaining=660.20s.\n",
      "Batch 6700: loss=0.722883, elapsed=701.41s, remaining=649.71s.\n",
      "Batch 6800: loss=0.723108, elapsed=711.82s, remaining=639.19s.\n",
      "Batch 6900: loss=0.722943, elapsed=722.25s, remaining=628.69s.\n",
      "Batch 7000: loss=0.722690, elapsed=732.71s, remaining=618.23s.\n",
      "Batch 7100: loss=0.722485, elapsed=743.10s, remaining=607.68s.\n",
      "Batch 7200: loss=0.722439, elapsed=753.80s, remaining=597.42s.\n",
      "Batch 7300: loss=0.722417, elapsed=764.33s, remaining=587.00s.\n",
      "Batch 7400: loss=0.721780, elapsed=774.55s, remaining=576.34s.\n",
      "Batch 7500: loss=0.721886, elapsed=784.97s, remaining=565.86s.\n",
      "Batch 7600: loss=0.721751, elapsed=795.52s, remaining=555.45s.\n",
      "Batch 7700: loss=0.721011, elapsed=806.17s, remaining=545.13s.\n",
      "Batch 7800: loss=0.721313, elapsed=816.56s, remaining=534.59s.\n",
      "Batch 7900: loss=0.720764, elapsed=827.11s, remaining=524.19s.\n",
      "Batch 8000: loss=0.720636, elapsed=837.61s, remaining=513.75s.\n",
      "Batch 8100: loss=0.721043, elapsed=847.93s, remaining=503.18s.\n",
      "Batch 8200: loss=0.720913, elapsed=858.57s, remaining=492.83s.\n",
      "Batch 8300: loss=0.721350, elapsed=868.96s, remaining=482.31s.\n",
      "Batch 8400: loss=0.720665, elapsed=879.28s, remaining=471.74s.\n",
      "Batch 8500: loss=0.719764, elapsed=889.94s, remaining=461.36s.\n",
      "Batch 8600: loss=0.719450, elapsed=900.55s, remaining=450.92s.\n",
      "Batch 8700: loss=0.719696, elapsed=911.19s, remaining=440.52s.\n",
      "Batch 8800: loss=0.718980, elapsed=921.56s, remaining=430.00s.\n",
      "Batch 8900: loss=0.718999, elapsed=932.05s, remaining=419.54s.\n",
      "Batch 9000: loss=0.718372, elapsed=942.55s, remaining=409.08s.\n",
      "Batch 9100: loss=0.718774, elapsed=952.75s, remaining=398.47s.\n",
      "Batch 9200: loss=0.718512, elapsed=963.39s, remaining=388.09s.\n",
      "Batch 9300: loss=0.717741, elapsed=974.04s, remaining=377.69s.\n",
      "Batch 9400: loss=0.717201, elapsed=984.62s, remaining=367.26s.\n",
      "Batch 9500: loss=0.716912, elapsed=994.94s, remaining=356.74s.\n",
      "Batch 9600: loss=0.716825, elapsed=1005.49s, remaining=346.28s.\n",
      "Batch 9700: loss=0.716887, elapsed=1015.84s, remaining=335.76s.\n",
      "Batch 9800: loss=0.716699, elapsed=1026.26s, remaining=325.27s.\n",
      "Batch 9900: loss=0.716665, elapsed=1036.66s, remaining=314.76s.\n",
      "Batch 10000: loss=0.716751, elapsed=1046.93s, remaining=304.23s.\n",
      "Batch 10100: loss=0.716824, elapsed=1057.35s, remaining=293.76s.\n",
      "Batch 10200: loss=0.716960, elapsed=1067.71s, remaining=283.26s.\n",
      "Batch 10300: loss=0.717236, elapsed=1078.16s, remaining=272.80s.\n",
      "Batch 10400: loss=0.717080, elapsed=1088.64s, remaining=262.33s.\n",
      "Batch 10500: loss=0.716881, elapsed=1098.93s, remaining=251.81s.\n",
      "Batch 10600: loss=0.716419, elapsed=1109.46s, remaining=241.36s.\n",
      "Batch 10700: loss=0.715555, elapsed=1119.81s, remaining=230.86s.\n",
      "Batch 10800: loss=0.715548, elapsed=1130.10s, remaining=220.37s.\n",
      "Batch 10900: loss=0.714477, elapsed=1140.56s, remaining=209.91s.\n",
      "Batch 11000: loss=0.713897, elapsed=1151.14s, remaining=199.47s.\n",
      "Batch 11100: loss=0.713563, elapsed=1161.47s, remaining=188.99s.\n",
      "Batch 11200: loss=0.713604, elapsed=1171.87s, remaining=178.51s.\n",
      "Batch 11300: loss=0.712746, elapsed=1182.62s, remaining=168.09s.\n",
      "Batch 11400: loss=0.712158, elapsed=1193.07s, remaining=157.63s.\n",
      "Batch 11500: loss=0.712814, elapsed=1203.62s, remaining=147.18s.\n",
      "Batch 11600: loss=0.712119, elapsed=1213.96s, remaining=136.70s.\n",
      "Batch 11700: loss=0.712024, elapsed=1224.31s, remaining=126.23s.\n",
      "Batch 11800: loss=0.711715, elapsed=1234.85s, remaining=115.78s.\n",
      "Batch 11900: loss=0.711711, elapsed=1245.52s, remaining=105.33s.\n",
      "Batch 12000: loss=0.711453, elapsed=1255.95s, remaining=94.85s.\n",
      "Batch 12100: loss=0.711441, elapsed=1266.44s, remaining=84.38s.\n",
      "Batch 12200: loss=0.711458, elapsed=1277.21s, remaining=73.95s.\n",
      "Batch 12300: loss=0.711684, elapsed=1287.86s, remaining=63.49s.\n",
      "Batch 12400: loss=0.711789, elapsed=1298.46s, remaining=53.03s.\n",
      "Batch 12500: loss=0.711711, elapsed=1309.09s, remaining=42.57s.\n",
      "Batch 12600: loss=0.711202, elapsed=1319.52s, remaining=32.10s.\n",
      "Batch 12700: loss=0.710817, elapsed=1329.93s, remaining=21.62s.\n",
      "Batch 12800: loss=0.710618, elapsed=1340.49s, remaining=11.15s.\n",
      "Batch 12900: loss=0.710542, elapsed=1351.16s, remaining=0.68s.\n",
      "Batch 13000: loss=0.710245, elapsed=1361.64s, remaining=-9.79s.\n",
      "Batch 13100: loss=0.710550, elapsed=1372.19s, remaining=-20.26s.\n",
      "Batch 13200: loss=0.710682, elapsed=1382.46s, remaining=-30.73s.\n",
      "\n",
      "Training epoch took: 1391.57s\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.886128 \n",
      "\n",
      "Accuracy - Sentiment: 85.4%, Avg loss: 0.886128 \n",
      "\n",
      "Accuracy - Topic: 84.6%, Avg loss: 0.886128 \n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.522886, elapsed=10.51s, remaining=1332.04s.\n",
      "Batch 200: loss=0.507571, elapsed=21.20s, remaining=1339.02s.\n",
      "Batch 300: loss=0.540103, elapsed=31.65s, remaining=1324.36s.\n",
      "Batch 400: loss=0.522761, elapsed=41.98s, remaining=1308.59s.\n",
      "Batch 500: loss=0.526367, elapsed=52.32s, remaining=1295.12s.\n",
      "Batch 600: loss=0.526467, elapsed=62.81s, remaining=1285.51s.\n",
      "Batch 700: loss=0.520447, elapsed=73.60s, remaining=1279.63s.\n",
      "Batch 800: loss=0.516938, elapsed=84.29s, remaining=1271.59s.\n",
      "Batch 900: loss=0.514220, elapsed=94.35s, remaining=1255.08s.\n",
      "Batch 1000: loss=0.516700, elapsed=105.03s, remaining=1247.34s.\n",
      "Batch 1100: loss=0.517936, elapsed=115.80s, remaining=1240.09s.\n",
      "Batch 1200: loss=0.525094, elapsed=126.30s, remaining=1229.61s.\n",
      "Batch 1300: loss=0.524790, elapsed=136.71s, remaining=1218.16s.\n",
      "Batch 1400: loss=0.523738, elapsed=147.07s, remaining=1206.46s.\n",
      "Batch 1500: loss=0.524605, elapsed=157.66s, remaining=1196.83s.\n",
      "Batch 1600: loss=0.525859, elapsed=167.98s, remaining=1185.06s.\n",
      "Batch 1700: loss=0.521558, elapsed=178.67s, remaining=1175.98s.\n",
      "Batch 1800: loss=0.522334, elapsed=189.07s, remaining=1164.95s.\n",
      "Batch 1900: loss=0.522509, elapsed=199.86s, remaining=1156.25s.\n",
      "Batch 2000: loss=0.521136, elapsed=210.29s, remaining=1145.30s.\n",
      "Batch 2100: loss=0.522292, elapsed=220.56s, remaining=1133.64s.\n",
      "Batch 2200: loss=0.520400, elapsed=230.96s, remaining=1122.59s.\n",
      "Batch 2300: loss=0.524164, elapsed=241.49s, remaining=1112.32s.\n",
      "Batch 2400: loss=0.525464, elapsed=252.04s, remaining=1102.06s.\n",
      "Batch 2500: loss=0.524038, elapsed=262.48s, remaining=1091.36s.\n",
      "Batch 2600: loss=0.523227, elapsed=276.30s, remaining=1093.85s.\n",
      "Batch 2700: loss=0.522369, elapsed=288.72s, remaining=1089.56s.\n",
      "Batch 2800: loss=0.519490, elapsed=299.75s, remaining=1080.11s.\n",
      "Batch 2900: loss=0.520499, elapsed=311.05s, remaining=1071.51s.\n",
      "Batch 3000: loss=0.521402, elapsed=322.15s, remaining=1061.99s.\n",
      "Batch 3100: loss=0.522590, elapsed=332.69s, remaining=1050.70s.\n",
      "Batch 3200: loss=0.520899, elapsed=343.30s, remaining=1039.61s.\n",
      "Batch 3300: loss=0.521869, elapsed=354.00s, remaining=1028.78s.\n",
      "Batch 3400: loss=0.522181, elapsed=364.43s, remaining=1017.26s.\n",
      "Batch 3500: loss=0.522454, elapsed=375.13s, remaining=1006.47s.\n",
      "Batch 3600: loss=0.524320, elapsed=385.72s, remaining=995.50s.\n",
      "Batch 3700: loss=0.524161, elapsed=396.21s, remaining=984.22s.\n",
      "Batch 3800: loss=0.525628, elapsed=407.05s, remaining=973.81s.\n",
      "Batch 3900: loss=0.525983, elapsed=417.42s, remaining=962.34s.\n",
      "Batch 4000: loss=0.526103, elapsed=427.97s, remaining=951.33s.\n",
      "Batch 4100: loss=0.526383, elapsed=438.77s, remaining=940.86s.\n",
      "Batch 4200: loss=0.525697, elapsed=449.38s, remaining=930.02s.\n",
      "Batch 4300: loss=0.525305, elapsed=460.00s, remaining=919.19s.\n",
      "Batch 4400: loss=0.524905, elapsed=470.58s, remaining=908.31s.\n",
      "Batch 4500: loss=0.526140, elapsed=481.24s, remaining=897.60s.\n",
      "Batch 4600: loss=0.526758, elapsed=491.59s, remaining=886.31s.\n",
      "Batch 4700: loss=0.526803, elapsed=502.42s, remaining=875.92s.\n",
      "Batch 4800: loss=0.526516, elapsed=512.79s, remaining=864.70s.\n",
      "Batch 4900: loss=0.527236, elapsed=523.21s, remaining=853.62s.\n",
      "Batch 5000: loss=0.528520, elapsed=533.76s, remaining=842.76s.\n",
      "Batch 5100: loss=0.527932, elapsed=544.34s, remaining=831.97s.\n",
      "Batch 5200: loss=0.527790, elapsed=554.62s, remaining=820.73s.\n",
      "Batch 5300: loss=0.528451, elapsed=565.38s, remaining=810.19s.\n",
      "Batch 5400: loss=0.528939, elapsed=576.13s, remaining=799.61s.\n",
      "Batch 5500: loss=0.528976, elapsed=586.72s, remaining=788.83s.\n",
      "Batch 5600: loss=0.528027, elapsed=597.19s, remaining=777.92s.\n",
      "Batch 5700: loss=0.527494, elapsed=607.66s, remaining=767.02s.\n",
      "Batch 5800: loss=0.527244, elapsed=618.11s, remaining=756.12s.\n",
      "Batch 5900: loss=0.527724, elapsed=628.61s, remaining=745.28s.\n",
      "Batch 6000: loss=0.528077, elapsed=639.16s, remaining=734.52s.\n",
      "Batch 6100: loss=0.527703, elapsed=649.74s, remaining=723.79s.\n",
      "Batch 6200: loss=0.527414, elapsed=660.30s, remaining=713.06s.\n",
      "Batch 6300: loss=0.526657, elapsed=671.34s, remaining=702.83s.\n",
      "Batch 6400: loss=0.526479, elapsed=682.00s, remaining=692.17s.\n",
      "Batch 6500: loss=0.526646, elapsed=692.58s, remaining=681.44s.\n",
      "Batch 6600: loss=0.526213, elapsed=702.95s, remaining=670.51s.\n",
      "Batch 6700: loss=0.525466, elapsed=713.76s, remaining=660.02s.\n",
      "Batch 6800: loss=0.525186, elapsed=724.38s, remaining=649.36s.\n",
      "Batch 6900: loss=0.524924, elapsed=734.98s, remaining=638.66s.\n",
      "Batch 7000: loss=0.524427, elapsed=745.99s, remaining=628.32s.\n",
      "Batch 7100: loss=0.523671, elapsed=756.83s, remaining=617.82s.\n",
      "Batch 7200: loss=0.523934, elapsed=767.42s, remaining=607.09s.\n",
      "Batch 7300: loss=0.523924, elapsed=778.26s, remaining=596.57s.\n",
      "Batch 7400: loss=0.524012, elapsed=789.36s, remaining=586.23s.\n",
      "Batch 7500: loss=0.523684, elapsed=800.32s, remaining=575.76s.\n",
      "Batch 7600: loss=0.523262, elapsed=811.44s, remaining=565.37s.\n",
      "Batch 7700: loss=0.523614, elapsed=822.78s, remaining=555.06s.\n",
      "Batch 7800: loss=0.523738, elapsed=833.66s, remaining=544.44s.\n",
      "Batch 7900: loss=0.523204, elapsed=844.64s, remaining=533.94s.\n",
      "Batch 8000: loss=0.523287, elapsed=855.35s, remaining=523.24s.\n",
      "Batch 8100: loss=0.523519, elapsed=865.82s, remaining=512.40s.\n",
      "Batch 8200: loss=0.524052, elapsed=876.27s, remaining=501.56s.\n",
      "Batch 8300: loss=0.523771, elapsed=886.67s, remaining=490.71s.\n",
      "Batch 8400: loss=0.523693, elapsed=897.31s, remaining=480.00s.\n",
      "Batch 8500: loss=0.523072, elapsed=907.99s, remaining=469.32s.\n",
      "Batch 8600: loss=0.522756, elapsed=918.57s, remaining=458.57s.\n",
      "Batch 8700: loss=0.522496, elapsed=929.09s, remaining=447.82s.\n",
      "Batch 8800: loss=0.522375, elapsed=939.67s, remaining=437.10s.\n",
      "Batch 8900: loss=0.521839, elapsed=950.61s, remaining=426.53s.\n",
      "Batch 9000: loss=0.521643, elapsed=961.16s, remaining=415.78s.\n",
      "Batch 9100: loss=0.521679, elapsed=971.70s, remaining=405.05s.\n",
      "Batch 9200: loss=0.521290, elapsed=982.32s, remaining=394.33s.\n",
      "Batch 9300: loss=0.521597, elapsed=993.95s, remaining=384.00s.\n",
      "Batch 9400: loss=0.521222, elapsed=1005.46s, remaining=373.57s.\n",
      "Batch 9500: loss=0.521280, elapsed=1016.92s, remaining=363.02s.\n",
      "Batch 9600: loss=0.520966, elapsed=1028.16s, remaining=352.46s.\n",
      "Batch 9700: loss=0.520535, elapsed=1038.81s, remaining=341.75s.\n",
      "Batch 9800: loss=0.520807, elapsed=1049.37s, remaining=331.00s.\n",
      "Batch 9900: loss=0.520331, elapsed=1060.34s, remaining=320.36s.\n",
      "Batch 10000: loss=0.520399, elapsed=1070.82s, remaining=309.59s.\n",
      "Batch 10100: loss=0.520085, elapsed=1081.40s, remaining=298.86s.\n",
      "Batch 10200: loss=0.520192, elapsed=1092.36s, remaining=288.21s.\n",
      "Batch 10300: loss=0.519867, elapsed=1103.24s, remaining=277.54s.\n",
      "Batch 10400: loss=0.519220, elapsed=1113.94s, remaining=266.83s.\n",
      "Batch 10500: loss=0.519153, elapsed=1124.60s, remaining=256.12s.\n",
      "Batch 10600: loss=0.519404, elapsed=1135.31s, remaining=245.43s.\n",
      "Batch 10700: loss=0.519404, elapsed=1146.79s, remaining=234.84s.\n",
      "Batch 10800: loss=0.518766, elapsed=1157.62s, remaining=224.16s.\n",
      "Batch 10900: loss=0.518545, elapsed=1168.60s, remaining=213.51s.\n",
      "Batch 11000: loss=0.518774, elapsed=1179.37s, remaining=202.80s.\n",
      "Batch 11100: loss=0.518329, elapsed=1189.91s, remaining=192.05s.\n",
      "Batch 11200: loss=0.518369, elapsed=1200.57s, remaining=181.34s.\n",
      "Batch 11300: loss=0.518561, elapsed=1211.40s, remaining=170.65s.\n",
      "Batch 11400: loss=0.518621, elapsed=1222.26s, remaining=159.97s.\n",
      "Batch 11500: loss=0.518615, elapsed=1233.12s, remaining=149.29s.\n",
      "Batch 11600: loss=0.518411, elapsed=1243.79s, remaining=138.58s.\n",
      "Batch 11700: loss=0.517850, elapsed=1254.86s, remaining=127.91s.\n",
      "Batch 11800: loss=0.517646, elapsed=1265.61s, remaining=117.20s.\n",
      "Batch 11900: loss=0.517399, elapsed=1276.31s, remaining=106.50s.\n",
      "Batch 12000: loss=0.517292, elapsed=1287.31s, remaining=95.82s.\n",
      "Batch 12100: loss=0.516590, elapsed=1297.95s, remaining=85.11s.\n",
      "Batch 12200: loss=0.516170, elapsed=1308.89s, remaining=74.41s.\n",
      "Batch 12300: loss=0.515793, elapsed=1319.68s, remaining=63.70s.\n",
      "Batch 12400: loss=0.515763, elapsed=1330.49s, remaining=52.99s.\n",
      "Batch 12500: loss=0.515915, elapsed=1341.32s, remaining=42.28s.\n",
      "Batch 12600: loss=0.515477, elapsed=1352.03s, remaining=31.57s.\n",
      "Batch 12700: loss=0.515089, elapsed=1362.86s, remaining=20.85s.\n",
      "Batch 12800: loss=0.514975, elapsed=1373.68s, remaining=10.14s.\n",
      "Batch 12900: loss=0.514853, elapsed=1384.20s, remaining=-0.59s.\n",
      "Batch 13000: loss=0.514535, elapsed=1394.90s, remaining=-11.31s.\n",
      "Batch 13100: loss=0.514514, elapsed=1405.85s, remaining=-22.02s.\n",
      "Batch 13200: loss=0.513874, elapsed=1416.49s, remaining=-32.74s.\n",
      "\n",
      "Training epoch took: 1425.57s\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.900495 \n",
      "\n",
      "Accuracy - Sentiment: 86.3%, Avg loss: 0.900495 \n",
      "\n",
      "Accuracy - Topic: 85.4%, Avg loss: 0.900495 \n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.415454, elapsed=10.95s, remaining=1387.62s.\n",
      "Batch 200: loss=0.401076, elapsed=21.77s, remaining=1375.98s.\n",
      "Batch 300: loss=0.391479, elapsed=32.40s, remaining=1356.94s.\n",
      "Batch 400: loss=0.386330, elapsed=43.02s, remaining=1341.71s.\n",
      "Batch 500: loss=0.381692, elapsed=53.84s, remaining=1333.34s.\n",
      "Batch 600: loss=0.388075, elapsed=64.69s, remaining=1324.84s.\n",
      "Batch 700: loss=0.384317, elapsed=75.65s, remaining=1317.47s.\n",
      "Batch 800: loss=0.381339, elapsed=86.53s, remaining=1308.14s.\n",
      "Batch 900: loss=0.379014, elapsed=97.30s, remaining=1296.96s.\n",
      "Batch 1000: loss=0.381980, elapsed=108.20s, remaining=1287.36s.\n",
      "Batch 1100: loss=0.380885, elapsed=119.25s, remaining=1279.31s.\n",
      "Batch 1200: loss=0.383136, elapsed=130.11s, remaining=1268.90s.\n",
      "Batch 1300: loss=0.384590, elapsed=140.88s, remaining=1257.51s.\n",
      "Batch 1400: loss=0.385652, elapsed=152.03s, remaining=1249.38s.\n",
      "Batch 1500: loss=0.384495, elapsed=163.03s, remaining=1239.69s.\n",
      "Batch 1600: loss=0.385537, elapsed=173.82s, remaining=1228.26s.\n",
      "Batch 1700: loss=0.385965, elapsed=184.60s, remaining=1216.83s.\n",
      "Batch 1800: loss=0.386072, elapsed=195.42s, remaining=1205.82s.\n",
      "Batch 1900: loss=0.384993, elapsed=206.35s, remaining=1195.50s.\n",
      "Batch 2000: loss=0.385081, elapsed=217.40s, remaining=1185.76s.\n",
      "Batch 2100: loss=0.384670, elapsed=228.35s, remaining=1175.40s.\n",
      "Batch 2200: loss=0.383914, elapsed=238.98s, remaining=1163.41s.\n",
      "Batch 2300: loss=0.384151, elapsed=249.92s, remaining=1152.93s.\n",
      "Batch 2400: loss=0.383334, elapsed=260.46s, remaining=1140.67s.\n",
      "Batch 2500: loss=0.381717, elapsed=271.21s, remaining=1129.40s.\n",
      "Batch 2600: loss=0.380445, elapsed=282.00s, remaining=1118.34s.\n",
      "Batch 2700: loss=0.380316, elapsed=292.55s, remaining=1106.38s.\n",
      "Batch 2800: loss=0.380126, elapsed=303.36s, remaining=1095.49s.\n",
      "Batch 2900: loss=0.379024, elapsed=314.28s, remaining=1084.94s.\n",
      "Batch 3000: loss=0.379777, elapsed=324.98s, remaining=1073.72s.\n",
      "Batch 3100: loss=0.379735, elapsed=335.71s, remaining=1062.57s.\n",
      "Batch 3200: loss=0.380009, elapsed=346.35s, remaining=1051.17s.\n",
      "Batch 3300: loss=0.379118, elapsed=356.91s, remaining=1039.59s.\n",
      "Batch 3400: loss=0.378951, elapsed=367.57s, remaining=1028.34s.\n",
      "Batch 3500: loss=0.379546, elapsed=378.41s, remaining=1017.61s.\n",
      "Batch 3600: loss=0.380176, elapsed=388.95s, remaining=1006.10s.\n",
      "Batch 3700: loss=0.380877, elapsed=399.59s, remaining=994.87s.\n",
      "Batch 3800: loss=0.381444, elapsed=410.48s, remaining=984.32s.\n",
      "Batch 3900: loss=0.380706, elapsed=420.99s, remaining=972.84s.\n",
      "Batch 4000: loss=0.380706, elapsed=431.84s, remaining=962.21s.\n",
      "Batch 4100: loss=0.379891, elapsed=442.38s, remaining=950.89s.\n",
      "Batch 4200: loss=0.379900, elapsed=453.06s, remaining=939.87s.\n",
      "Batch 4300: loss=0.380361, elapsed=463.62s, remaining=928.60s.\n",
      "Batch 4400: loss=0.380281, elapsed=474.48s, remaining=918.00s.\n",
      "Batch 4500: loss=0.378820, elapsed=485.40s, remaining=907.51s.\n",
      "Batch 4600: loss=0.379009, elapsed=496.28s, remaining=896.89s.\n",
      "Batch 4700: loss=0.378431, elapsed=507.06s, remaining=886.11s.\n",
      "Batch 4800: loss=0.379568, elapsed=517.89s, remaining=875.40s.\n",
      "Batch 4900: loss=0.379963, elapsed=528.68s, remaining=864.63s.\n",
      "Batch 5000: loss=0.379231, elapsed=539.53s, remaining=853.94s.\n",
      "Batch 5100: loss=0.378267, elapsed=550.62s, remaining=843.61s.\n",
      "Batch 5200: loss=0.378275, elapsed=561.29s, remaining=832.62s.\n",
      "Batch 5300: loss=0.378117, elapsed=571.90s, remaining=821.58s.\n",
      "Batch 5400: loss=0.378335, elapsed=582.55s, remaining=810.58s.\n",
      "Batch 5500: loss=0.378230, elapsed=593.48s, remaining=800.00s.\n",
      "Batch 5600: loss=0.378839, elapsed=604.36s, remaining=789.31s.\n",
      "Batch 5700: loss=0.377998, elapsed=615.09s, remaining=778.44s.\n",
      "Batch 5800: loss=0.378362, elapsed=625.96s, remaining=767.74s.\n",
      "Batch 5900: loss=0.377853, elapsed=636.72s, remaining=756.91s.\n",
      "Batch 6000: loss=0.377503, elapsed=647.43s, remaining=746.02s.\n",
      "Batch 6100: loss=0.377136, elapsed=658.12s, remaining=735.13s.\n",
      "Batch 6200: loss=0.376822, elapsed=668.81s, remaining=724.22s.\n",
      "Batch 6300: loss=0.376495, elapsed=679.56s, remaining=713.41s.\n",
      "Batch 6400: loss=0.377333, elapsed=689.99s, remaining=702.25s.\n",
      "Batch 6500: loss=0.376917, elapsed=700.64s, remaining=691.33s.\n",
      "Batch 6600: loss=0.376314, elapsed=711.39s, remaining=680.53s.\n",
      "Batch 6700: loss=0.377042, elapsed=722.03s, remaining=669.61s.\n",
      "Batch 6800: loss=0.376775, elapsed=732.90s, remaining=658.94s.\n",
      "Batch 6900: loss=0.376735, elapsed=743.69s, remaining=648.18s.\n",
      "Batch 7000: loss=0.376217, elapsed=754.75s, remaining=637.64s.\n",
      "Batch 7100: loss=0.375718, elapsed=765.68s, remaining=626.99s.\n",
      "Batch 7200: loss=0.375187, elapsed=776.46s, remaining=616.22s.\n",
      "Batch 7300: loss=0.375194, elapsed=787.36s, remaining=605.51s.\n",
      "Batch 7400: loss=0.374840, elapsed=798.22s, remaining=594.81s.\n",
      "Batch 7500: loss=0.375015, elapsed=809.13s, remaining=584.11s.\n",
      "Batch 7600: loss=0.375113, elapsed=820.16s, remaining=573.51s.\n",
      "Batch 7700: loss=0.374427, elapsed=831.13s, remaining=562.85s.\n",
      "Batch 7800: loss=0.374353, elapsed=841.91s, remaining=552.05s.\n",
      "Batch 7900: loss=0.374825, elapsed=852.58s, remaining=541.18s.\n",
      "Batch 8000: loss=0.375942, elapsed=863.43s, remaining=530.44s.\n",
      "Batch 8100: loss=0.376333, elapsed=874.18s, remaining=519.63s.\n",
      "Batch 8200: loss=0.377012, elapsed=885.05s, remaining=508.87s.\n",
      "Batch 8300: loss=0.375889, elapsed=896.09s, remaining=498.22s.\n",
      "Batch 8400: loss=0.376024, elapsed=906.72s, remaining=487.33s.\n",
      "Batch 8500: loss=0.376226, elapsed=917.33s, remaining=476.44s.\n",
      "Batch 8600: loss=0.376354, elapsed=928.20s, remaining=465.70s.\n",
      "Batch 8700: loss=0.376040, elapsed=938.72s, remaining=454.76s.\n",
      "Batch 8800: loss=0.375912, elapsed=949.33s, remaining=443.87s.\n",
      "Batch 8900: loss=0.376102, elapsed=960.31s, remaining=433.19s.\n",
      "Batch 9000: loss=0.375283, elapsed=970.84s, remaining=422.28s.\n",
      "Batch 9100: loss=0.374986, elapsed=981.58s, remaining=411.49s.\n",
      "Batch 9200: loss=0.375061, elapsed=992.48s, remaining=400.76s.\n",
      "Batch 9300: loss=0.374754, elapsed=1003.28s, remaining=389.98s.\n",
      "Batch 9400: loss=0.374510, elapsed=1014.07s, remaining=379.19s.\n",
      "Batch 9500: loss=0.374296, elapsed=1025.01s, remaining=368.46s.\n",
      "Batch 9600: loss=0.374925, elapsed=1035.80s, remaining=357.67s.\n",
      "Batch 9700: loss=0.375378, elapsed=1046.74s, remaining=346.94s.\n",
      "Batch 9800: loss=0.375212, elapsed=1057.51s, remaining=336.14s.\n",
      "Batch 9900: loss=0.375258, elapsed=1068.33s, remaining=325.35s.\n",
      "Batch 10000: loss=0.375120, elapsed=1079.05s, remaining=314.54s.\n",
      "Batch 10100: loss=0.375162, elapsed=1089.76s, remaining=303.74s.\n",
      "Batch 10200: loss=0.375076, elapsed=1100.56s, remaining=292.95s.\n",
      "Batch 10300: loss=0.375007, elapsed=1111.37s, remaining=282.17s.\n",
      "Batch 10400: loss=0.375160, elapsed=1122.29s, remaining=271.41s.\n",
      "Batch 10500: loss=0.374801, elapsed=1133.13s, remaining=260.63s.\n",
      "Batch 10600: loss=0.375144, elapsed=1144.17s, remaining=249.89s.\n",
      "Batch 10700: loss=0.374960, elapsed=1154.82s, remaining=239.08s.\n",
      "Batch 10800: loss=0.374840, elapsed=1165.53s, remaining=228.27s.\n",
      "Batch 10900: loss=0.375001, elapsed=1176.17s, remaining=217.45s.\n",
      "Batch 11000: loss=0.374859, elapsed=1186.79s, remaining=206.62s.\n",
      "Batch 11100: loss=0.374699, elapsed=1197.65s, remaining=195.84s.\n",
      "Batch 11200: loss=0.374524, elapsed=1208.42s, remaining=185.05s.\n",
      "Batch 11300: loss=0.374616, elapsed=1219.12s, remaining=174.26s.\n",
      "Batch 11400: loss=0.374986, elapsed=1229.99s, remaining=163.49s.\n",
      "Batch 11500: loss=0.374910, elapsed=1240.41s, remaining=152.64s.\n",
      "Batch 11600: loss=0.374811, elapsed=1251.22s, remaining=141.86s.\n",
      "Batch 11700: loss=0.375025, elapsed=1262.10s, remaining=131.08s.\n",
      "Batch 11800: loss=0.374869, elapsed=1272.77s, remaining=120.28s.\n",
      "Batch 11900: loss=0.374696, elapsed=1283.51s, remaining=109.49s.\n",
      "Batch 12000: loss=0.374639, elapsed=1294.27s, remaining=98.70s.\n",
      "Batch 12100: loss=0.374493, elapsed=1305.48s, remaining=87.96s.\n",
      "Batch 12200: loss=0.374133, elapsed=1316.25s, remaining=77.16s.\n",
      "Batch 12300: loss=0.373737, elapsed=1327.05s, remaining=66.36s.\n",
      "Batch 12400: loss=0.373743, elapsed=1337.94s, remaining=55.58s.\n",
      "Batch 12500: loss=0.373286, elapsed=1348.84s, remaining=44.80s.\n",
      "Batch 12600: loss=0.373175, elapsed=1359.59s, remaining=34.01s.\n",
      "Batch 12700: loss=0.373100, elapsed=1370.16s, remaining=23.21s.\n",
      "Batch 12800: loss=0.373385, elapsed=1380.90s, remaining=12.42s.\n",
      "Batch 12900: loss=0.373433, elapsed=1391.63s, remaining=1.63s.\n",
      "Batch 13000: loss=0.372970, elapsed=1402.48s, remaining=-9.16s.\n",
      "Batch 13100: loss=0.372966, elapsed=1413.33s, remaining=-19.94s.\n",
      "Batch 13200: loss=0.372860, elapsed=1423.93s, remaining=-30.72s.\n",
      "\n",
      "Training epoch took: 1433.11s\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.942752 \n",
      "\n",
      "Accuracy - Sentiment: 86.9%, Avg loss: 0.942752 \n",
      "\n",
      "Accuracy - Topic: 86.1%, Avg loss: 0.942752 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    timing_log = train_loop(train_dataloader, model,optimizer, scheduler, device, criterion_sent, criterion_topic, sentiment_var='sentiment',\n",
    "               topic_var='topic', timing_log=True)\n",
    "    eval_loop(eval_dataloader, model, device, criterion_sent, criterion_topic, sentiment_var='sentiment', topic_var='topic')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/manifesto_ContextScalePrediction_sf/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 45.56s, Estimated remaining time: 50.53s\n",
      "Elapsed time: 91.37s, Estimated remaining time: 4.98s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_sf = scale_func(test_dataloader, \n",
    "               model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  precision  recall  accuracy\n",
       "0   0.83       0.82    0.84      0.84\n",
       "1   0.86       0.85    0.87      0.87\n",
       "2   0.90       0.90    0.90      0.90\n",
       "3   0.89       0.90    0.89      0.89\n",
       "4   0.87       0.86    0.88      0.88\n",
       "5   0.83       0.83    0.83      0.83\n",
       "6   0.79       0.79    0.79      0.79\n",
       "7   0.86       0.86    0.86      0.86\n",
       "8   0.89       0.89    0.89      0.89\n",
       "9   0.89       0.89    0.89      0.89\n",
       "10  0.82       0.83    0.81      0.81\n",
       "11  0.83       0.84    0.82      0.82"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sf['res_table_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sf['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  precision  recall  accuracy\n",
       "0  0.90       0.90    0.91      0.91\n",
       "1  0.84       0.84    0.83      0.83\n",
       "2  0.82       0.82    0.82      0.82"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sf['res_table_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sf['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_sf'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_sf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_sf['res_table_sentiment'].to_csv('results/classification results/sf_sentiment.csv', index=False)\n",
    "outputs_sf['res_table_topic'].to_csv('results/classification results/sf_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with shared attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_shared_attention=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion_sent = nn.CrossEntropyLoss()\n",
    "criterion_topic =  nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.516776, elapsed=10.98s, remaining=1389.96s.\n",
      "Batch 200: loss=3.473603, elapsed=21.79s, remaining=1376.26s.\n",
      "Batch 300: loss=3.422491, elapsed=32.55s, remaining=1362.36s.\n",
      "Batch 400: loss=3.382882, elapsed=42.99s, remaining=1340.16s.\n",
      "Batch 500: loss=3.322140, elapsed=53.69s, remaining=1328.40s.\n",
      "Batch 600: loss=3.243311, elapsed=64.18s, remaining=1313.02s.\n",
      "Batch 700: loss=3.150156, elapsed=75.06s, remaining=1305.93s.\n",
      "Batch 800: loss=3.050005, elapsed=85.93s, remaining=1297.85s.\n",
      "Batch 900: loss=2.955178, elapsed=96.59s, remaining=1286.37s.\n",
      "Batch 1000: loss=2.869950, elapsed=107.43s, remaining=1277.18s.\n",
      "Batch 1100: loss=2.792143, elapsed=118.24s, remaining=1267.31s.\n",
      "Batch 1200: loss=2.721892, elapsed=128.98s, remaining=1256.67s.\n",
      "Batch 1300: loss=2.659945, elapsed=139.60s, remaining=1244.88s.\n",
      "Batch 1400: loss=2.600168, elapsed=150.26s, remaining=1233.63s.\n",
      "Batch 1500: loss=2.542570, elapsed=160.97s, remaining=1222.73s.\n",
      "Batch 1600: loss=2.491336, elapsed=171.52s, remaining=1210.78s.\n",
      "Batch 1700: loss=2.447396, elapsed=182.27s, remaining=1200.35s.\n",
      "Batch 1800: loss=2.403730, elapsed=193.03s, remaining=1190.01s.\n",
      "Batch 1900: loss=2.366077, elapsed=203.49s, remaining=1177.69s.\n",
      "Batch 2000: loss=2.329806, elapsed=214.14s, remaining=1166.72s.\n",
      "Batch 2100: loss=2.293377, elapsed=224.76s, remaining=1155.55s.\n",
      "Batch 2200: loss=2.259794, elapsed=235.51s, remaining=1145.10s.\n",
      "Batch 2300: loss=2.231316, elapsed=246.38s, remaining=1135.27s.\n",
      "Batch 2400: loss=2.204711, elapsed=257.05s, remaining=1124.43s.\n",
      "Batch 2500: loss=2.180166, elapsed=267.76s, remaining=1113.73s.\n",
      "Batch 2600: loss=2.155614, elapsed=278.63s, remaining=1103.73s.\n",
      "Batch 2700: loss=2.134576, elapsed=289.16s, remaining=1092.35s.\n",
      "Batch 2800: loss=2.113127, elapsed=299.91s, remaining=1081.80s.\n",
      "Batch 2900: loss=2.091884, elapsed=310.50s, remaining=1070.69s.\n",
      "Batch 3000: loss=2.072517, elapsed=321.06s, remaining=1059.50s.\n",
      "Batch 3100: loss=2.053177, elapsed=331.66s, remaining=1048.52s.\n",
      "Batch 3200: loss=2.037117, elapsed=342.28s, remaining=1037.62s.\n",
      "Batch 3300: loss=2.019595, elapsed=352.97s, remaining=1026.90s.\n",
      "Batch 3400: loss=2.005289, elapsed=363.53s, remaining=1015.83s.\n",
      "Batch 3500: loss=1.988562, elapsed=374.38s, remaining=1005.58s.\n",
      "Batch 3600: loss=1.973646, elapsed=384.84s, remaining=994.27s.\n",
      "Batch 3700: loss=1.960188, elapsed=395.74s, remaining=984.18s.\n",
      "Batch 3800: loss=1.945740, elapsed=406.57s, remaining=973.86s.\n",
      "Batch 3900: loss=1.931070, elapsed=417.15s, remaining=962.87s.\n",
      "Batch 4000: loss=1.919102, elapsed=427.58s, remaining=951.57s.\n",
      "Batch 4100: loss=1.905096, elapsed=438.22s, remaining=940.78s.\n",
      "Batch 4200: loss=1.892620, elapsed=448.99s, remaining=930.28s.\n",
      "Batch 4300: loss=1.880870, elapsed=459.71s, remaining=919.60s.\n",
      "Batch 4400: loss=1.869062, elapsed=470.33s, remaining=908.78s.\n",
      "Batch 4500: loss=1.858319, elapsed=480.83s, remaining=897.76s.\n",
      "Batch 4600: loss=1.847334, elapsed=491.39s, remaining=886.83s.\n",
      "Batch 4700: loss=1.837400, elapsed=502.10s, remaining=876.21s.\n",
      "Batch 4800: loss=1.828712, elapsed=512.69s, remaining=865.41s.\n",
      "Batch 4900: loss=1.819816, elapsed=523.42s, remaining=854.80s.\n",
      "Batch 5000: loss=1.809727, elapsed=534.36s, remaining=844.55s.\n",
      "Batch 5100: loss=1.801464, elapsed=544.95s, remaining=833.71s.\n",
      "Batch 5200: loss=1.793150, elapsed=555.69s, remaining=823.11s.\n",
      "Batch 5300: loss=1.786488, elapsed=566.64s, remaining=812.82s.\n",
      "Batch 5400: loss=1.779581, elapsed=577.35s, remaining=802.16s.\n",
      "Batch 5500: loss=1.771648, elapsed=588.06s, remaining=791.48s.\n",
      "Batch 5600: loss=1.764019, elapsed=598.61s, remaining=780.61s.\n",
      "Batch 5700: loss=1.756665, elapsed=609.23s, remaining=769.85s.\n",
      "Batch 5800: loss=1.748111, elapsed=619.98s, remaining=759.25s.\n",
      "Batch 5900: loss=1.742082, elapsed=630.66s, remaining=748.56s.\n",
      "Batch 6000: loss=1.733964, elapsed=641.46s, remaining=738.02s.\n",
      "Batch 6100: loss=1.727410, elapsed=651.85s, remaining=727.00s.\n",
      "Batch 6200: loss=1.721482, elapsed=662.52s, remaining=716.30s.\n",
      "Batch 6300: loss=1.715840, elapsed=673.31s, remaining=705.72s.\n",
      "Batch 6400: loss=1.710050, elapsed=683.63s, remaining=694.65s.\n",
      "Batch 6500: loss=1.704997, elapsed=694.19s, remaining=683.87s.\n",
      "Batch 6600: loss=1.699420, elapsed=704.98s, remaining=673.31s.\n",
      "Batch 6700: loss=1.694252, elapsed=715.68s, remaining=662.65s.\n",
      "Batch 6800: loss=1.688385, elapsed=726.28s, remaining=651.91s.\n",
      "Batch 6900: loss=1.683051, elapsed=736.91s, remaining=641.20s.\n",
      "Batch 7000: loss=1.677337, elapsed=747.74s, remaining=630.65s.\n",
      "Batch 7100: loss=1.672011, elapsed=758.65s, remaining=620.17s.\n",
      "Batch 7200: loss=1.666588, elapsed=769.29s, remaining=609.46s.\n",
      "Batch 7300: loss=1.661735, elapsed=779.89s, remaining=598.69s.\n",
      "Batch 7400: loss=1.656685, elapsed=790.47s, remaining=587.92s.\n",
      "Batch 7500: loss=1.651939, elapsed=801.10s, remaining=577.22s.\n",
      "Batch 7600: loss=1.647198, elapsed=811.58s, remaining=566.42s.\n",
      "Batch 7700: loss=1.642034, elapsed=822.09s, remaining=555.63s.\n",
      "Batch 7800: loss=1.637586, elapsed=832.94s, remaining=545.07s.\n",
      "Batch 7900: loss=1.633095, elapsed=843.85s, remaining=534.55s.\n",
      "Batch 8000: loss=1.628278, elapsed=854.43s, remaining=523.81s.\n",
      "Batch 8100: loss=1.623636, elapsed=865.05s, remaining=513.07s.\n",
      "Batch 8200: loss=1.618543, elapsed=875.63s, remaining=502.34s.\n",
      "Batch 8300: loss=1.614793, elapsed=886.32s, remaining=491.67s.\n",
      "Batch 8400: loss=1.610888, elapsed=896.99s, remaining=480.97s.\n",
      "Batch 8500: loss=1.606872, elapsed=907.86s, remaining=470.41s.\n",
      "Batch 8600: loss=1.602647, elapsed=918.48s, remaining=459.69s.\n",
      "Batch 8700: loss=1.598712, elapsed=929.29s, remaining=449.08s.\n",
      "Batch 8800: loss=1.595072, elapsed=939.96s, remaining=438.40s.\n",
      "Batch 8900: loss=1.590897, elapsed=950.81s, remaining=427.79s.\n",
      "Batch 9000: loss=1.587607, elapsed=961.96s, remaining=417.31s.\n",
      "Batch 9100: loss=1.583750, elapsed=972.61s, remaining=406.61s.\n",
      "Batch 9200: loss=1.580020, elapsed=983.14s, remaining=395.86s.\n",
      "Batch 9300: loss=1.576091, elapsed=993.73s, remaining=385.14s.\n",
      "Batch 9400: loss=1.572323, elapsed=1004.38s, remaining=374.44s.\n",
      "Batch 9500: loss=1.568693, elapsed=1015.21s, remaining=363.83s.\n",
      "Batch 9600: loss=1.565040, elapsed=1026.05s, remaining=353.20s.\n",
      "Batch 9700: loss=1.562408, elapsed=1036.53s, remaining=342.44s.\n",
      "Batch 9800: loss=1.558961, elapsed=1047.00s, remaining=331.68s.\n",
      "Batch 9900: loss=1.555542, elapsed=1057.93s, remaining=321.06s.\n",
      "Batch 10000: loss=1.551433, elapsed=1068.67s, remaining=310.38s.\n",
      "Batch 10100: loss=1.547974, elapsed=1079.24s, remaining=299.66s.\n",
      "Batch 10200: loss=1.544357, elapsed=1089.65s, remaining=288.89s.\n",
      "Batch 10300: loss=1.540368, elapsed=1100.49s, remaining=278.25s.\n",
      "Batch 10400: loss=1.537584, elapsed=1111.16s, remaining=267.56s.\n",
      "Batch 10500: loss=1.533693, elapsed=1121.60s, remaining=256.82s.\n",
      "Batch 10600: loss=1.530974, elapsed=1132.36s, remaining=246.16s.\n",
      "Batch 10700: loss=1.527629, elapsed=1142.99s, remaining=235.47s.\n",
      "Batch 10800: loss=1.524299, elapsed=1153.65s, remaining=224.78s.\n",
      "Batch 10900: loss=1.521309, elapsed=1164.25s, remaining=214.08s.\n",
      "Batch 11000: loss=1.517697, elapsed=1174.72s, remaining=203.35s.\n",
      "Batch 11100: loss=1.514695, elapsed=1185.53s, remaining=192.70s.\n",
      "Batch 11200: loss=1.511870, elapsed=1196.15s, remaining=182.01s.\n",
      "Batch 11300: loss=1.509013, elapsed=1207.14s, remaining=171.37s.\n",
      "Batch 11400: loss=1.506061, elapsed=1218.18s, remaining=160.74s.\n",
      "Batch 11500: loss=1.503035, elapsed=1228.79s, remaining=150.04s.\n",
      "Batch 11600: loss=1.500259, elapsed=1239.36s, remaining=139.33s.\n",
      "Batch 11700: loss=1.497364, elapsed=1250.17s, remaining=128.66s.\n",
      "Batch 11800: loss=1.494402, elapsed=1260.83s, remaining=117.96s.\n",
      "Batch 11900: loss=1.492006, elapsed=1271.38s, remaining=107.26s.\n",
      "Batch 12000: loss=1.488864, elapsed=1282.27s, remaining=96.60s.\n",
      "Batch 12100: loss=1.486447, elapsed=1293.00s, remaining=85.90s.\n",
      "Batch 12200: loss=1.483665, elapsed=1303.98s, remaining=75.25s.\n",
      "Batch 12300: loss=1.481094, elapsed=1314.65s, remaining=64.57s.\n",
      "Batch 12400: loss=1.477889, elapsed=1325.09s, remaining=53.87s.\n",
      "Batch 12500: loss=1.475101, elapsed=1335.77s, remaining=43.18s.\n",
      "Batch 12600: loss=1.472923, elapsed=1346.26s, remaining=32.48s.\n",
      "Batch 12700: loss=1.470186, elapsed=1356.92s, remaining=21.81s.\n",
      "Batch 12800: loss=1.467632, elapsed=1367.44s, remaining=11.12s.\n",
      "Batch 12900: loss=1.465148, elapsed=1378.32s, remaining=0.44s.\n",
      "Batch 13000: loss=1.462881, elapsed=1388.71s, remaining=-10.26s.\n",
      "Batch 13100: loss=1.460080, elapsed=1399.64s, remaining=-20.95s.\n",
      "Batch 13200: loss=1.457668, elapsed=1410.29s, remaining=-31.63s.\n",
      "\n",
      "Training epoch took: 1419.31s\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 1.130742 \n",
      "\n",
      "Accuracy - Sentiment: 79.5%, Avg loss: 1.130742 \n",
      "\n",
      "Accuracy - Topic: 79.7%, Avg loss: 1.130742 \n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.982962, elapsed=10.80s, remaining=1367.31s.\n",
      "Batch 200: loss=1.016640, elapsed=21.46s, remaining=1356.17s.\n",
      "Batch 300: loss=1.011054, elapsed=32.28s, remaining=1351.49s.\n",
      "Batch 400: loss=1.000701, elapsed=42.85s, remaining=1336.12s.\n",
      "Batch 500: loss=0.991148, elapsed=53.40s, remaining=1322.05s.\n",
      "Batch 600: loss=0.987340, elapsed=64.06s, remaining=1311.15s.\n",
      "Batch 700: loss=0.986638, elapsed=74.78s, remaining=1301.58s.\n",
      "Batch 800: loss=0.977998, elapsed=85.35s, remaining=1289.47s.\n",
      "Batch 900: loss=0.982555, elapsed=96.08s, remaining=1279.82s.\n",
      "Batch 1000: loss=0.987423, elapsed=106.74s, remaining=1269.09s.\n",
      "Batch 1100: loss=0.991669, elapsed=117.37s, remaining=1258.12s.\n",
      "Batch 1200: loss=0.987409, elapsed=128.05s, remaining=1247.67s.\n",
      "Batch 1300: loss=0.984721, elapsed=138.68s, remaining=1236.80s.\n",
      "Batch 1400: loss=0.988478, elapsed=149.27s, remaining=1225.62s.\n",
      "Batch 1500: loss=0.990743, elapsed=159.94s, remaining=1214.97s.\n",
      "Batch 1600: loss=0.989717, elapsed=170.66s, remaining=1204.89s.\n",
      "Batch 1700: loss=0.989623, elapsed=181.02s, remaining=1192.24s.\n",
      "Batch 1800: loss=0.992141, elapsed=191.75s, remaining=1182.16s.\n",
      "Batch 1900: loss=0.994403, elapsed=202.64s, remaining=1172.89s.\n",
      "Batch 2000: loss=0.995655, elapsed=213.35s, remaining=1162.53s.\n",
      "Batch 2100: loss=0.993528, elapsed=224.01s, remaining=1151.79s.\n",
      "Batch 2200: loss=0.994403, elapsed=234.75s, remaining=1141.52s.\n",
      "Batch 2300: loss=0.994122, elapsed=245.16s, remaining=1129.64s.\n",
      "Batch 2400: loss=0.994366, elapsed=255.92s, remaining=1119.45s.\n",
      "Batch 2500: loss=0.996872, elapsed=266.57s, remaining=1108.77s.\n",
      "Batch 2600: loss=0.997071, elapsed=277.17s, remaining=1097.87s.\n",
      "Batch 2700: loss=0.996135, elapsed=287.77s, remaining=1086.98s.\n",
      "Batch 2800: loss=0.996714, elapsed=298.27s, remaining=1075.79s.\n",
      "Batch 2900: loss=0.996725, elapsed=308.98s, remaining=1065.34s.\n",
      "Batch 3000: loss=0.994791, elapsed=319.63s, remaining=1054.69s.\n",
      "Batch 3100: loss=0.992521, elapsed=330.13s, remaining=1043.59s.\n",
      "Batch 3200: loss=0.992246, elapsed=340.70s, remaining=1032.69s.\n",
      "Batch 3300: loss=0.992485, elapsed=351.55s, remaining=1022.62s.\n",
      "Batch 3400: loss=0.990528, elapsed=362.43s, remaining=1012.60s.\n",
      "Batch 3500: loss=0.991562, elapsed=373.01s, remaining=1001.73s.\n",
      "Batch 3600: loss=0.992632, elapsed=383.72s, remaining=991.23s.\n",
      "Batch 3700: loss=0.990456, elapsed=394.22s, remaining=980.16s.\n",
      "Batch 3800: loss=0.987959, elapsed=404.88s, remaining=969.52s.\n",
      "Batch 3900: loss=0.987307, elapsed=415.41s, remaining=958.58s.\n",
      "Batch 4000: loss=0.988373, elapsed=426.01s, remaining=947.83s.\n",
      "Batch 4100: loss=0.989167, elapsed=436.68s, remaining=937.23s.\n",
      "Batch 4200: loss=0.989005, elapsed=447.38s, remaining=926.68s.\n",
      "Batch 4300: loss=0.989240, elapsed=458.05s, remaining=916.05s.\n",
      "Batch 4400: loss=0.989884, elapsed=468.85s, remaining=905.71s.\n",
      "Batch 4500: loss=0.990421, elapsed=479.51s, remaining=895.06s.\n",
      "Batch 4600: loss=0.989631, elapsed=490.21s, remaining=884.48s.\n",
      "Batch 4700: loss=0.989410, elapsed=500.75s, remaining=873.64s.\n",
      "Batch 4800: loss=0.989569, elapsed=511.75s, remaining=863.57s.\n",
      "Batch 4900: loss=0.987965, elapsed=522.43s, remaining=852.97s.\n",
      "Batch 5000: loss=0.987647, elapsed=533.01s, remaining=842.18s.\n",
      "Batch 5100: loss=0.986441, elapsed=543.72s, remaining=831.62s.\n",
      "Batch 5200: loss=0.985731, elapsed=554.34s, remaining=820.90s.\n",
      "Batch 5300: loss=0.984777, elapsed=564.84s, remaining=810.03s.\n",
      "Batch 5400: loss=0.985542, elapsed=575.54s, remaining=799.45s.\n",
      "Batch 5500: loss=0.985108, elapsed=586.44s, remaining=789.13s.\n",
      "Batch 5600: loss=0.984291, elapsed=596.87s, remaining=778.16s.\n",
      "Batch 5700: loss=0.985728, elapsed=607.57s, remaining=767.56s.\n",
      "Batch 5800: loss=0.985269, elapsed=618.25s, remaining=756.94s.\n",
      "Batch 5900: loss=0.985772, elapsed=628.80s, remaining=746.15s.\n",
      "Batch 6000: loss=0.985181, elapsed=639.40s, remaining=735.44s.\n",
      "Batch 6100: loss=0.983928, elapsed=650.34s, remaining=725.11s.\n",
      "Batch 6200: loss=0.982750, elapsed=660.69s, remaining=714.09s.\n",
      "Batch 6300: loss=0.982879, elapsed=671.58s, remaining=703.54s.\n",
      "Batch 6400: loss=0.981951, elapsed=682.46s, remaining=693.09s.\n",
      "Batch 6500: loss=0.983168, elapsed=693.03s, remaining=682.34s.\n",
      "Batch 6600: loss=0.982819, elapsed=703.71s, remaining=671.68s.\n",
      "Batch 6700: loss=0.982272, elapsed=714.37s, remaining=661.03s.\n",
      "Batch 6800: loss=0.983348, elapsed=725.29s, remaining=650.62s.\n",
      "Batch 6900: loss=0.983267, elapsed=735.77s, remaining=639.79s.\n",
      "Batch 7000: loss=0.982729, elapsed=746.25s, remaining=628.97s.\n",
      "Batch 7100: loss=0.981963, elapsed=756.92s, remaining=618.33s.\n",
      "Batch 7200: loss=0.982674, elapsed=767.66s, remaining=607.74s.\n",
      "Batch 7300: loss=0.982919, elapsed=778.46s, remaining=597.19s.\n",
      "Batch 7400: loss=0.982406, elapsed=789.19s, remaining=586.58s.\n",
      "Batch 7500: loss=0.980975, elapsed=799.69s, remaining=575.81s.\n",
      "Batch 7600: loss=0.980629, elapsed=810.53s, remaining=565.26s.\n",
      "Batch 7700: loss=0.979645, elapsed=821.40s, remaining=554.74s.\n",
      "Batch 7800: loss=0.978915, elapsed=832.13s, remaining=544.12s.\n",
      "Batch 7900: loss=0.978448, elapsed=842.85s, remaining=533.47s.\n",
      "Batch 8000: loss=0.977955, elapsed=853.42s, remaining=522.75s.\n",
      "Batch 8100: loss=0.977871, elapsed=864.08s, remaining=512.06s.\n",
      "Batch 8200: loss=0.977958, elapsed=874.59s, remaining=501.30s.\n",
      "Batch 8300: loss=0.978236, elapsed=885.15s, remaining=490.59s.\n",
      "Batch 8400: loss=0.978447, elapsed=895.69s, remaining=479.86s.\n",
      "Batch 8500: loss=0.978166, elapsed=906.42s, remaining=469.25s.\n",
      "Batch 8600: loss=0.978091, elapsed=917.32s, remaining=458.71s.\n",
      "Batch 8700: loss=0.977179, elapsed=928.02s, remaining=448.05s.\n",
      "Batch 8800: loss=0.977047, elapsed=938.79s, remaining=437.44s.\n",
      "Batch 8900: loss=0.976489, elapsed=949.74s, remaining=426.92s.\n",
      "Batch 9000: loss=0.975954, elapsed=960.07s, remaining=416.10s.\n",
      "Batch 9100: loss=0.975357, elapsed=970.90s, remaining=405.51s.\n",
      "Batch 9200: loss=0.975239, elapsed=981.52s, remaining=394.83s.\n",
      "Batch 9300: loss=0.974530, elapsed=992.70s, remaining=384.37s.\n",
      "Batch 9400: loss=0.974409, elapsed=1003.36s, remaining=373.71s.\n",
      "Batch 9500: loss=0.974353, elapsed=1014.06s, remaining=363.05s.\n",
      "Batch 9600: loss=0.973736, elapsed=1024.76s, remaining=352.39s.\n",
      "Batch 9700: loss=0.973845, elapsed=1035.45s, remaining=341.72s.\n",
      "Batch 9800: loss=0.973063, elapsed=1046.31s, remaining=331.12s.\n",
      "Batch 9900: loss=0.972645, elapsed=1057.12s, remaining=320.48s.\n",
      "Batch 10000: loss=0.971744, elapsed=1067.80s, remaining=309.81s.\n",
      "Batch 10100: loss=0.971334, elapsed=1078.51s, remaining=299.15s.\n",
      "Batch 10200: loss=0.971211, elapsed=1089.35s, remaining=288.53s.\n",
      "Batch 10300: loss=0.970747, elapsed=1099.94s, remaining=277.82s.\n",
      "Batch 10400: loss=0.970412, elapsed=1110.74s, remaining=267.19s.\n",
      "Batch 10500: loss=0.969151, elapsed=1121.48s, remaining=256.53s.\n",
      "Batch 10600: loss=0.968401, elapsed=1132.11s, remaining=245.83s.\n",
      "Batch 10700: loss=0.967421, elapsed=1142.75s, remaining=235.13s.\n",
      "Batch 10800: loss=0.967033, elapsed=1153.39s, remaining=224.44s.\n",
      "Batch 10900: loss=0.966885, elapsed=1164.05s, remaining=213.76s.\n",
      "Batch 11000: loss=0.966454, elapsed=1174.57s, remaining=203.06s.\n",
      "Batch 11100: loss=0.965853, elapsed=1185.03s, remaining=192.35s.\n",
      "Batch 11200: loss=0.965064, elapsed=1195.71s, remaining=181.68s.\n",
      "Batch 11300: loss=0.964817, elapsed=1206.28s, remaining=170.99s.\n",
      "Batch 11400: loss=0.964553, elapsed=1216.87s, remaining=160.30s.\n",
      "Batch 11500: loss=0.964190, elapsed=1227.69s, remaining=149.64s.\n",
      "Batch 11600: loss=0.963677, elapsed=1238.47s, remaining=138.99s.\n",
      "Batch 11700: loss=0.962978, elapsed=1249.27s, remaining=128.33s.\n",
      "Batch 11800: loss=0.962525, elapsed=1259.69s, remaining=117.62s.\n",
      "Batch 11900: loss=0.962138, elapsed=1270.48s, remaining=106.96s.\n",
      "Batch 12000: loss=0.962370, elapsed=1281.40s, remaining=96.30s.\n",
      "Batch 12100: loss=0.962129, elapsed=1292.08s, remaining=85.63s.\n",
      "Batch 12200: loss=0.961614, elapsed=1302.59s, remaining=74.93s.\n",
      "Batch 12300: loss=0.961396, elapsed=1313.39s, remaining=64.27s.\n",
      "Batch 12400: loss=0.961097, elapsed=1323.94s, remaining=53.58s.\n",
      "Batch 12500: loss=0.960539, elapsed=1334.53s, remaining=42.89s.\n",
      "Batch 12600: loss=0.960157, elapsed=1345.32s, remaining=32.22s.\n",
      "Batch 12700: loss=0.959749, elapsed=1355.84s, remaining=21.53s.\n",
      "Batch 12800: loss=0.959325, elapsed=1366.33s, remaining=10.85s.\n",
      "Batch 12900: loss=0.959006, elapsed=1377.05s, remaining=0.18s.\n",
      "Batch 13000: loss=0.958281, elapsed=1387.81s, remaining=-10.49s.\n",
      "Batch 13100: loss=0.957633, elapsed=1398.36s, remaining=-21.16s.\n",
      "Batch 13200: loss=0.956956, elapsed=1409.09s, remaining=-31.83s.\n",
      "\n",
      "Training epoch took: 1418.37s\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.930026 \n",
      "\n",
      "Accuracy - Sentiment: 83.6%, Avg loss: 0.930026 \n",
      "\n",
      "Accuracy - Topic: 82.9%, Avg loss: 0.930026 \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.714269, elapsed=10.58s, remaining=1339.31s.\n",
      "Batch 200: loss=0.709759, elapsed=21.36s, remaining=1349.17s.\n",
      "Batch 300: loss=0.722515, elapsed=31.98s, remaining=1338.78s.\n",
      "Batch 400: loss=0.723356, elapsed=42.51s, remaining=1325.07s.\n",
      "Batch 500: loss=0.719453, elapsed=53.25s, remaining=1318.01s.\n",
      "Batch 600: loss=0.715437, elapsed=63.91s, remaining=1308.08s.\n",
      "Batch 700: loss=0.720079, elapsed=74.54s, remaining=1297.44s.\n",
      "Batch 800: loss=0.720561, elapsed=85.25s, remaining=1287.97s.\n",
      "Batch 900: loss=0.726415, elapsed=96.08s, remaining=1279.83s.\n",
      "Batch 1000: loss=0.720883, elapsed=106.58s, remaining=1267.23s.\n",
      "Batch 1100: loss=0.718560, elapsed=117.25s, remaining=1256.77s.\n",
      "Batch 1200: loss=0.716961, elapsed=128.04s, remaining=1247.56s.\n",
      "Batch 1300: loss=0.715795, elapsed=138.55s, remaining=1235.55s.\n",
      "Batch 1400: loss=0.714054, elapsed=148.99s, remaining=1223.14s.\n",
      "Batch 1500: loss=0.714322, elapsed=159.41s, remaining=1210.82s.\n",
      "Batch 1600: loss=0.712035, elapsed=170.23s, remaining=1201.75s.\n",
      "Batch 1700: loss=0.717013, elapsed=180.96s, remaining=1191.78s.\n",
      "Batch 1800: loss=0.716698, elapsed=191.72s, remaining=1181.94s.\n",
      "Batch 1900: loss=0.715063, elapsed=202.50s, remaining=1172.11s.\n",
      "Batch 2000: loss=0.719503, elapsed=212.82s, remaining=1159.55s.\n",
      "Batch 2100: loss=0.719395, elapsed=223.38s, remaining=1148.42s.\n",
      "Batch 2200: loss=0.718958, elapsed=233.96s, remaining=1137.58s.\n",
      "Batch 2300: loss=0.717687, elapsed=244.65s, remaining=1127.22s.\n",
      "Batch 2400: loss=0.716428, elapsed=255.29s, remaining=1116.60s.\n",
      "Batch 2500: loss=0.716947, elapsed=265.91s, remaining=1105.89s.\n",
      "Batch 2600: loss=0.717773, elapsed=276.86s, remaining=1096.58s.\n",
      "Batch 2700: loss=0.718624, elapsed=287.58s, remaining=1086.22s.\n",
      "Batch 2800: loss=0.719388, elapsed=298.35s, remaining=1076.01s.\n",
      "Batch 2900: loss=0.717667, elapsed=308.93s, remaining=1065.12s.\n",
      "Batch 3000: loss=0.717307, elapsed=319.67s, remaining=1054.80s.\n",
      "Batch 3100: loss=0.717572, elapsed=330.26s, remaining=1043.95s.\n",
      "Batch 3200: loss=0.718628, elapsed=341.06s, remaining=1033.77s.\n",
      "Batch 3300: loss=0.717251, elapsed=352.56s, remaining=1025.26s.\n",
      "Batch 3400: loss=0.718129, elapsed=363.23s, remaining=1014.59s.\n",
      "Batch 3500: loss=0.718207, elapsed=374.01s, remaining=1004.22s.\n",
      "Batch 3600: loss=0.719012, elapsed=384.72s, remaining=993.61s.\n",
      "Batch 3700: loss=0.721061, elapsed=395.66s, remaining=983.62s.\n",
      "Batch 3800: loss=0.721265, elapsed=406.48s, remaining=973.29s.\n",
      "Batch 3900: loss=0.721285, elapsed=416.99s, remaining=962.20s.\n",
      "Batch 4000: loss=0.720489, elapsed=427.47s, remaining=951.06s.\n",
      "Batch 4100: loss=0.720925, elapsed=438.15s, remaining=940.39s.\n",
      "Batch 4200: loss=0.719771, elapsed=449.00s, remaining=930.06s.\n",
      "Batch 4300: loss=0.720496, elapsed=459.57s, remaining=919.13s.\n",
      "Batch 4400: loss=0.721152, elapsed=470.26s, remaining=908.46s.\n",
      "Batch 4500: loss=0.720883, elapsed=481.19s, remaining=898.26s.\n",
      "Batch 4600: loss=0.721397, elapsed=491.92s, remaining=887.64s.\n",
      "Batch 4700: loss=0.720283, elapsed=502.43s, remaining=876.63s.\n",
      "Batch 4800: loss=0.721144, elapsed=513.23s, remaining=866.11s.\n",
      "Batch 4900: loss=0.721388, elapsed=523.90s, remaining=855.36s.\n",
      "Batch 5000: loss=0.721964, elapsed=534.47s, remaining=844.47s.\n",
      "Batch 5100: loss=0.721396, elapsed=545.15s, remaining=833.77s.\n",
      "Batch 5200: loss=0.721754, elapsed=555.65s, remaining=822.82s.\n",
      "Batch 5300: loss=0.721288, elapsed=566.16s, remaining=811.87s.\n",
      "Batch 5400: loss=0.721275, elapsed=576.92s, remaining=801.34s.\n",
      "Batch 5500: loss=0.721988, elapsed=587.28s, remaining=790.20s.\n",
      "Batch 5600: loss=0.721414, elapsed=598.16s, remaining=779.81s.\n",
      "Batch 5700: loss=0.721167, elapsed=609.03s, remaining=769.36s.\n",
      "Batch 5800: loss=0.720405, elapsed=619.77s, remaining=758.74s.\n",
      "Batch 5900: loss=0.720458, elapsed=630.37s, remaining=747.95s.\n",
      "Batch 6000: loss=0.719986, elapsed=641.07s, remaining=737.29s.\n",
      "Batch 6100: loss=0.719212, elapsed=651.65s, remaining=726.48s.\n",
      "Batch 6200: loss=0.718994, elapsed=662.39s, remaining=715.87s.\n",
      "Batch 6300: loss=0.718927, elapsed=673.33s, remaining=705.45s.\n",
      "Batch 6400: loss=0.718393, elapsed=684.04s, remaining=694.80s.\n",
      "Batch 6500: loss=0.718102, elapsed=694.65s, remaining=684.03s.\n",
      "Batch 6600: loss=0.719004, elapsed=705.43s, remaining=673.44s.\n",
      "Batch 6700: loss=0.718721, elapsed=716.04s, remaining=662.67s.\n",
      "Batch 6800: loss=0.718503, elapsed=726.70s, remaining=651.97s.\n",
      "Batch 6900: loss=0.718495, elapsed=737.62s, remaining=641.52s.\n",
      "Batch 7000: loss=0.718383, elapsed=748.27s, remaining=630.79s.\n",
      "Batch 7100: loss=0.718673, elapsed=759.08s, remaining=620.20s.\n",
      "Batch 7200: loss=0.718288, elapsed=769.70s, remaining=609.45s.\n",
      "Batch 7300: loss=0.718747, elapsed=780.74s, remaining=599.03s.\n",
      "Batch 7400: loss=0.718418, elapsed=791.37s, remaining=588.29s.\n",
      "Batch 7500: loss=0.718020, elapsed=802.18s, remaining=577.68s.\n",
      "Batch 7600: loss=0.717982, elapsed=813.02s, remaining=567.09s.\n",
      "Batch 7700: loss=0.716984, elapsed=824.00s, remaining=556.60s.\n",
      "Batch 7800: loss=0.717249, elapsed=834.46s, remaining=545.73s.\n",
      "Batch 7900: loss=0.715970, elapsed=845.26s, remaining=535.10s.\n",
      "Batch 8000: loss=0.715424, elapsed=855.79s, remaining=524.26s.\n",
      "Batch 8100: loss=0.715022, elapsed=866.55s, remaining=513.60s.\n",
      "Batch 8200: loss=0.714832, elapsed=877.12s, remaining=502.82s.\n",
      "Batch 8300: loss=0.714677, elapsed=887.83s, remaining=492.13s.\n",
      "Batch 8400: loss=0.714456, elapsed=898.47s, remaining=481.40s.\n",
      "Batch 8500: loss=0.714469, elapsed=908.91s, remaining=470.58s.\n",
      "Batch 8600: loss=0.713613, elapsed=919.80s, remaining=459.99s.\n",
      "Batch 8700: loss=0.714105, elapsed=930.52s, remaining=449.31s.\n",
      "Batch 8800: loss=0.713872, elapsed=941.21s, remaining=438.62s.\n",
      "Batch 8900: loss=0.713767, elapsed=952.11s, remaining=428.01s.\n",
      "Batch 9000: loss=0.714514, elapsed=962.86s, remaining=417.32s.\n",
      "Batch 9100: loss=0.713968, elapsed=973.52s, remaining=406.57s.\n",
      "Batch 9200: loss=0.714009, elapsed=984.30s, remaining=395.92s.\n",
      "Batch 9300: loss=0.713274, elapsed=995.00s, remaining=385.23s.\n",
      "Batch 9400: loss=0.713385, elapsed=1005.68s, remaining=374.53s.\n",
      "Batch 9500: loss=0.713591, elapsed=1016.64s, remaining=363.92s.\n",
      "Batch 9600: loss=0.713371, elapsed=1027.21s, remaining=353.17s.\n",
      "Batch 9700: loss=0.713176, elapsed=1038.01s, remaining=342.52s.\n",
      "Batch 9800: loss=0.713013, elapsed=1048.60s, remaining=331.78s.\n",
      "Batch 9900: loss=0.713338, elapsed=1059.32s, remaining=321.09s.\n",
      "Batch 10000: loss=0.713106, elapsed=1070.00s, remaining=310.40s.\n",
      "Batch 10100: loss=0.712756, elapsed=1080.56s, remaining=299.65s.\n",
      "Batch 10200: loss=0.712613, elapsed=1091.06s, remaining=288.89s.\n",
      "Batch 10300: loss=0.712281, elapsed=1101.79s, remaining=278.22s.\n",
      "Batch 10400: loss=0.712035, elapsed=1112.39s, remaining=267.49s.\n",
      "Batch 10500: loss=0.711907, elapsed=1122.97s, remaining=256.78s.\n",
      "Batch 10600: loss=0.711928, elapsed=1133.46s, remaining=246.04s.\n",
      "Batch 10700: loss=0.711368, elapsed=1144.04s, remaining=235.33s.\n",
      "Batch 10800: loss=0.710896, elapsed=1154.65s, remaining=224.62s.\n",
      "Batch 10900: loss=0.710658, elapsed=1165.19s, remaining=213.90s.\n",
      "Batch 11000: loss=0.710496, elapsed=1175.78s, remaining=203.19s.\n",
      "Batch 11100: loss=0.710205, elapsed=1186.78s, remaining=192.57s.\n",
      "Batch 11200: loss=0.709722, elapsed=1197.41s, remaining=181.87s.\n",
      "Batch 11300: loss=0.708864, elapsed=1208.07s, remaining=171.16s.\n",
      "Batch 11400: loss=0.708139, elapsed=1218.71s, remaining=160.46s.\n",
      "Batch 11500: loss=0.707323, elapsed=1229.29s, remaining=149.76s.\n",
      "Batch 11600: loss=0.707113, elapsed=1239.82s, remaining=139.05s.\n",
      "Batch 11700: loss=0.707756, elapsed=1250.74s, remaining=128.40s.\n",
      "Batch 11800: loss=0.707195, elapsed=1261.49s, remaining=117.73s.\n",
      "Batch 11900: loss=0.707296, elapsed=1272.19s, remaining=107.02s.\n",
      "Batch 12000: loss=0.706997, elapsed=1282.84s, remaining=96.33s.\n",
      "Batch 12100: loss=0.706613, elapsed=1293.39s, remaining=85.63s.\n",
      "Batch 12200: loss=0.706863, elapsed=1304.17s, remaining=74.96s.\n",
      "Batch 12300: loss=0.706403, elapsed=1314.71s, remaining=64.26s.\n",
      "Batch 12400: loss=0.705768, elapsed=1325.38s, remaining=53.56s.\n",
      "Batch 12500: loss=0.705183, elapsed=1336.03s, remaining=42.88s.\n",
      "Batch 12600: loss=0.705020, elapsed=1346.69s, remaining=32.19s.\n",
      "Batch 12700: loss=0.704511, elapsed=1357.39s, remaining=21.50s.\n",
      "Batch 12800: loss=0.704425, elapsed=1367.94s, remaining=10.81s.\n",
      "Batch 12900: loss=0.704539, elapsed=1378.55s, remaining=0.11s.\n",
      "Batch 13000: loss=0.704285, elapsed=1389.40s, remaining=-10.57s.\n",
      "Batch 13100: loss=0.704395, elapsed=1399.72s, remaining=-21.26s.\n",
      "Batch 13200: loss=0.704507, elapsed=1410.27s, remaining=-31.94s.\n",
      "\n",
      "Training epoch took: 1419.30s\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.879545 \n",
      "\n",
      "Accuracy - Sentiment: 85.2%, Avg loss: 0.879545 \n",
      "\n",
      "Accuracy - Topic: 84.4%, Avg loss: 0.879545 \n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.527974, elapsed=10.58s, remaining=1340.16s.\n",
      "Batch 200: loss=0.524926, elapsed=21.42s, remaining=1353.08s.\n",
      "Batch 300: loss=0.525297, elapsed=32.25s, remaining=1350.48s.\n",
      "Batch 400: loss=0.535956, elapsed=42.90s, remaining=1337.57s.\n",
      "Batch 500: loss=0.527855, elapsed=53.61s, remaining=1327.28s.\n",
      "Batch 600: loss=0.532883, elapsed=64.23s, remaining=1314.73s.\n",
      "Batch 700: loss=0.531879, elapsed=74.76s, remaining=1301.19s.\n",
      "Batch 800: loss=0.528689, elapsed=85.23s, remaining=1287.53s.\n",
      "Batch 900: loss=0.534276, elapsed=95.83s, remaining=1276.41s.\n",
      "Batch 1000: loss=0.533546, elapsed=106.44s, remaining=1265.32s.\n",
      "Batch 1100: loss=0.533349, elapsed=117.14s, remaining=1255.41s.\n",
      "Batch 1200: loss=0.532974, elapsed=127.89s, remaining=1245.92s.\n",
      "Batch 1300: loss=0.528821, elapsed=138.73s, remaining=1237.11s.\n",
      "Batch 1400: loss=0.529273, elapsed=149.66s, remaining=1228.66s.\n",
      "Batch 1500: loss=0.526933, elapsed=160.30s, remaining=1217.54s.\n",
      "Batch 1600: loss=0.526067, elapsed=170.99s, remaining=1207.06s.\n",
      "Batch 1700: loss=0.528268, elapsed=181.74s, remaining=1196.88s.\n",
      "Batch 1800: loss=0.525589, elapsed=192.29s, remaining=1185.34s.\n",
      "Batch 1900: loss=0.524418, elapsed=203.02s, remaining=1174.97s.\n",
      "Batch 2000: loss=0.523344, elapsed=213.66s, remaining=1163.98s.\n",
      "Batch 2100: loss=0.522375, elapsed=224.34s, remaining=1153.35s.\n",
      "Batch 2200: loss=0.522090, elapsed=234.84s, remaining=1141.86s.\n",
      "Batch 2300: loss=0.519989, elapsed=245.73s, remaining=1132.21s.\n",
      "Batch 2400: loss=0.519724, elapsed=256.28s, remaining=1120.98s.\n",
      "Batch 2500: loss=0.518838, elapsed=266.83s, remaining=1109.77s.\n",
      "Batch 2600: loss=0.517647, elapsed=277.76s, remaining=1100.13s.\n",
      "Batch 2700: loss=0.517162, elapsed=288.44s, remaining=1089.53s.\n",
      "Batch 2800: loss=0.517946, elapsed=299.48s, remaining=1080.15s.\n",
      "Batch 2900: loss=0.516603, elapsed=310.11s, remaining=1069.23s.\n",
      "Batch 3000: loss=0.517491, elapsed=321.18s, remaining=1059.84s.\n",
      "Batch 3100: loss=0.517428, elapsed=331.73s, remaining=1048.64s.\n",
      "Batch 3200: loss=0.517163, elapsed=342.71s, remaining=1038.79s.\n",
      "Batch 3300: loss=0.517379, elapsed=353.29s, remaining=1027.68s.\n",
      "Batch 3400: loss=0.517179, elapsed=364.00s, remaining=1017.00s.\n",
      "Batch 3500: loss=0.515974, elapsed=374.81s, remaining=1006.61s.\n",
      "Batch 3600: loss=0.516156, elapsed=385.45s, remaining=995.75s.\n",
      "Batch 3700: loss=0.515555, elapsed=395.95s, remaining=984.51s.\n",
      "Batch 3800: loss=0.514617, elapsed=406.61s, remaining=973.75s.\n",
      "Batch 3900: loss=0.514411, elapsed=417.07s, remaining=962.49s.\n",
      "Batch 4000: loss=0.513865, elapsed=427.66s, remaining=951.59s.\n",
      "Batch 4100: loss=0.513323, elapsed=438.37s, remaining=940.95s.\n",
      "Batch 4200: loss=0.511929, elapsed=449.97s, remaining=931.93s.\n",
      "Batch 4300: loss=0.510651, elapsed=460.60s, remaining=921.02s.\n",
      "Batch 4400: loss=0.511214, elapsed=470.99s, remaining=909.69s.\n",
      "Batch 4500: loss=0.512279, elapsed=481.96s, remaining=899.50s.\n",
      "Batch 4600: loss=0.512061, elapsed=492.74s, remaining=888.93s.\n",
      "Batch 4700: loss=0.512662, elapsed=503.29s, remaining=877.94s.\n",
      "Batch 4800: loss=0.512275, elapsed=513.90s, remaining=867.07s.\n",
      "Batch 4900: loss=0.511798, elapsed=524.40s, remaining=856.05s.\n",
      "Batch 5000: loss=0.511973, elapsed=534.98s, remaining=845.16s.\n",
      "Batch 5100: loss=0.512575, elapsed=545.81s, remaining=834.69s.\n",
      "Batch 5200: loss=0.513401, elapsed=556.30s, remaining=823.68s.\n",
      "Batch 5300: loss=0.514314, elapsed=567.00s, remaining=812.99s.\n",
      "Batch 5400: loss=0.513237, elapsed=577.82s, remaining=802.49s.\n",
      "Batch 5500: loss=0.513040, elapsed=588.44s, remaining=791.68s.\n",
      "Batch 5600: loss=0.513764, elapsed=599.13s, remaining=780.98s.\n",
      "Batch 5700: loss=0.513741, elapsed=609.69s, remaining=770.14s.\n",
      "Batch 5800: loss=0.513969, elapsed=620.50s, remaining=759.58s.\n",
      "Batch 5900: loss=0.512486, elapsed=631.36s, remaining=749.09s.\n",
      "Batch 6000: loss=0.512927, elapsed=642.01s, remaining=738.35s.\n",
      "Batch 6100: loss=0.512450, elapsed=652.62s, remaining=727.56s.\n",
      "Batch 6200: loss=0.511847, elapsed=663.59s, remaining=717.19s.\n",
      "Batch 6300: loss=0.511647, elapsed=674.24s, remaining=706.45s.\n",
      "Batch 6400: loss=0.511503, elapsed=684.74s, remaining=695.53s.\n",
      "Batch 6500: loss=0.512260, elapsed=695.37s, remaining=684.76s.\n",
      "Batch 6600: loss=0.512670, elapsed=705.83s, remaining=673.85s.\n",
      "Batch 6700: loss=0.513254, elapsed=716.39s, remaining=663.03s.\n",
      "Batch 6800: loss=0.513285, elapsed=726.98s, remaining=652.24s.\n",
      "Batch 6900: loss=0.513675, elapsed=737.57s, remaining=641.47s.\n",
      "Batch 7000: loss=0.514193, elapsed=748.37s, remaining=630.88s.\n",
      "Batch 7100: loss=0.513996, elapsed=758.90s, remaining=620.06s.\n",
      "Batch 7200: loss=0.513856, elapsed=770.00s, remaining=609.71s.\n",
      "Batch 7300: loss=0.514351, elapsed=780.66s, remaining=599.00s.\n",
      "Batch 7400: loss=0.514139, elapsed=791.26s, remaining=588.24s.\n",
      "Batch 7500: loss=0.514027, elapsed=801.83s, remaining=577.48s.\n",
      "Batch 7600: loss=0.513976, elapsed=812.81s, remaining=566.99s.\n",
      "Batch 7700: loss=0.513738, elapsed=823.57s, remaining=556.34s.\n",
      "Batch 7800: loss=0.513207, elapsed=834.29s, remaining=545.66s.\n",
      "Batch 7900: loss=0.513344, elapsed=844.95s, remaining=534.95s.\n",
      "Batch 8000: loss=0.512774, elapsed=855.50s, remaining=524.18s.\n",
      "Batch 8100: loss=0.512642, elapsed=866.06s, remaining=513.40s.\n",
      "Batch 8200: loss=0.512475, elapsed=876.84s, remaining=502.77s.\n",
      "Batch 8300: loss=0.512801, elapsed=887.73s, remaining=492.19s.\n",
      "Batch 8400: loss=0.512405, elapsed=898.55s, remaining=481.57s.\n",
      "Batch 8500: loss=0.512040, elapsed=909.28s, remaining=470.90s.\n",
      "Batch 8600: loss=0.512461, elapsed=919.87s, remaining=460.13s.\n",
      "Batch 8700: loss=0.512952, elapsed=930.87s, remaining=449.59s.\n",
      "Batch 8800: loss=0.512321, elapsed=941.53s, remaining=438.86s.\n",
      "Batch 8900: loss=0.511966, elapsed=952.30s, remaining=428.20s.\n",
      "Batch 9000: loss=0.512054, elapsed=962.84s, remaining=417.42s.\n",
      "Batch 9100: loss=0.511700, elapsed=973.63s, remaining=406.77s.\n",
      "Batch 9200: loss=0.511729, elapsed=984.10s, remaining=395.97s.\n",
      "Batch 9300: loss=0.511843, elapsed=994.54s, remaining=385.17s.\n",
      "Batch 9400: loss=0.511547, elapsed=1005.16s, remaining=374.44s.\n",
      "Batch 9500: loss=0.511475, elapsed=1015.90s, remaining=363.76s.\n",
      "Batch 9600: loss=0.511282, elapsed=1026.61s, remaining=353.06s.\n",
      "Batch 9700: loss=0.511605, elapsed=1037.39s, remaining=342.39s.\n",
      "Batch 9800: loss=0.511894, elapsed=1047.83s, remaining=331.60s.\n",
      "Batch 9900: loss=0.511936, elapsed=1058.45s, remaining=320.90s.\n",
      "Batch 10000: loss=0.511554, elapsed=1069.13s, remaining=310.22s.\n",
      "Batch 10100: loss=0.511500, elapsed=1079.94s, remaining=299.56s.\n",
      "Batch 10200: loss=0.511811, elapsed=1090.47s, remaining=288.84s.\n",
      "Batch 10300: loss=0.511552, elapsed=1101.09s, remaining=278.13s.\n",
      "Batch 10400: loss=0.511709, elapsed=1111.83s, remaining=267.45s.\n",
      "Batch 10500: loss=0.512029, elapsed=1122.41s, remaining=256.74s.\n",
      "Batch 10600: loss=0.512237, elapsed=1133.14s, remaining=246.06s.\n",
      "Batch 10700: loss=0.512342, elapsed=1143.81s, remaining=235.38s.\n",
      "Batch 10800: loss=0.512291, elapsed=1154.43s, remaining=224.66s.\n",
      "Batch 10900: loss=0.512060, elapsed=1165.30s, remaining=214.01s.\n",
      "Batch 11000: loss=0.511783, elapsed=1176.15s, remaining=203.35s.\n",
      "Batch 11100: loss=0.511376, elapsed=1186.72s, remaining=192.63s.\n",
      "Batch 11200: loss=0.511225, elapsed=1197.52s, remaining=181.96s.\n",
      "Batch 11300: loss=0.511148, elapsed=1208.20s, remaining=171.26s.\n",
      "Batch 11400: loss=0.510508, elapsed=1218.68s, remaining=160.53s.\n",
      "Batch 11500: loss=0.510520, elapsed=1229.39s, remaining=149.84s.\n",
      "Batch 11600: loss=0.510340, elapsed=1240.18s, remaining=139.16s.\n",
      "Batch 11700: loss=0.510147, elapsed=1250.78s, remaining=128.46s.\n",
      "Batch 11800: loss=0.510187, elapsed=1261.57s, remaining=117.78s.\n",
      "Batch 11900: loss=0.509715, elapsed=1272.27s, remaining=107.08s.\n",
      "Batch 12000: loss=0.509518, elapsed=1283.01s, remaining=96.40s.\n",
      "Batch 12100: loss=0.509137, elapsed=1293.58s, remaining=85.71s.\n",
      "Batch 12200: loss=0.508778, elapsed=1304.30s, remaining=75.02s.\n",
      "Batch 12300: loss=0.508571, elapsed=1315.12s, remaining=64.34s.\n",
      "Batch 12400: loss=0.508477, elapsed=1325.75s, remaining=53.64s.\n",
      "Batch 12500: loss=0.508292, elapsed=1336.35s, remaining=42.94s.\n",
      "Batch 12600: loss=0.507983, elapsed=1347.05s, remaining=32.25s.\n",
      "Batch 12700: loss=0.508155, elapsed=1357.70s, remaining=21.56s.\n",
      "Batch 12800: loss=0.507840, elapsed=1368.60s, remaining=10.87s.\n",
      "Batch 12900: loss=0.507292, elapsed=1379.29s, remaining=0.18s.\n",
      "Batch 13000: loss=0.507585, elapsed=1389.92s, remaining=-10.52s.\n",
      "Batch 13100: loss=0.507186, elapsed=1400.72s, remaining=-21.21s.\n",
      "Batch 13200: loss=0.507030, elapsed=1411.50s, remaining=-31.90s.\n",
      "\n",
      "Training epoch took: 1420.70s\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.906226 \n",
      "\n",
      "Accuracy - Sentiment: 86.5%, Avg loss: 0.906226 \n",
      "\n",
      "Accuracy - Topic: 85.6%, Avg loss: 0.906226 \n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.387890, elapsed=10.72s, remaining=1358.21s.\n",
      "Batch 200: loss=0.364761, elapsed=21.40s, remaining=1351.59s.\n",
      "Batch 300: loss=0.358983, elapsed=31.99s, remaining=1338.63s.\n",
      "Batch 400: loss=0.360415, elapsed=42.98s, remaining=1339.76s.\n",
      "Batch 500: loss=0.358703, elapsed=53.66s, remaining=1328.16s.\n",
      "Batch 600: loss=0.358437, elapsed=64.25s, remaining=1315.10s.\n",
      "Batch 700: loss=0.364302, elapsed=74.91s, remaining=1303.85s.\n",
      "Batch 800: loss=0.363343, elapsed=85.42s, remaining=1290.23s.\n",
      "Batch 900: loss=0.365674, elapsed=96.15s, remaining=1280.46s.\n",
      "Batch 1000: loss=0.369162, elapsed=107.11s, remaining=1273.43s.\n",
      "Batch 1100: loss=0.370307, elapsed=117.84s, remaining=1263.03s.\n",
      "Batch 1200: loss=0.365227, elapsed=128.56s, remaining=1252.55s.\n",
      "Batch 1300: loss=0.366241, elapsed=139.15s, remaining=1240.87s.\n",
      "Batch 1400: loss=0.362797, elapsed=149.75s, remaining=1229.45s.\n",
      "Batch 1500: loss=0.363775, elapsed=160.24s, remaining=1217.23s.\n",
      "Batch 1600: loss=0.363865, elapsed=170.93s, remaining=1206.59s.\n",
      "Batch 1700: loss=0.366075, elapsed=181.69s, remaining=1196.47s.\n",
      "Batch 1800: loss=0.362983, elapsed=192.53s, remaining=1186.77s.\n",
      "Batch 1900: loss=0.362669, elapsed=203.10s, remaining=1175.32s.\n",
      "Batch 2000: loss=0.363659, elapsed=213.64s, remaining=1163.91s.\n",
      "Batch 2100: loss=0.365515, elapsed=224.27s, remaining=1152.95s.\n",
      "Batch 2200: loss=0.366544, elapsed=235.03s, remaining=1142.67s.\n",
      "Batch 2300: loss=0.366488, elapsed=245.68s, remaining=1131.82s.\n",
      "Batch 2400: loss=0.366968, elapsed=256.36s, remaining=1121.17s.\n",
      "Batch 2500: loss=0.368052, elapsed=267.33s, remaining=1111.69s.\n",
      "Batch 2600: loss=0.366915, elapsed=277.91s, remaining=1100.57s.\n",
      "Batch 2700: loss=0.366784, elapsed=288.66s, remaining=1090.15s.\n",
      "Batch 2800: loss=0.367932, elapsed=299.35s, remaining=1079.51s.\n",
      "Batch 2900: loss=0.366888, elapsed=309.76s, remaining=1067.86s.\n",
      "Batch 3000: loss=0.367284, elapsed=320.53s, remaining=1057.51s.\n",
      "Batch 3100: loss=0.367279, elapsed=330.97s, remaining=1046.04s.\n",
      "Batch 3200: loss=0.366086, elapsed=341.70s, remaining=1035.53s.\n",
      "Batch 3300: loss=0.365997, elapsed=352.47s, remaining=1025.13s.\n",
      "Batch 3400: loss=0.365728, elapsed=363.36s, remaining=1015.02s.\n",
      "Batch 3500: loss=0.366180, elapsed=373.99s, remaining=1004.22s.\n",
      "Batch 3600: loss=0.365336, elapsed=384.83s, remaining=993.99s.\n",
      "Batch 3700: loss=0.365131, elapsed=395.72s, remaining=983.81s.\n",
      "Batch 3800: loss=0.364915, elapsed=406.36s, remaining=972.98s.\n",
      "Batch 3900: loss=0.364214, elapsed=417.05s, remaining=962.32s.\n",
      "Batch 4000: loss=0.364344, elapsed=427.68s, remaining=951.50s.\n",
      "Batch 4100: loss=0.364450, elapsed=438.52s, remaining=941.11s.\n",
      "Batch 4200: loss=0.365813, elapsed=449.01s, remaining=929.98s.\n",
      "Batch 4300: loss=0.364819, elapsed=459.59s, remaining=919.09s.\n",
      "Batch 4400: loss=0.365515, elapsed=470.23s, remaining=908.31s.\n",
      "Batch 4500: loss=0.366009, elapsed=480.85s, remaining=897.52s.\n",
      "Batch 4600: loss=0.366120, elapsed=491.40s, remaining=886.59s.\n",
      "Batch 4700: loss=0.367586, elapsed=502.30s, remaining=876.30s.\n",
      "Batch 4800: loss=0.366456, elapsed=513.18s, remaining=865.98s.\n",
      "Batch 4900: loss=0.366918, elapsed=523.90s, remaining=855.34s.\n",
      "Batch 5000: loss=0.366666, elapsed=534.58s, remaining=844.62s.\n",
      "Batch 5100: loss=0.366349, elapsed=545.15s, remaining=833.75s.\n",
      "Batch 5200: loss=0.366533, elapsed=555.82s, remaining=823.02s.\n",
      "Batch 5300: loss=0.366904, elapsed=566.43s, remaining=812.22s.\n",
      "Batch 5400: loss=0.366637, elapsed=576.99s, remaining=801.36s.\n",
      "Batch 5500: loss=0.366848, elapsed=587.61s, remaining=790.59s.\n",
      "Batch 5600: loss=0.366744, elapsed=598.19s, remaining=779.81s.\n",
      "Batch 5700: loss=0.367513, elapsed=608.93s, remaining=769.22s.\n",
      "Batch 5800: loss=0.368117, elapsed=619.40s, remaining=758.28s.\n",
      "Batch 5900: loss=0.368202, elapsed=630.21s, remaining=747.76s.\n",
      "Batch 6000: loss=0.368494, elapsed=640.79s, remaining=736.97s.\n",
      "Batch 6100: loss=0.369365, elapsed=651.64s, remaining=726.51s.\n",
      "Batch 6200: loss=0.369408, elapsed=662.01s, remaining=715.49s.\n",
      "Batch 6300: loss=0.370248, elapsed=672.60s, remaining=704.72s.\n",
      "Batch 6400: loss=0.370415, elapsed=683.39s, remaining=694.19s.\n",
      "Batch 6500: loss=0.370643, elapsed=693.99s, remaining=683.41s.\n",
      "Batch 6600: loss=0.371024, elapsed=704.65s, remaining=672.72s.\n",
      "Batch 6700: loss=0.371089, elapsed=715.34s, remaining=662.06s.\n",
      "Batch 6800: loss=0.370613, elapsed=726.37s, remaining=651.72s.\n",
      "Batch 6900: loss=0.370305, elapsed=737.01s, remaining=641.01s.\n",
      "Batch 7000: loss=0.369715, elapsed=747.83s, remaining=630.46s.\n",
      "Batch 7100: loss=0.369385, elapsed=758.48s, remaining=619.74s.\n",
      "Batch 7200: loss=0.370114, elapsed=769.15s, remaining=609.05s.\n",
      "Batch 7300: loss=0.370187, elapsed=779.70s, remaining=598.25s.\n",
      "Batch 7400: loss=0.370507, elapsed=790.34s, remaining=587.55s.\n",
      "Batch 7500: loss=0.370234, elapsed=800.70s, remaining=576.63s.\n",
      "Batch 7600: loss=0.369797, elapsed=811.38s, remaining=565.95s.\n",
      "Batch 7700: loss=0.370006, elapsed=822.01s, remaining=555.25s.\n",
      "Batch 7800: loss=0.369946, elapsed=832.81s, remaining=544.64s.\n",
      "Batch 7900: loss=0.370061, elapsed=843.42s, remaining=533.92s.\n",
      "Batch 8000: loss=0.370602, elapsed=854.11s, remaining=523.26s.\n",
      "Batch 8100: loss=0.370477, elapsed=864.92s, remaining=512.68s.\n",
      "Batch 8200: loss=0.370047, elapsed=875.72s, remaining=502.08s.\n",
      "Batch 8300: loss=0.369873, elapsed=886.33s, remaining=491.36s.\n",
      "Batch 8400: loss=0.369247, elapsed=896.95s, remaining=480.67s.\n",
      "Batch 8500: loss=0.369864, elapsed=907.88s, remaining=470.13s.\n",
      "Batch 8600: loss=0.369960, elapsed=918.44s, remaining=459.40s.\n",
      "Batch 8700: loss=0.369432, elapsed=929.08s, remaining=448.72s.\n",
      "Batch 8800: loss=0.369617, elapsed=939.81s, remaining=438.05s.\n",
      "Batch 8900: loss=0.369738, elapsed=950.40s, remaining=427.33s.\n",
      "Batch 9000: loss=0.369326, elapsed=960.97s, remaining=416.61s.\n",
      "Batch 9100: loss=0.369579, elapsed=971.56s, remaining=405.88s.\n",
      "Batch 9200: loss=0.369377, elapsed=982.14s, remaining=395.17s.\n",
      "Batch 9300: loss=0.369423, elapsed=992.71s, remaining=384.46s.\n",
      "Batch 9400: loss=0.369376, elapsed=1003.23s, remaining=373.73s.\n",
      "Batch 9500: loss=0.369002, elapsed=1013.80s, remaining=363.01s.\n",
      "Batch 9600: loss=0.368638, elapsed=1024.68s, remaining=352.42s.\n",
      "Batch 9700: loss=0.368892, elapsed=1035.26s, remaining=341.71s.\n",
      "Batch 9800: loss=0.369161, elapsed=1046.11s, remaining=331.11s.\n",
      "Batch 9900: loss=0.368905, elapsed=1056.78s, remaining=320.43s.\n",
      "Batch 10000: loss=0.368726, elapsed=1067.27s, remaining=309.70s.\n",
      "Batch 10100: loss=0.368511, elapsed=1077.72s, remaining=298.97s.\n",
      "Batch 10200: loss=0.368630, elapsed=1088.43s, remaining=288.31s.\n",
      "Batch 10300: loss=0.369214, elapsed=1099.18s, remaining=277.66s.\n",
      "Batch 10400: loss=0.368756, elapsed=1109.76s, remaining=266.97s.\n",
      "Batch 10500: loss=0.368675, elapsed=1120.51s, remaining=256.31s.\n",
      "Batch 10600: loss=0.368290, elapsed=1131.32s, remaining=245.67s.\n",
      "Batch 10700: loss=0.368210, elapsed=1142.27s, remaining=235.05s.\n",
      "Batch 10800: loss=0.367929, elapsed=1153.11s, remaining=224.40s.\n",
      "Batch 10900: loss=0.367695, elapsed=1163.77s, remaining=213.71s.\n",
      "Batch 11000: loss=0.367629, elapsed=1174.26s, remaining=203.00s.\n",
      "Batch 11100: loss=0.367311, elapsed=1185.01s, remaining=192.34s.\n",
      "Batch 11200: loss=0.367211, elapsed=1195.75s, remaining=181.67s.\n",
      "Batch 11300: loss=0.367300, elapsed=1206.38s, remaining=170.98s.\n",
      "Batch 11400: loss=0.366819, elapsed=1216.90s, remaining=160.28s.\n",
      "Batch 11500: loss=0.366869, elapsed=1227.74s, remaining=149.62s.\n",
      "Batch 11600: loss=0.366549, elapsed=1238.22s, remaining=138.93s.\n",
      "Batch 11700: loss=0.366365, elapsed=1248.78s, remaining=128.25s.\n",
      "Batch 11800: loss=0.366233, elapsed=1259.50s, remaining=117.58s.\n",
      "Batch 11900: loss=0.365656, elapsed=1270.32s, remaining=106.92s.\n",
      "Batch 12000: loss=0.365341, elapsed=1281.04s, remaining=96.25s.\n",
      "Batch 12100: loss=0.365057, elapsed=1291.78s, remaining=85.57s.\n",
      "Batch 12200: loss=0.364834, elapsed=1302.53s, remaining=74.89s.\n",
      "Batch 12300: loss=0.364794, elapsed=1312.96s, remaining=64.20s.\n",
      "Batch 12400: loss=0.365011, elapsed=1323.77s, remaining=53.53s.\n",
      "Batch 12500: loss=0.365027, elapsed=1334.50s, remaining=42.85s.\n",
      "Batch 12600: loss=0.364711, elapsed=1345.07s, remaining=32.17s.\n",
      "Batch 12700: loss=0.364497, elapsed=1355.73s, remaining=21.51s.\n",
      "Batch 12800: loss=0.364315, elapsed=1366.43s, remaining=10.82s.\n",
      "Batch 12900: loss=0.364136, elapsed=1376.90s, remaining=0.15s.\n",
      "Batch 13000: loss=0.364042, elapsed=1387.54s, remaining=-10.53s.\n",
      "Batch 13100: loss=0.363967, elapsed=1398.29s, remaining=-21.20s.\n",
      "Batch 13200: loss=0.363978, elapsed=1409.17s, remaining=-31.88s.\n",
      "\n",
      "Training epoch took: 1418.41s\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.963989 \n",
      "\n",
      "Accuracy - Sentiment: 87.1%, Avg loss: 0.963989 \n",
      "\n",
      "Accuracy - Topic: 86.1%, Avg loss: 0.963989 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    timing_log = train_loop(train_dataloader, model,optimizer, scheduler, device, criterion_sent, criterion_topic, sentiment_var='sentiment',\n",
    "               topic_var='topic', timing_log=True)\n",
    "    eval_loop(eval_dataloader, model, device, criterion_sent, criterion_topic, sentiment_var='sentiment', topic_var='topic')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/manifesto_ContextScalePrediction_sa/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 45.65s, Estimated remaining time: 50.62s\n",
      "Elapsed time: 91.61s, Estimated remaining time: 4.99s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_sa = scale_func(test_dataloader, \n",
    "               model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  precision  recall  accuracy\n",
       "0   0.82       0.82    0.83      0.83\n",
       "1   0.86       0.86    0.87      0.87\n",
       "2   0.90       0.89    0.90      0.90\n",
       "3   0.90       0.89    0.90      0.90\n",
       "4   0.87       0.86    0.87      0.87\n",
       "5   0.83       0.84    0.83      0.83\n",
       "6   0.79       0.79    0.79      0.79\n",
       "7   0.87       0.87    0.86      0.86\n",
       "8   0.89       0.89    0.89      0.89\n",
       "9   0.90       0.89    0.91      0.91\n",
       "10  0.83       0.84    0.81      0.81\n",
       "11  0.83       0.84    0.82      0.82"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sa['res_table_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sa['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  precision  recall  accuracy\n",
       "0  0.91       0.90    0.91      0.91\n",
       "1  0.84       0.85    0.83      0.83\n",
       "2  0.82       0.83    0.82      0.82"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sa['res_table_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_sa['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_sa'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_sa, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_sa['res_table_sentiment'].to_csv('results/classification results/sa_sentiment.csv', index=False)\n",
    "outputs_sa['res_table_topic'].to_csv('results/classification results/sa_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with dynamic gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_dynamic_gating=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion_sent = nn.CrossEntropyLoss()\n",
    "criterion_topic =  nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.635184, elapsed=12.06s, remaining=1522.24s.\n",
      "Batch 200: loss=3.591522, elapsed=22.56s, remaining=1419.77s.\n",
      "Batch 300: loss=3.541844, elapsed=32.83s, remaining=1368.64s.\n",
      "Batch 400: loss=3.491652, elapsed=43.26s, remaining=1342.44s.\n",
      "Batch 500: loss=3.436597, elapsed=54.04s, remaining=1331.66s.\n",
      "Batch 600: loss=3.367661, elapsed=64.29s, remaining=1310.26s.\n",
      "Batch 700: loss=3.288763, elapsed=74.42s, remaining=1289.36s.\n",
      "Batch 800: loss=3.185872, elapsed=84.74s, remaining=1274.32s.\n",
      "Batch 900: loss=3.090013, elapsed=95.11s, remaining=1261.15s.\n",
      "Batch 1000: loss=2.998415, elapsed=105.25s, remaining=1245.70s.\n",
      "Batch 1100: loss=2.915122, elapsed=115.63s, remaining=1234.00s.\n",
      "Batch 1200: loss=2.838422, elapsed=125.75s, remaining=1219.92s.\n",
      "Batch 1300: loss=2.767479, elapsed=136.04s, remaining=1207.93s.\n",
      "Batch 1400: loss=2.706929, elapsed=146.11s, remaining=1194.44s.\n",
      "Batch 1500: loss=2.652048, elapsed=155.92s, remaining=1179.29s.\n",
      "Batch 1600: loss=2.599709, elapsed=165.68s, remaining=1164.57s.\n",
      "Batch 1700: loss=2.551203, elapsed=176.01s, remaining=1154.21s.\n",
      "Batch 1800: loss=2.507066, elapsed=185.95s, remaining=1141.41s.\n",
      "Batch 1900: loss=2.461284, elapsed=195.88s, remaining=1128.86s.\n",
      "Batch 2000: loss=2.419612, elapsed=205.93s, remaining=1117.30s.\n",
      "Batch 2100: loss=2.383690, elapsed=215.97s, remaining=1105.76s.\n",
      "Batch 2200: loss=2.350274, elapsed=226.12s, remaining=1094.91s.\n",
      "Batch 2300: loss=2.315964, elapsed=236.10s, remaining=1083.31s.\n",
      "Batch 2400: loss=2.285782, elapsed=246.12s, remaining=1072.07s.\n",
      "Batch 2500: loss=2.258899, elapsed=256.23s, remaining=1061.30s.\n",
      "Batch 2600: loss=2.231481, elapsed=266.04s, remaining=1049.33s.\n",
      "Batch 2700: loss=2.208808, elapsed=276.02s, remaining=1038.17s.\n",
      "Batch 2800: loss=2.182860, elapsed=286.11s, remaining=1027.56s.\n",
      "Batch 2900: loss=2.159581, elapsed=296.08s, remaining=1016.50s.\n",
      "Batch 3000: loss=2.137192, elapsed=306.11s, remaining=1005.71s.\n",
      "Batch 3100: loss=2.115194, elapsed=316.41s, remaining=995.88s.\n",
      "Batch 3200: loss=2.094004, elapsed=326.87s, remaining=986.44s.\n",
      "Batch 3300: loss=2.076713, elapsed=337.22s, remaining=976.65s.\n",
      "Batch 3400: loss=2.060534, elapsed=347.44s, remaining=966.45s.\n",
      "Batch 3500: loss=2.042905, elapsed=357.26s, remaining=955.23s.\n",
      "Batch 3600: loss=2.027312, elapsed=367.38s, remaining=944.80s.\n",
      "Batch 3700: loss=2.012935, elapsed=377.20s, remaining=933.69s.\n",
      "Batch 3800: loss=1.998535, elapsed=387.21s, remaining=923.09s.\n",
      "Batch 3900: loss=1.983362, elapsed=397.33s, remaining=912.75s.\n",
      "Batch 4000: loss=1.971311, elapsed=407.32s, remaining=902.09s.\n",
      "Batch 4100: loss=1.958071, elapsed=417.81s, remaining=892.58s.\n",
      "Batch 4200: loss=1.943266, elapsed=428.41s, remaining=883.26s.\n",
      "Batch 4300: loss=1.931471, elapsed=438.48s, remaining=872.80s.\n",
      "Batch 4400: loss=1.921682, elapsed=448.49s, remaining=862.30s.\n",
      "Batch 4500: loss=1.909850, elapsed=458.95s, remaining=852.61s.\n",
      "Batch 4600: loss=1.898538, elapsed=469.29s, remaining=842.67s.\n",
      "Batch 4700: loss=1.889339, elapsed=479.81s, remaining=833.06s.\n",
      "Batch 4800: loss=1.879097, elapsed=490.22s, remaining=823.20s.\n",
      "Batch 4900: loss=1.868241, elapsed=500.74s, remaining=813.48s.\n",
      "Batch 5000: loss=1.857665, elapsed=510.89s, remaining=803.12s.\n",
      "Batch 5100: loss=1.847617, elapsed=521.43s, remaining=793.35s.\n",
      "Batch 5200: loss=1.837930, elapsed=532.57s, remaining=784.60s.\n",
      "Batch 5300: loss=1.830472, elapsed=543.72s, remaining=775.74s.\n",
      "Batch 5400: loss=1.821655, elapsed=554.94s, remaining=766.90s.\n",
      "Batch 5500: loss=1.812778, elapsed=566.20s, remaining=758.08s.\n",
      "Batch 5600: loss=1.804290, elapsed=577.35s, remaining=748.99s.\n",
      "Batch 5700: loss=1.797140, elapsed=588.54s, remaining=739.91s.\n",
      "Batch 5800: loss=1.791447, elapsed=599.36s, remaining=730.28s.\n",
      "Batch 5900: loss=1.783738, elapsed=610.65s, remaining=721.18s.\n",
      "Batch 6000: loss=1.775902, elapsed=621.75s, remaining=711.80s.\n",
      "Batch 6100: loss=1.769476, elapsed=632.40s, remaining=701.84s.\n",
      "Batch 6200: loss=1.761688, elapsed=643.34s, remaining=692.16s.\n",
      "Batch 6300: loss=1.754042, elapsed=654.54s, remaining=682.74s.\n",
      "Batch 6400: loss=1.747625, elapsed=665.47s, remaining=672.95s.\n",
      "Batch 6500: loss=1.741048, elapsed=676.62s, remaining=663.38s.\n",
      "Batch 6600: loss=1.734580, elapsed=687.54s, remaining=653.51s.\n",
      "Batch 6700: loss=1.728425, elapsed=698.33s, remaining=643.51s.\n",
      "Batch 6800: loss=1.721785, elapsed=709.28s, remaining=633.61s.\n",
      "Batch 6900: loss=1.716160, elapsed=720.32s, remaining=623.77s.\n",
      "Batch 7000: loss=1.710054, elapsed=731.47s, remaining=614.00s.\n",
      "Batch 7100: loss=1.703801, elapsed=742.35s, remaining=603.97s.\n",
      "Batch 7200: loss=1.698785, elapsed=753.56s, remaining=594.18s.\n",
      "Batch 7300: loss=1.694265, elapsed=764.34s, remaining=584.00s.\n",
      "Batch 7400: loss=1.688680, elapsed=775.25s, remaining=573.92s.\n",
      "Batch 7500: loss=1.683455, elapsed=786.17s, remaining=563.82s.\n",
      "Batch 7600: loss=1.677806, elapsed=797.19s, remaining=553.78s.\n",
      "Batch 7700: loss=1.673142, elapsed=808.36s, remaining=543.81s.\n",
      "Batch 7800: loss=1.667675, elapsed=819.13s, remaining=533.53s.\n",
      "Batch 7900: loss=1.662464, elapsed=830.03s, remaining=523.34s.\n",
      "Batch 8000: loss=1.656915, elapsed=841.07s, remaining=513.22s.\n",
      "Batch 8100: loss=1.652443, elapsed=852.09s, remaining=503.06s.\n",
      "Batch 8200: loss=1.647262, elapsed=863.26s, remaining=492.94s.\n",
      "Batch 8300: loss=1.642770, elapsed=874.21s, remaining=482.70s.\n",
      "Batch 8400: loss=1.638556, elapsed=885.00s, remaining=472.34s.\n",
      "Batch 8500: loss=1.633604, elapsed=895.92s, remaining=462.04s.\n",
      "Batch 8600: loss=1.628616, elapsed=906.83s, remaining=451.73s.\n",
      "Batch 8700: loss=1.624314, elapsed=917.79s, remaining=441.42s.\n",
      "Batch 8800: loss=1.620420, elapsed=928.76s, remaining=431.09s.\n",
      "Batch 8900: loss=1.615920, elapsed=939.46s, remaining=420.64s.\n",
      "Batch 9000: loss=1.612100, elapsed=950.38s, remaining=410.27s.\n",
      "Batch 9100: loss=1.607344, elapsed=961.58s, remaining=400.01s.\n",
      "Batch 9200: loss=1.602876, elapsed=972.63s, remaining=389.68s.\n",
      "Batch 9300: loss=1.599297, elapsed=983.64s, remaining=379.31s.\n",
      "Batch 9400: loss=1.595548, elapsed=994.46s, remaining=368.84s.\n",
      "Batch 9500: loss=1.591774, elapsed=1005.38s, remaining=358.41s.\n",
      "Batch 9600: loss=1.588276, elapsed=1016.46s, remaining=348.03s.\n",
      "Batch 9700: loss=1.583986, elapsed=1027.30s, remaining=337.55s.\n",
      "Batch 9800: loss=1.580318, elapsed=1038.03s, remaining=327.02s.\n",
      "Batch 9900: loss=1.576616, elapsed=1049.06s, remaining=316.59s.\n",
      "Batch 10000: loss=1.572834, elapsed=1059.84s, remaining=306.08s.\n",
      "Batch 10100: loss=1.569279, elapsed=1070.82s, remaining=295.60s.\n",
      "Batch 10200: loss=1.565428, elapsed=1081.78s, remaining=285.13s.\n",
      "Batch 10300: loss=1.561559, elapsed=1092.74s, remaining=274.65s.\n",
      "Batch 10400: loss=1.558182, elapsed=1103.69s, remaining=264.15s.\n",
      "Batch 10500: loss=1.554182, elapsed=1114.75s, remaining=253.67s.\n",
      "Batch 10600: loss=1.551171, elapsed=1125.55s, remaining=243.12s.\n",
      "Batch 10700: loss=1.548334, elapsed=1136.56s, remaining=232.61s.\n",
      "Batch 10800: loss=1.545137, elapsed=1147.43s, remaining=222.07s.\n",
      "Batch 10900: loss=1.542437, elapsed=1158.30s, remaining=211.50s.\n",
      "Batch 11000: loss=1.538561, elapsed=1169.20s, remaining=200.95s.\n",
      "Batch 11100: loss=1.535286, elapsed=1180.23s, remaining=190.40s.\n",
      "Batch 11200: loss=1.532547, elapsed=1191.46s, remaining=179.90s.\n",
      "Batch 11300: loss=1.529230, elapsed=1202.15s, remaining=169.28s.\n",
      "Batch 11400: loss=1.525582, elapsed=1212.96s, remaining=158.69s.\n",
      "Batch 11500: loss=1.522831, elapsed=1223.88s, remaining=148.10s.\n",
      "Batch 11600: loss=1.519998, elapsed=1235.00s, remaining=137.53s.\n",
      "Batch 11700: loss=1.516895, elapsed=1246.06s, remaining=126.95s.\n",
      "Batch 11800: loss=1.514474, elapsed=1256.97s, remaining=116.34s.\n",
      "Batch 11900: loss=1.511913, elapsed=1267.74s, remaining=105.72s.\n",
      "Batch 12000: loss=1.508911, elapsed=1278.66s, remaining=95.11s.\n",
      "Batch 12100: loss=1.506132, elapsed=1289.70s, remaining=84.50s.\n",
      "Batch 12200: loss=1.503002, elapsed=1300.63s, remaining=73.86s.\n",
      "Batch 12300: loss=1.500047, elapsed=1311.59s, remaining=63.24s.\n",
      "Batch 12400: loss=1.496863, elapsed=1322.55s, remaining=52.61s.\n",
      "Batch 12500: loss=1.494502, elapsed=1333.51s, remaining=41.98s.\n",
      "Batch 12600: loss=1.491952, elapsed=1344.41s, remaining=31.33s.\n",
      "Batch 12700: loss=1.489049, elapsed=1355.15s, remaining=20.68s.\n",
      "Batch 12800: loss=1.485962, elapsed=1366.10s, remaining=10.04s.\n",
      "Batch 12900: loss=1.483897, elapsed=1377.10s, remaining=-0.61s.\n",
      "Batch 13000: loss=1.481578, elapsed=1388.19s, remaining=-11.27s.\n",
      "Batch 13100: loss=1.478767, elapsed=1399.12s, remaining=-21.94s.\n",
      "Batch 13200: loss=1.476032, elapsed=1410.01s, remaining=-32.61s.\n",
      "\n",
      "Training epoch took: 1419.31s\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 1.075076 \n",
      "\n",
      "Accuracy - Sentiment: 80.9%, Avg loss: 1.075076 \n",
      "\n",
      "Accuracy - Topic: 80.3%, Avg loss: 1.075076 \n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=1.013018, elapsed=10.92s, remaining=1384.47s.\n",
      "Batch 200: loss=1.010189, elapsed=21.90s, remaining=1384.47s.\n",
      "Batch 300: loss=1.007366, elapsed=32.79s, remaining=1373.41s.\n",
      "Batch 400: loss=0.995482, elapsed=43.82s, remaining=1367.01s.\n",
      "Batch 500: loss=0.989136, elapsed=54.91s, remaining=1360.20s.\n",
      "Batch 600: loss=0.996029, elapsed=65.90s, remaining=1349.85s.\n",
      "Batch 700: loss=1.001657, elapsed=76.93s, remaining=1340.11s.\n",
      "Batch 800: loss=1.004134, elapsed=87.75s, remaining=1326.84s.\n",
      "Batch 900: loss=1.002658, elapsed=98.79s, remaining=1317.05s.\n",
      "Batch 1000: loss=1.005603, elapsed=109.63s, remaining=1304.66s.\n",
      "Batch 1100: loss=1.008321, elapsed=120.40s, remaining=1291.60s.\n",
      "Batch 1200: loss=1.009537, elapsed=131.25s, remaining=1279.73s.\n",
      "Batch 1300: loss=1.008082, elapsed=142.09s, remaining=1268.04s.\n",
      "Batch 1400: loss=1.012100, elapsed=153.04s, remaining=1257.35s.\n",
      "Batch 1500: loss=1.011840, elapsed=163.75s, remaining=1244.88s.\n",
      "Batch 1600: loss=1.009651, elapsed=174.79s, remaining=1234.87s.\n",
      "Batch 1700: loss=1.014040, elapsed=185.79s, remaining=1224.51s.\n",
      "Batch 1800: loss=1.014368, elapsed=196.68s, remaining=1213.35s.\n",
      "Batch 1900: loss=1.010486, elapsed=207.53s, remaining=1202.04s.\n",
      "Batch 2000: loss=1.011228, elapsed=218.50s, remaining=1191.39s.\n",
      "Batch 2100: loss=1.007803, elapsed=229.64s, remaining=1181.63s.\n",
      "Batch 2200: loss=1.008021, elapsed=240.62s, remaining=1171.02s.\n",
      "Batch 2300: loss=1.007430, elapsed=251.56s, remaining=1160.11s.\n",
      "Batch 2400: loss=1.008153, elapsed=262.97s, remaining=1151.32s.\n",
      "Batch 2500: loss=1.007343, elapsed=273.99s, remaining=1140.68s.\n",
      "Batch 2600: loss=1.007790, elapsed=285.09s, remaining=1130.32s.\n",
      "Batch 2700: loss=1.007828, elapsed=295.89s, remaining=1118.69s.\n",
      "Batch 2800: loss=1.010702, elapsed=307.05s, remaining=1108.50s.\n",
      "Batch 2900: loss=1.011482, elapsed=318.07s, remaining=1097.68s.\n",
      "Batch 3000: loss=1.010714, elapsed=328.99s, remaining=1086.51s.\n",
      "Batch 3100: loss=1.008635, elapsed=339.70s, remaining=1074.61s.\n",
      "Batch 3200: loss=1.009482, elapsed=350.39s, remaining=1062.84s.\n",
      "Batch 3300: loss=1.007535, elapsed=361.27s, remaining=1051.72s.\n",
      "Batch 3400: loss=1.005500, elapsed=372.00s, remaining=1040.12s.\n",
      "Batch 3500: loss=1.004290, elapsed=382.52s, remaining=1028.03s.\n",
      "Batch 3600: loss=1.004903, elapsed=393.32s, remaining=1016.80s.\n",
      "Batch 3700: loss=1.004570, elapsed=403.88s, remaining=1004.93s.\n",
      "Batch 3800: loss=1.003645, elapsed=414.56s, remaining=993.46s.\n",
      "Batch 3900: loss=1.003181, elapsed=425.21s, remaining=981.94s.\n",
      "Batch 4000: loss=1.004865, elapsed=435.90s, remaining=970.59s.\n",
      "Batch 4100: loss=1.003684, elapsed=446.58s, remaining=959.21s.\n",
      "Batch 4200: loss=1.002395, elapsed=457.25s, remaining=947.83s.\n",
      "Batch 4300: loss=1.001471, elapsed=468.18s, remaining=937.06s.\n",
      "Batch 4400: loss=0.999242, elapsed=478.89s, remaining=925.82s.\n",
      "Batch 4500: loss=0.998353, elapsed=489.30s, remaining=914.06s.\n",
      "Batch 4600: loss=0.998731, elapsed=499.89s, remaining=902.66s.\n",
      "Batch 4700: loss=0.998492, elapsed=510.78s, remaining=891.82s.\n",
      "Batch 4800: loss=0.997725, elapsed=521.47s, remaining=880.65s.\n",
      "Batch 4900: loss=0.997213, elapsed=532.30s, remaining=869.71s.\n",
      "Batch 5000: loss=0.997245, elapsed=542.90s, remaining=858.41s.\n",
      "Batch 5100: loss=0.997286, elapsed=553.32s, remaining=846.86s.\n",
      "Batch 5200: loss=0.996211, elapsed=563.95s, remaining=835.66s.\n",
      "Batch 5300: loss=0.996674, elapsed=574.63s, remaining=824.61s.\n",
      "Batch 5400: loss=0.995924, elapsed=585.46s, remaining=813.73s.\n",
      "Batch 5500: loss=0.995195, elapsed=596.38s, remaining=802.99s.\n",
      "Batch 5600: loss=0.994697, elapsed=607.50s, remaining=792.52s.\n",
      "Batch 5700: loss=0.994050, elapsed=618.15s, remaining=781.44s.\n",
      "Batch 5800: loss=0.993037, elapsed=628.78s, remaining=770.33s.\n",
      "Batch 5900: loss=0.992512, elapsed=639.33s, remaining=759.15s.\n",
      "Batch 6000: loss=0.991054, elapsed=649.89s, remaining=747.99s.\n",
      "Batch 6100: loss=0.989297, elapsed=660.53s, remaining=736.94s.\n",
      "Batch 6200: loss=0.988369, elapsed=671.10s, remaining=725.82s.\n",
      "Batch 6300: loss=0.987347, elapsed=681.84s, remaining=714.91s.\n",
      "Batch 6400: loss=0.987459, elapsed=692.33s, remaining=703.74s.\n",
      "Batch 6500: loss=0.986462, elapsed=703.29s, remaining=693.08s.\n",
      "Batch 6600: loss=0.985545, elapsed=714.04s, remaining=682.20s.\n",
      "Batch 6700: loss=0.985192, elapsed=724.84s, remaining=671.39s.\n",
      "Batch 6800: loss=0.984666, elapsed=735.37s, remaining=660.29s.\n",
      "Batch 6900: loss=0.984182, elapsed=746.21s, remaining=649.50s.\n",
      "Batch 7000: loss=0.982607, elapsed=756.94s, remaining=638.61s.\n",
      "Batch 7100: loss=0.981529, elapsed=767.63s, remaining=627.69s.\n",
      "Batch 7200: loss=0.981579, elapsed=778.21s, remaining=616.67s.\n",
      "Batch 7300: loss=0.980198, elapsed=788.78s, remaining=605.67s.\n",
      "Batch 7400: loss=0.978781, elapsed=799.43s, remaining=594.74s.\n",
      "Batch 7500: loss=0.978707, elapsed=810.10s, remaining=583.82s.\n",
      "Batch 7600: loss=0.978329, elapsed=820.82s, remaining=572.98s.\n",
      "Batch 7700: loss=0.978310, elapsed=831.44s, remaining=562.05s.\n",
      "Batch 7800: loss=0.976812, elapsed=842.37s, remaining=551.33s.\n",
      "Batch 7900: loss=0.975803, elapsed=853.11s, remaining=540.48s.\n",
      "Batch 8000: loss=0.975655, elapsed=863.80s, remaining=529.62s.\n",
      "Batch 8100: loss=0.975554, elapsed=874.42s, remaining=518.73s.\n",
      "Batch 8200: loss=0.975074, elapsed=885.15s, remaining=507.88s.\n",
      "Batch 8300: loss=0.974142, elapsed=895.73s, remaining=496.96s.\n",
      "Batch 8400: loss=0.973565, elapsed=906.17s, remaining=485.99s.\n",
      "Batch 8500: loss=0.973285, elapsed=916.81s, remaining=475.12s.\n",
      "Batch 8600: loss=0.972937, elapsed=927.36s, remaining=464.21s.\n",
      "Batch 8700: loss=0.972523, elapsed=938.34s, remaining=453.53s.\n",
      "Batch 8800: loss=0.971286, elapsed=949.05s, remaining=442.70s.\n",
      "Batch 8900: loss=0.970941, elapsed=959.65s, remaining=431.82s.\n",
      "Batch 9000: loss=0.970158, elapsed=970.20s, remaining=420.94s.\n",
      "Batch 9100: loss=0.970054, elapsed=980.81s, remaining=410.09s.\n",
      "Batch 9200: loss=0.969259, elapsed=991.52s, remaining=399.28s.\n",
      "Batch 9300: loss=0.969214, elapsed=1002.25s, remaining=388.49s.\n",
      "Batch 9400: loss=0.968626, elapsed=1012.99s, remaining=377.71s.\n",
      "Batch 9500: loss=0.967136, elapsed=1023.79s, remaining=366.95s.\n",
      "Batch 9600: loss=0.966538, elapsed=1034.43s, remaining=356.13s.\n",
      "Batch 9700: loss=0.966979, elapsed=1044.87s, remaining=345.23s.\n",
      "Batch 9800: loss=0.966704, elapsed=1055.32s, remaining=334.36s.\n",
      "Batch 9900: loss=0.966460, elapsed=1065.86s, remaining=323.52s.\n",
      "Batch 10000: loss=0.966313, elapsed=1076.42s, remaining=312.68s.\n",
      "Batch 10100: loss=0.965269, elapsed=1087.18s, remaining=301.91s.\n",
      "Batch 10200: loss=0.965001, elapsed=1097.72s, remaining=291.08s.\n",
      "Batch 10300: loss=0.964934, elapsed=1108.31s, remaining=280.26s.\n",
      "Batch 10400: loss=0.964591, elapsed=1118.93s, remaining=269.47s.\n",
      "Batch 10500: loss=0.963586, elapsed=1129.58s, remaining=258.69s.\n",
      "Batch 10600: loss=0.962321, elapsed=1140.22s, remaining=247.90s.\n",
      "Batch 10700: loss=0.962000, elapsed=1150.64s, remaining=237.07s.\n",
      "Batch 10800: loss=0.961562, elapsed=1161.25s, remaining=226.29s.\n",
      "Batch 10900: loss=0.962088, elapsed=1172.09s, remaining=215.54s.\n",
      "Batch 11000: loss=0.961786, elapsed=1182.67s, remaining=204.76s.\n",
      "Batch 11100: loss=0.961610, elapsed=1193.27s, remaining=193.98s.\n",
      "Batch 11200: loss=0.961097, elapsed=1204.00s, remaining=183.22s.\n",
      "Batch 11300: loss=0.960699, elapsed=1214.69s, remaining=172.46s.\n",
      "Batch 11400: loss=0.960887, elapsed=1224.98s, remaining=161.65s.\n",
      "Batch 11500: loss=0.960705, elapsed=1235.40s, remaining=150.84s.\n",
      "Batch 11600: loss=0.960571, elapsed=1246.27s, remaining=140.12s.\n",
      "Batch 11700: loss=0.959933, elapsed=1256.71s, remaining=129.34s.\n",
      "Batch 11800: loss=0.959370, elapsed=1267.40s, remaining=118.59s.\n",
      "Batch 11900: loss=0.959285, elapsed=1277.78s, remaining=107.82s.\n",
      "Batch 12000: loss=0.959570, elapsed=1288.09s, remaining=97.05s.\n",
      "Batch 12100: loss=0.959421, elapsed=1298.82s, remaining=86.31s.\n",
      "Batch 12200: loss=0.959028, elapsed=1309.40s, remaining=75.56s.\n",
      "Batch 12300: loss=0.958571, elapsed=1319.99s, remaining=64.81s.\n",
      "Batch 12400: loss=0.957803, elapsed=1330.39s, remaining=54.06s.\n",
      "Batch 12500: loss=0.957209, elapsed=1340.86s, remaining=43.31s.\n",
      "Batch 12600: loss=0.957052, elapsed=1351.51s, remaining=32.58s.\n",
      "Batch 12700: loss=0.956427, elapsed=1362.23s, remaining=21.86s.\n",
      "Batch 12800: loss=0.956008, elapsed=1372.83s, remaining=11.13s.\n",
      "Batch 12900: loss=0.955473, elapsed=1383.57s, remaining=0.40s.\n",
      "Batch 13000: loss=0.954681, elapsed=1394.30s, remaining=-10.32s.\n",
      "Batch 13100: loss=0.954406, elapsed=1405.18s, remaining=-21.04s.\n",
      "Batch 13200: loss=0.954154, elapsed=1415.86s, remaining=-31.77s.\n",
      "\n",
      "Training epoch took: 1424.91s\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.914999 \n",
      "\n",
      "Accuracy - Sentiment: 83.8%, Avg loss: 0.914999 \n",
      "\n",
      "Accuracy - Topic: 83.2%, Avg loss: 0.914999 \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.728809, elapsed=10.69s, remaining=1354.30s.\n",
      "Batch 200: loss=0.732151, elapsed=21.43s, remaining=1353.82s.\n",
      "Batch 300: loss=0.718953, elapsed=32.09s, remaining=1343.38s.\n",
      "Batch 400: loss=0.729242, elapsed=43.02s, remaining=1341.10s.\n",
      "Batch 500: loss=0.721119, elapsed=53.66s, remaining=1328.11s.\n",
      "Batch 600: loss=0.721336, elapsed=64.16s, remaining=1313.27s.\n",
      "Batch 700: loss=0.722980, elapsed=74.91s, remaining=1303.99s.\n",
      "Batch 800: loss=0.725313, elapsed=85.61s, remaining=1293.57s.\n",
      "Batch 900: loss=0.724426, elapsed=96.17s, remaining=1281.19s.\n",
      "Batch 1000: loss=0.726088, elapsed=106.84s, remaining=1270.36s.\n",
      "Batch 1100: loss=0.726586, elapsed=117.51s, remaining=1259.57s.\n",
      "Batch 1200: loss=0.726443, elapsed=128.13s, remaining=1248.33s.\n",
      "Batch 1300: loss=0.724975, elapsed=138.80s, remaining=1237.64s.\n",
      "Batch 1400: loss=0.723225, elapsed=149.53s, remaining=1227.47s.\n",
      "Batch 1500: loss=0.721901, elapsed=160.34s, remaining=1217.82s.\n",
      "Batch 1600: loss=0.722344, elapsed=170.95s, remaining=1206.64s.\n",
      "Batch 1700: loss=0.721319, elapsed=181.66s, remaining=1196.19s.\n",
      "Batch 1800: loss=0.721170, elapsed=192.22s, remaining=1184.84s.\n",
      "Batch 1900: loss=0.720035, elapsed=202.87s, remaining=1174.04s.\n",
      "Batch 2000: loss=0.719317, elapsed=213.58s, remaining=1163.61s.\n",
      "Batch 2100: loss=0.716912, elapsed=224.30s, remaining=1153.09s.\n",
      "Batch 2200: loss=0.716058, elapsed=234.73s, remaining=1141.23s.\n",
      "Batch 2300: loss=0.714138, elapsed=245.48s, remaining=1130.95s.\n",
      "Batch 2400: loss=0.716967, elapsed=256.16s, remaining=1120.30s.\n",
      "Batch 2500: loss=0.717292, elapsed=266.68s, remaining=1109.01s.\n",
      "Batch 2600: loss=0.717695, elapsed=277.25s, remaining=1097.96s.\n",
      "Batch 2700: loss=0.718061, elapsed=287.85s, remaining=1087.07s.\n",
      "Batch 2800: loss=0.717491, elapsed=298.64s, remaining=1076.91s.\n",
      "Batch 2900: loss=0.716966, elapsed=309.27s, remaining=1066.14s.\n",
      "Batch 3000: loss=0.715514, elapsed=319.99s, remaining=1055.69s.\n",
      "Batch 3100: loss=0.714373, elapsed=330.79s, remaining=1045.44s.\n",
      "Batch 3200: loss=0.714581, elapsed=341.46s, remaining=1034.77s.\n",
      "Batch 3300: loss=0.715238, elapsed=352.11s, remaining=1024.06s.\n",
      "Batch 3400: loss=0.715624, elapsed=362.72s, remaining=1013.26s.\n",
      "Batch 3500: loss=0.717225, elapsed=373.31s, remaining=1002.39s.\n",
      "Batch 3600: loss=0.716630, elapsed=386.27s, remaining=997.39s.\n",
      "Batch 3700: loss=0.717090, elapsed=397.71s, remaining=987.94s.\n",
      "Batch 3800: loss=0.716748, elapsed=408.74s, remaining=977.83s.\n",
      "Batch 3900: loss=0.716157, elapsed=419.12s, remaining=966.17s.\n",
      "Batch 4000: loss=0.715812, elapsed=429.91s, remaining=955.47s.\n",
      "Batch 4100: loss=0.717255, elapsed=440.33s, remaining=943.91s.\n",
      "Batch 4200: loss=0.716799, elapsed=451.05s, remaining=933.03s.\n",
      "Batch 4300: loss=0.716018, elapsed=461.60s, remaining=921.73s.\n",
      "Batch 4400: loss=0.716173, elapsed=472.55s, remaining=911.55s.\n",
      "Batch 4500: loss=0.716198, elapsed=483.24s, remaining=900.79s.\n",
      "Batch 4600: loss=0.716253, elapsed=494.49s, remaining=890.73s.\n",
      "Batch 4700: loss=0.717164, elapsed=505.36s, remaining=880.10s.\n",
      "Batch 4800: loss=0.717678, elapsed=516.40s, remaining=869.94s.\n",
      "Batch 4900: loss=0.717304, elapsed=527.20s, remaining=859.07s.\n",
      "Batch 5000: loss=0.715713, elapsed=537.86s, remaining=848.01s.\n",
      "Batch 5100: loss=0.716134, elapsed=548.65s, remaining=837.22s.\n",
      "Batch 5200: loss=0.715390, elapsed=559.32s, remaining=826.39s.\n",
      "Batch 5300: loss=0.716606, elapsed=570.12s, remaining=815.73s.\n",
      "Batch 5400: loss=0.715597, elapsed=581.18s, remaining=805.20s.\n",
      "Batch 5500: loss=0.714077, elapsed=591.93s, remaining=794.39s.\n",
      "Batch 5600: loss=0.715200, elapsed=602.76s, remaining=783.64s.\n",
      "Batch 5700: loss=0.715037, elapsed=613.43s, remaining=772.71s.\n",
      "Batch 5800: loss=0.714002, elapsed=624.10s, remaining=761.79s.\n",
      "Batch 5900: loss=0.713131, elapsed=635.42s, remaining=751.48s.\n",
      "Batch 6000: loss=0.712690, elapsed=648.56s, remaining=743.80s.\n",
      "Batch 6100: loss=0.712340, elapsed=659.60s, remaining=733.33s.\n",
      "Batch 6200: loss=0.711654, elapsed=670.52s, remaining=722.76s.\n",
      "Batch 6300: loss=0.712506, elapsed=681.62s, remaining=712.09s.\n",
      "Batch 6400: loss=0.713127, elapsed=692.46s, remaining=701.37s.\n",
      "Batch 6500: loss=0.713315, elapsed=702.89s, remaining=690.28s.\n",
      "Batch 6600: loss=0.712903, elapsed=713.50s, remaining=679.29s.\n",
      "Batch 6700: loss=0.711760, elapsed=724.19s, remaining=668.37s.\n",
      "Batch 6800: loss=0.712491, elapsed=734.85s, remaining=657.47s.\n",
      "Batch 6900: loss=0.712023, elapsed=745.47s, remaining=646.59s.\n",
      "Batch 7000: loss=0.712018, elapsed=756.25s, remaining=635.84s.\n",
      "Batch 7100: loss=0.712024, elapsed=767.04s, remaining=625.00s.\n",
      "Batch 7200: loss=0.711461, elapsed=777.86s, remaining=614.18s.\n",
      "Batch 7300: loss=0.711290, elapsed=788.55s, remaining=603.25s.\n",
      "Batch 7400: loss=0.710976, elapsed=799.44s, remaining=592.51s.\n",
      "Batch 7500: loss=0.710544, elapsed=810.25s, remaining=581.71s.\n",
      "Batch 7600: loss=0.710496, elapsed=821.15s, remaining=570.95s.\n",
      "Batch 7700: loss=0.710921, elapsed=831.94s, remaining=560.18s.\n",
      "Batch 7800: loss=0.711349, elapsed=842.98s, remaining=549.62s.\n",
      "Batch 7900: loss=0.711109, elapsed=853.80s, remaining=538.77s.\n",
      "Batch 8000: loss=0.710478, elapsed=864.29s, remaining=527.80s.\n",
      "Batch 8100: loss=0.710436, elapsed=875.56s, remaining=517.26s.\n",
      "Batch 8200: loss=0.710319, elapsed=886.14s, remaining=506.31s.\n",
      "Batch 8300: loss=0.710263, elapsed=896.84s, remaining=495.47s.\n",
      "Batch 8400: loss=0.709481, elapsed=907.53s, remaining=484.65s.\n",
      "Batch 8500: loss=0.709186, elapsed=918.06s, remaining=473.66s.\n",
      "Batch 8600: loss=0.709244, elapsed=928.62s, remaining=462.76s.\n",
      "Batch 8700: loss=0.709490, elapsed=939.69s, remaining=452.15s.\n",
      "Batch 8800: loss=0.709167, elapsed=950.81s, remaining=441.45s.\n",
      "Batch 8900: loss=0.709768, elapsed=961.73s, remaining=430.72s.\n",
      "Batch 9000: loss=0.709478, elapsed=972.40s, remaining=419.87s.\n",
      "Batch 9100: loss=0.708931, elapsed=983.61s, remaining=409.30s.\n",
      "Batch 9200: loss=0.709009, elapsed=994.46s, remaining=398.51s.\n",
      "Batch 9300: loss=0.708809, elapsed=1005.07s, remaining=387.62s.\n",
      "Batch 9400: loss=0.708433, elapsed=1015.79s, remaining=376.81s.\n",
      "Batch 9500: loss=0.708053, elapsed=1026.31s, remaining=365.86s.\n",
      "Batch 9600: loss=0.707888, elapsed=1036.99s, remaining=355.00s.\n",
      "Batch 9700: loss=0.707737, elapsed=1047.97s, remaining=344.25s.\n",
      "Batch 9800: loss=0.707717, elapsed=1058.51s, remaining=333.32s.\n",
      "Batch 9900: loss=0.706902, elapsed=1069.22s, remaining=322.62s.\n",
      "Batch 10000: loss=0.706510, elapsed=1079.90s, remaining=311.74s.\n",
      "Batch 10100: loss=0.706203, elapsed=1090.38s, remaining=300.86s.\n",
      "Batch 10200: loss=0.706109, elapsed=1101.53s, remaining=290.08s.\n",
      "Batch 10300: loss=0.706597, elapsed=1112.20s, remaining=279.28s.\n",
      "Batch 10400: loss=0.706549, elapsed=1122.93s, remaining=268.50s.\n",
      "Batch 10500: loss=0.706040, elapsed=1133.51s, remaining=257.61s.\n",
      "Batch 10600: loss=0.706182, elapsed=1144.55s, remaining=246.77s.\n",
      "Batch 10700: loss=0.705375, elapsed=1155.33s, remaining=236.01s.\n",
      "Batch 10800: loss=0.704960, elapsed=1166.18s, remaining=225.18s.\n",
      "Batch 10900: loss=0.704636, elapsed=1177.07s, remaining=214.40s.\n",
      "Batch 11000: loss=0.703986, elapsed=1188.05s, remaining=203.62s.\n",
      "Batch 11100: loss=0.704271, elapsed=1198.84s, remaining=192.80s.\n",
      "Batch 11200: loss=0.703359, elapsed=1209.83s, remaining=182.03s.\n",
      "Batch 11300: loss=0.703773, elapsed=1220.72s, remaining=171.26s.\n",
      "Batch 11400: loss=0.702991, elapsed=1231.91s, remaining=160.53s.\n",
      "Batch 11500: loss=0.702408, elapsed=1242.87s, remaining=149.70s.\n",
      "Batch 11600: loss=0.702360, elapsed=1253.61s, remaining=138.91s.\n",
      "Batch 11700: loss=0.702128, elapsed=1264.31s, remaining=128.13s.\n",
      "Batch 11800: loss=0.702509, elapsed=1275.23s, remaining=117.33s.\n",
      "Batch 11900: loss=0.702285, elapsed=1285.80s, remaining=106.50s.\n",
      "Batch 12000: loss=0.702386, elapsed=1296.60s, remaining=95.73s.\n",
      "Batch 12100: loss=0.702007, elapsed=1307.66s, remaining=84.90s.\n",
      "Batch 12200: loss=0.701931, elapsed=1318.41s, remaining=74.07s.\n",
      "Batch 12300: loss=0.701637, elapsed=1329.23s, remaining=63.27s.\n",
      "Batch 12400: loss=0.701226, elapsed=1340.25s, remaining=52.48s.\n",
      "Batch 12500: loss=0.701022, elapsed=1351.01s, remaining=41.72s.\n",
      "Batch 12600: loss=0.700631, elapsed=1361.94s, remaining=30.87s.\n",
      "Batch 12700: loss=0.700020, elapsed=1372.56s, remaining=20.05s.\n",
      "Batch 12800: loss=0.699689, elapsed=1383.23s, remaining=9.21s.\n",
      "Batch 12900: loss=0.699592, elapsed=1393.99s, remaining=-1.62s.\n",
      "Batch 13000: loss=0.699148, elapsed=1404.71s, remaining=-12.44s.\n",
      "Batch 13100: loss=0.698739, elapsed=1415.39s, remaining=-23.25s.\n",
      "Batch 13200: loss=0.698282, elapsed=1425.94s, remaining=-34.03s.\n",
      "\n",
      "Training epoch took: 1434.66s\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.884184 \n",
      "\n",
      "Accuracy - Sentiment: 85.4%, Avg loss: 0.884184 \n",
      "\n",
      "Accuracy - Topic: 84.7%, Avg loss: 0.884184 \n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.486291, elapsed=10.63s, remaining=1346.32s.\n",
      "Batch 200: loss=0.492489, elapsed=20.98s, remaining=1324.40s.\n",
      "Batch 300: loss=0.492973, elapsed=31.47s, remaining=1316.01s.\n",
      "Batch 400: loss=0.493239, elapsed=42.11s, remaining=1311.10s.\n",
      "Batch 500: loss=0.487119, elapsed=52.58s, remaining=1299.72s.\n",
      "Batch 600: loss=0.488370, elapsed=63.05s, remaining=1288.68s.\n",
      "Batch 700: loss=0.496713, elapsed=73.86s, remaining=1283.73s.\n",
      "Batch 800: loss=0.500372, elapsed=84.36s, remaining=1272.63s.\n",
      "Batch 900: loss=0.504273, elapsed=94.95s, remaining=1262.82s.\n",
      "Batch 1000: loss=0.508483, elapsed=105.70s, remaining=1254.60s.\n",
      "Batch 1100: loss=0.506662, elapsed=116.06s, remaining=1241.93s.\n",
      "Batch 1200: loss=0.507627, elapsed=126.65s, remaining=1231.87s.\n",
      "Batch 1300: loss=0.512100, elapsed=137.40s, remaining=1223.02s.\n",
      "Batch 1400: loss=0.508796, elapsed=148.20s, remaining=1214.30s.\n",
      "Batch 1500: loss=0.510154, elapsed=158.78s, remaining=1203.87s.\n",
      "Batch 1600: loss=0.507652, elapsed=169.93s, remaining=1197.19s.\n",
      "Batch 1700: loss=0.511293, elapsed=180.72s, remaining=1187.78s.\n",
      "Batch 1800: loss=0.510580, elapsed=191.51s, remaining=1178.02s.\n",
      "Batch 1900: loss=0.512391, elapsed=201.93s, remaining=1166.62s.\n",
      "Batch 2000: loss=0.511977, elapsed=212.21s, remaining=1153.76s.\n",
      "Batch 2100: loss=0.513726, elapsed=222.86s, remaining=1142.91s.\n",
      "Batch 2200: loss=0.514602, elapsed=233.54s, remaining=1132.45s.\n",
      "Batch 2300: loss=0.517320, elapsed=243.80s, remaining=1120.63s.\n",
      "Batch 2400: loss=0.515458, elapsed=254.15s, remaining=1109.08s.\n",
      "Batch 2500: loss=0.515946, elapsed=264.51s, remaining=1097.47s.\n",
      "Batch 2600: loss=0.514508, elapsed=275.10s, remaining=1087.38s.\n",
      "Batch 2700: loss=0.515467, elapsed=285.33s, remaining=1075.63s.\n",
      "Batch 2800: loss=0.515129, elapsed=295.81s, remaining=1064.79s.\n",
      "Batch 2900: loss=0.514671, elapsed=306.27s, remaining=1054.06s.\n",
      "Batch 3000: loss=0.514848, elapsed=316.86s, remaining=1043.62s.\n",
      "Batch 3100: loss=0.516207, elapsed=327.52s, remaining=1033.39s.\n",
      "Batch 3200: loss=0.516790, elapsed=338.11s, remaining=1022.91s.\n",
      "Batch 3300: loss=0.516871, elapsed=349.18s, remaining=1013.82s.\n",
      "Batch 3400: loss=0.516963, elapsed=360.54s, remaining=1005.31s.\n",
      "Batch 3500: loss=0.515194, elapsed=371.31s, remaining=995.14s.\n",
      "Batch 3600: loss=0.514279, elapsed=382.37s, remaining=985.91s.\n",
      "Batch 3700: loss=0.513729, elapsed=392.93s, remaining=975.16s.\n",
      "Batch 3800: loss=0.514910, elapsed=403.69s, remaining=964.59s.\n",
      "Batch 3900: loss=0.515803, elapsed=414.18s, remaining=953.42s.\n",
      "Batch 4000: loss=0.515105, elapsed=424.69s, remaining=942.62s.\n",
      "Batch 4100: loss=0.514340, elapsed=435.29s, remaining=932.23s.\n",
      "Batch 4200: loss=0.514086, elapsed=446.21s, remaining=922.03s.\n",
      "Batch 4300: loss=0.513224, elapsed=457.27s, remaining=912.39s.\n",
      "Batch 4400: loss=0.514526, elapsed=468.29s, remaining=902.57s.\n",
      "Batch 4500: loss=0.514235, elapsed=479.30s, remaining=892.71s.\n",
      "Batch 4600: loss=0.514027, elapsed=489.91s, remaining=882.12s.\n",
      "Batch 4700: loss=0.513692, elapsed=500.89s, remaining=871.98s.\n",
      "Batch 4800: loss=0.512856, elapsed=512.03s, remaining=862.12s.\n",
      "Batch 4900: loss=0.513085, elapsed=522.75s, remaining=851.40s.\n",
      "Batch 5000: loss=0.514043, elapsed=533.34s, remaining=840.59s.\n",
      "Batch 5100: loss=0.513460, elapsed=544.19s, remaining=830.21s.\n",
      "Batch 5200: loss=0.512733, elapsed=554.71s, remaining=819.38s.\n",
      "Batch 5300: loss=0.512755, elapsed=565.25s, remaining=808.54s.\n",
      "Batch 5400: loss=0.513133, elapsed=576.04s, remaining=798.10s.\n",
      "Batch 5500: loss=0.512411, elapsed=586.95s, remaining=787.83s.\n",
      "Batch 5600: loss=0.513249, elapsed=597.55s, remaining=777.20s.\n",
      "Batch 5700: loss=0.512728, elapsed=608.37s, remaining=766.81s.\n",
      "Batch 5800: loss=0.512974, elapsed=619.01s, remaining=756.09s.\n",
      "Batch 5900: loss=0.512730, elapsed=629.73s, remaining=745.51s.\n",
      "Batch 6000: loss=0.512781, elapsed=640.29s, remaining=734.67s.\n",
      "Batch 6100: loss=0.512535, elapsed=650.89s, remaining=724.03s.\n",
      "Batch 6200: loss=0.512170, elapsed=661.45s, remaining=713.20s.\n",
      "Batch 6300: loss=0.511269, elapsed=672.44s, remaining=702.98s.\n",
      "Batch 6400: loss=0.511765, elapsed=683.24s, remaining=692.54s.\n",
      "Batch 6500: loss=0.511735, elapsed=693.91s, remaining=681.91s.\n",
      "Batch 6600: loss=0.511822, elapsed=704.60s, remaining=671.31s.\n",
      "Batch 6700: loss=0.511439, elapsed=715.13s, remaining=660.51s.\n",
      "Batch 6800: loss=0.511384, elapsed=726.00s, remaining=650.06s.\n",
      "Batch 6900: loss=0.511153, elapsed=736.46s, remaining=639.15s.\n",
      "Batch 7000: loss=0.511627, elapsed=747.07s, remaining=628.38s.\n",
      "Batch 7100: loss=0.511231, elapsed=757.55s, remaining=617.57s.\n",
      "Batch 7200: loss=0.510838, elapsed=768.16s, remaining=606.82s.\n",
      "Batch 7300: loss=0.511015, elapsed=778.72s, remaining=596.01s.\n",
      "Batch 7400: loss=0.511415, elapsed=789.31s, remaining=585.17s.\n",
      "Batch 7500: loss=0.511110, elapsed=800.01s, remaining=574.48s.\n",
      "Batch 7600: loss=0.511280, elapsed=810.74s, remaining=563.79s.\n",
      "Batch 7700: loss=0.510786, elapsed=821.66s, remaining=553.32s.\n",
      "Batch 7800: loss=0.510442, elapsed=832.17s, remaining=542.48s.\n",
      "Batch 7900: loss=0.510311, elapsed=842.82s, remaining=531.80s.\n",
      "Batch 8000: loss=0.510237, elapsed=853.28s, remaining=521.02s.\n",
      "Batch 8100: loss=0.509548, elapsed=863.86s, remaining=510.34s.\n",
      "Batch 8200: loss=0.509841, elapsed=874.52s, remaining=499.61s.\n",
      "Batch 8300: loss=0.509977, elapsed=885.19s, remaining=488.95s.\n",
      "Batch 8400: loss=0.510138, elapsed=896.03s, remaining=478.27s.\n",
      "Batch 8500: loss=0.509897, elapsed=906.52s, remaining=467.42s.\n",
      "Batch 8600: loss=0.509644, elapsed=916.97s, remaining=456.65s.\n",
      "Batch 8700: loss=0.509617, elapsed=927.61s, remaining=446.01s.\n",
      "Batch 8800: loss=0.509534, elapsed=938.27s, remaining=435.34s.\n",
      "Batch 8900: loss=0.509601, elapsed=948.80s, remaining=424.63s.\n",
      "Batch 9000: loss=0.509536, elapsed=959.38s, remaining=413.97s.\n",
      "Batch 9100: loss=0.509131, elapsed=970.06s, remaining=403.32s.\n",
      "Batch 9200: loss=0.508933, elapsed=980.77s, remaining=392.67s.\n",
      "Batch 9300: loss=0.508998, elapsed=991.77s, remaining=382.13s.\n",
      "Batch 9400: loss=0.509009, elapsed=1002.41s, remaining=371.44s.\n",
      "Batch 9500: loss=0.508626, elapsed=1013.14s, remaining=360.79s.\n",
      "Batch 9600: loss=0.508489, elapsed=1024.80s, remaining=350.49s.\n",
      "Batch 9700: loss=0.508900, elapsed=1035.78s, remaining=339.90s.\n",
      "Batch 9800: loss=0.508402, elapsed=1047.00s, remaining=329.39s.\n",
      "Batch 9900: loss=0.508335, elapsed=1057.77s, remaining=318.72s.\n",
      "Batch 10000: loss=0.508569, elapsed=1068.25s, remaining=307.95s.\n",
      "Batch 10100: loss=0.508018, elapsed=1078.71s, remaining=297.18s.\n",
      "Batch 10200: loss=0.508318, elapsed=1089.35s, remaining=286.50s.\n",
      "Batch 10300: loss=0.507664, elapsed=1099.82s, remaining=275.78s.\n",
      "Batch 10400: loss=0.507737, elapsed=1110.49s, remaining=265.11s.\n",
      "Batch 10500: loss=0.507917, elapsed=1121.21s, remaining=254.45s.\n",
      "Batch 10600: loss=0.507908, elapsed=1131.75s, remaining=243.73s.\n",
      "Batch 10700: loss=0.507811, elapsed=1142.38s, remaining=233.03s.\n",
      "Batch 10800: loss=0.507591, elapsed=1153.02s, remaining=222.35s.\n",
      "Batch 10900: loss=0.507055, elapsed=1163.84s, remaining=211.71s.\n",
      "Batch 11000: loss=0.506823, elapsed=1174.63s, remaining=201.06s.\n",
      "Batch 11100: loss=0.506638, elapsed=1185.17s, remaining=190.35s.\n",
      "Batch 11200: loss=0.506224, elapsed=1195.65s, remaining=179.66s.\n",
      "Batch 11300: loss=0.505623, elapsed=1206.36s, remaining=168.98s.\n",
      "Batch 11400: loss=0.505745, elapsed=1217.00s, remaining=158.30s.\n",
      "Batch 11500: loss=0.505826, elapsed=1227.61s, remaining=147.65s.\n",
      "Batch 11600: loss=0.505737, elapsed=1238.43s, remaining=137.03s.\n",
      "Batch 11700: loss=0.505656, elapsed=1249.33s, remaining=126.38s.\n",
      "Batch 11800: loss=0.505828, elapsed=1260.02s, remaining=115.68s.\n",
      "Batch 11900: loss=0.505346, elapsed=1270.82s, remaining=105.03s.\n",
      "Batch 12000: loss=0.505157, elapsed=1281.62s, remaining=94.30s.\n",
      "Batch 12100: loss=0.504903, elapsed=1292.59s, remaining=83.57s.\n",
      "Batch 12200: loss=0.504617, elapsed=1303.09s, remaining=72.92s.\n",
      "Batch 12300: loss=0.504523, elapsed=1313.46s, remaining=62.21s.\n",
      "Batch 12400: loss=0.504545, elapsed=1324.23s, remaining=51.54s.\n",
      "Batch 12500: loss=0.504627, elapsed=1334.87s, remaining=40.91s.\n",
      "Batch 12600: loss=0.504339, elapsed=1345.69s, remaining=30.15s.\n",
      "Batch 12700: loss=0.504325, elapsed=1356.56s, remaining=19.46s.\n",
      "Batch 12800: loss=0.504517, elapsed=1367.08s, remaining=8.79s.\n",
      "Batch 12900: loss=0.504195, elapsed=1377.90s, remaining=-1.89s.\n",
      "Batch 13000: loss=0.504120, elapsed=1388.70s, remaining=-12.58s.\n",
      "Batch 13100: loss=0.504026, elapsed=1399.20s, remaining=-23.26s.\n",
      "Batch 13200: loss=0.504137, elapsed=1409.89s, remaining=-33.93s.\n",
      "\n",
      "Training epoch took: 1419.00s\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.896011 \n",
      "\n",
      "Accuracy - Sentiment: 86.2%, Avg loss: 0.896011 \n",
      "\n",
      "Accuracy - Topic: 85.3%, Avg loss: 0.896011 \n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.363740, elapsed=10.74s, remaining=1355.07s.\n",
      "Batch 200: loss=0.337559, elapsed=21.51s, remaining=1356.43s.\n",
      "Batch 300: loss=0.365086, elapsed=32.15s, remaining=1343.50s.\n",
      "Batch 400: loss=0.376684, elapsed=42.50s, remaining=1322.93s.\n",
      "Batch 500: loss=0.377281, elapsed=53.04s, remaining=1310.61s.\n",
      "Batch 600: loss=0.372843, elapsed=63.67s, remaining=1300.98s.\n",
      "Batch 700: loss=0.366339, elapsed=74.25s, remaining=1290.19s.\n",
      "Batch 800: loss=0.370680, elapsed=84.58s, remaining=1275.18s.\n",
      "Batch 900: loss=0.371032, elapsed=95.09s, remaining=1264.12s.\n",
      "Batch 1000: loss=0.372552, elapsed=105.69s, remaining=1254.10s.\n",
      "Batch 1100: loss=0.369173, elapsed=116.23s, remaining=1243.33s.\n",
      "Batch 1200: loss=0.368387, elapsed=126.96s, remaining=1234.25s.\n",
      "Batch 1300: loss=0.367814, elapsed=137.67s, remaining=1224.77s.\n",
      "Batch 1400: loss=0.369477, elapsed=148.02s, remaining=1212.35s.\n",
      "Batch 1500: loss=0.369758, elapsed=158.74s, remaining=1203.02s.\n",
      "Batch 1600: loss=0.369482, elapsed=169.33s, remaining=1192.53s.\n",
      "Batch 1700: loss=0.370028, elapsed=180.42s, remaining=1184.94s.\n",
      "Batch 1800: loss=0.370122, elapsed=191.00s, remaining=1174.34s.\n",
      "Batch 1900: loss=0.370694, elapsed=201.65s, remaining=1164.57s.\n",
      "Batch 2000: loss=0.369594, elapsed=212.09s, remaining=1153.30s.\n",
      "Batch 2100: loss=0.370133, elapsed=222.73s, remaining=1143.16s.\n",
      "Batch 2200: loss=0.368924, elapsed=233.18s, remaining=1131.65s.\n",
      "Batch 2300: loss=0.368869, elapsed=243.73s, remaining=1121.00s.\n",
      "Batch 2400: loss=0.368094, elapsed=254.47s, remaining=1111.11s.\n",
      "Batch 2500: loss=0.365778, elapsed=265.01s, remaining=1100.17s.\n",
      "Batch 2600: loss=0.365724, elapsed=275.58s, remaining=1089.38s.\n",
      "Batch 2700: loss=0.365435, elapsed=286.00s, remaining=1078.09s.\n",
      "Batch 2800: loss=0.366834, elapsed=296.73s, remaining=1067.97s.\n",
      "Batch 2900: loss=0.365635, elapsed=307.53s, remaining=1058.12s.\n",
      "Batch 3000: loss=0.367000, elapsed=317.81s, remaining=1046.12s.\n",
      "Batch 3100: loss=0.366477, elapsed=328.61s, remaining=1036.21s.\n",
      "Batch 3200: loss=0.367459, elapsed=339.28s, remaining=1025.84s.\n",
      "Batch 3300: loss=0.367555, elapsed=349.85s, remaining=1015.17s.\n",
      "Batch 3400: loss=0.367249, elapsed=360.80s, remaining=1005.57s.\n",
      "Batch 3500: loss=0.367356, elapsed=371.58s, remaining=995.43s.\n",
      "Batch 3600: loss=0.367629, elapsed=382.16s, remaining=984.75s.\n",
      "Batch 3700: loss=0.369323, elapsed=392.75s, remaining=974.17s.\n",
      "Batch 3800: loss=0.369309, elapsed=403.61s, remaining=964.18s.\n",
      "Batch 3900: loss=0.368525, elapsed=414.13s, remaining=953.35s.\n",
      "Batch 4000: loss=0.368777, elapsed=424.66s, remaining=942.58s.\n",
      "Batch 4100: loss=0.369059, elapsed=434.87s, remaining=931.08s.\n",
      "Batch 4200: loss=0.368815, elapsed=445.21s, remaining=919.95s.\n",
      "Batch 4300: loss=0.368611, elapsed=455.78s, remaining=909.26s.\n",
      "Batch 4400: loss=0.367876, elapsed=466.29s, remaining=898.46s.\n",
      "Batch 4500: loss=0.367685, elapsed=476.80s, remaining=887.72s.\n",
      "Batch 4600: loss=0.367587, elapsed=487.33s, remaining=877.08s.\n",
      "Batch 4700: loss=0.367859, elapsed=497.82s, remaining=866.35s.\n",
      "Batch 4800: loss=0.367726, elapsed=508.19s, remaining=855.36s.\n",
      "Batch 4900: loss=0.368164, elapsed=518.91s, remaining=845.01s.\n",
      "Batch 5000: loss=0.367956, elapsed=529.56s, remaining=834.55s.\n",
      "Batch 5100: loss=0.368629, elapsed=540.01s, remaining=823.79s.\n",
      "Batch 5200: loss=0.368618, elapsed=550.43s, remaining=813.03s.\n",
      "Batch 5300: loss=0.368353, elapsed=561.12s, remaining=802.58s.\n",
      "Batch 5400: loss=0.367546, elapsed=571.68s, remaining=791.89s.\n",
      "Batch 5500: loss=0.368207, elapsed=582.12s, remaining=781.33s.\n",
      "Batch 5600: loss=0.367368, elapsed=592.43s, remaining=770.45s.\n",
      "Batch 5700: loss=0.367348, elapsed=602.89s, remaining=759.89s.\n",
      "Batch 5800: loss=0.366692, elapsed=613.28s, remaining=749.40s.\n",
      "Batch 5900: loss=0.366647, elapsed=623.85s, remaining=738.62s.\n",
      "Batch 6000: loss=0.366940, elapsed=634.40s, remaining=728.09s.\n",
      "Batch 6100: loss=0.367481, elapsed=644.97s, remaining=717.67s.\n",
      "Batch 6200: loss=0.367548, elapsed=655.38s, remaining=706.88s.\n",
      "Batch 6300: loss=0.366901, elapsed=665.74s, remaining=696.02s.\n",
      "Batch 6400: loss=0.366545, elapsed=676.11s, remaining=685.31s.\n",
      "Batch 6500: loss=0.366271, elapsed=686.44s, remaining=674.55s.\n",
      "Batch 6600: loss=0.366291, elapsed=696.99s, remaining=663.99s.\n",
      "Batch 6700: loss=0.367166, elapsed=707.59s, remaining=653.57s.\n",
      "Batch 6800: loss=0.366529, elapsed=718.05s, remaining=642.73s.\n",
      "Batch 6900: loss=0.366233, elapsed=728.50s, remaining=632.12s.\n",
      "Batch 7000: loss=0.365837, elapsed=738.88s, remaining=621.63s.\n",
      "Batch 7100: loss=0.365600, elapsed=749.59s, remaining=611.23s.\n",
      "Batch 7200: loss=0.365951, elapsed=760.25s, remaining=600.83s.\n",
      "Batch 7300: loss=0.366133, elapsed=770.84s, remaining=590.34s.\n",
      "Batch 7400: loss=0.366110, elapsed=781.43s, remaining=579.94s.\n",
      "Batch 7500: loss=0.366651, elapsed=791.76s, remaining=569.37s.\n",
      "Batch 7600: loss=0.366686, elapsed=802.25s, remaining=558.83s.\n",
      "Batch 7700: loss=0.366365, elapsed=812.55s, remaining=548.14s.\n",
      "Batch 7800: loss=0.366223, elapsed=822.87s, remaining=537.36s.\n",
      "Batch 7900: loss=0.366882, elapsed=833.40s, remaining=526.84s.\n",
      "Batch 8000: loss=0.367212, elapsed=844.06s, remaining=516.36s.\n",
      "Batch 8100: loss=0.367597, elapsed=854.49s, remaining=505.79s.\n",
      "Batch 8200: loss=0.367589, elapsed=865.22s, remaining=495.42s.\n",
      "Batch 8300: loss=0.367724, elapsed=875.68s, remaining=484.94s.\n",
      "Batch 8400: loss=0.367519, elapsed=886.24s, remaining=474.36s.\n",
      "Batch 8500: loss=0.367124, elapsed=896.51s, remaining=463.52s.\n",
      "Batch 8600: loss=0.366690, elapsed=906.75s, remaining=452.88s.\n",
      "Batch 8700: loss=0.366941, elapsed=917.53s, remaining=442.43s.\n",
      "Batch 8800: loss=0.367234, elapsed=928.11s, remaining=431.89s.\n",
      "Batch 8900: loss=0.366642, elapsed=938.49s, remaining=421.25s.\n",
      "Batch 9000: loss=0.366331, elapsed=948.91s, remaining=410.61s.\n",
      "Batch 9100: loss=0.366153, elapsed=959.39s, remaining=400.03s.\n",
      "Batch 9200: loss=0.366235, elapsed=969.71s, remaining=389.29s.\n",
      "Batch 9300: loss=0.365935, elapsed=980.26s, remaining=378.65s.\n",
      "Batch 9400: loss=0.366245, elapsed=990.59s, remaining=368.02s.\n",
      "Batch 9500: loss=0.366470, elapsed=1000.79s, remaining=357.34s.\n",
      "Batch 9600: loss=0.365888, elapsed=1011.36s, remaining=346.72s.\n",
      "Batch 9700: loss=0.365471, elapsed=1021.80s, remaining=336.09s.\n",
      "Batch 9800: loss=0.365583, elapsed=1032.15s, remaining=325.50s.\n",
      "Batch 9900: loss=0.365775, elapsed=1042.65s, remaining=314.95s.\n",
      "Batch 10000: loss=0.365747, elapsed=1053.12s, remaining=304.36s.\n",
      "Batch 10100: loss=0.366167, elapsed=1063.66s, remaining=293.91s.\n",
      "Batch 10200: loss=0.365900, elapsed=1074.08s, remaining=283.43s.\n",
      "Batch 10300: loss=0.365724, elapsed=1084.71s, remaining=273.03s.\n",
      "Batch 10400: loss=0.366019, elapsed=1095.23s, remaining=262.49s.\n",
      "Batch 10500: loss=0.365672, elapsed=1105.90s, remaining=251.94s.\n",
      "Batch 10600: loss=0.365659, elapsed=1116.50s, remaining=241.36s.\n",
      "Batch 10700: loss=0.365611, elapsed=1126.86s, remaining=230.78s.\n",
      "Batch 10800: loss=0.365353, elapsed=1137.28s, remaining=220.22s.\n",
      "Batch 10900: loss=0.365540, elapsed=1147.63s, remaining=209.63s.\n",
      "Batch 11000: loss=0.365334, elapsed=1158.43s, remaining=199.04s.\n",
      "Batch 11100: loss=0.364755, elapsed=1169.41s, remaining=188.51s.\n",
      "Batch 11200: loss=0.364460, elapsed=1180.05s, remaining=178.00s.\n",
      "Batch 11300: loss=0.364278, elapsed=1190.98s, remaining=167.52s.\n",
      "Batch 11400: loss=0.364259, elapsed=1201.29s, remaining=156.95s.\n",
      "Batch 11500: loss=0.363754, elapsed=1211.79s, remaining=146.41s.\n",
      "Batch 11600: loss=0.363829, elapsed=1222.44s, remaining=135.88s.\n",
      "Batch 11700: loss=0.363759, elapsed=1233.04s, remaining=125.30s.\n",
      "Batch 11800: loss=0.363493, elapsed=1243.65s, remaining=114.78s.\n",
      "Batch 11900: loss=0.363386, elapsed=1254.09s, remaining=104.23s.\n",
      "Batch 12000: loss=0.363280, elapsed=1264.72s, remaining=93.66s.\n",
      "Batch 12100: loss=0.363199, elapsed=1275.29s, remaining=83.13s.\n",
      "Batch 12200: loss=0.362593, elapsed=1285.94s, remaining=72.59s.\n",
      "Batch 12300: loss=0.362388, elapsed=1296.49s, remaining=62.14s.\n",
      "Batch 12400: loss=0.362160, elapsed=1306.92s, remaining=51.57s.\n",
      "Batch 12500: loss=0.361653, elapsed=1317.37s, remaining=41.15s.\n",
      "Batch 12600: loss=0.361575, elapsed=1328.01s, remaining=30.62s.\n",
      "Batch 12700: loss=0.361092, elapsed=1338.74s, remaining=20.12s.\n",
      "Batch 12800: loss=0.361383, elapsed=1349.32s, remaining=9.59s.\n",
      "Batch 12900: loss=0.361298, elapsed=1359.91s, remaining=-0.91s.\n",
      "Batch 13000: loss=0.361333, elapsed=1370.37s, remaining=-11.48s.\n",
      "Batch 13100: loss=0.361054, elapsed=1381.03s, remaining=-21.99s.\n",
      "Batch 13200: loss=0.361101, elapsed=1391.37s, remaining=-32.55s.\n",
      "\n",
      "Training epoch took: 1400.15s\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.955040 \n",
      "\n",
      "Accuracy - Sentiment: 87.1%, Avg loss: 0.955040 \n",
      "\n",
      "Accuracy - Topic: 86.0%, Avg loss: 0.955040 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    timing_log = train_loop(train_dataloader, model,optimizer, scheduler, device, criterion_sent, criterion_topic, sentiment_var='sentiment',\n",
    "               topic_var='topic', timing_log=True)\n",
    "    eval_loop(eval_dataloader, model, device, criterion_sent, criterion_topic, sentiment_var='sentiment', topic_var='topic')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/manifesto_ContextScalePrediction_dg/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 45.44s, Estimated remaining time: 50.39s\n",
      "Elapsed time: 91.18s, Estimated remaining time: 4.97s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_dg = scale_func(test_dataloader, \n",
    "               model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  precision  recall  accuracy\n",
       "0   0.84       0.83    0.84      0.84\n",
       "1   0.86       0.85    0.87      0.87\n",
       "2   0.89       0.89    0.90      0.90\n",
       "3   0.89       0.90    0.89      0.89\n",
       "4   0.87       0.87    0.87      0.87\n",
       "5   0.83       0.84    0.82      0.82\n",
       "6   0.80       0.79    0.80      0.80\n",
       "7   0.86       0.85    0.88      0.88\n",
       "8   0.89       0.89    0.90      0.90\n",
       "9   0.90       0.91    0.88      0.88\n",
       "10  0.82       0.83    0.81      0.81\n",
       "11  0.82       0.83    0.82      0.82"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dg['res_table_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dg['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  precision  recall  accuracy\n",
       "0  0.91       0.90    0.91      0.91\n",
       "1  0.84       0.85    0.83      0.83\n",
       "2  0.82       0.83    0.81      0.81"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dg['res_table_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dg['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_dg'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_dg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dg['res_table_sentiment'].to_csv('results/classification results/dg_sentiment.csv', index=False)\n",
    "outputs_dg['res_table_topic'].to_csv('results/classification results/dg_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model, using 90% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 303670\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 33742\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_datasets = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "manifesto_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c9138b284a44fa9e3945fd03be55c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/303670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8870ff2afad144d088003174cf03d12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['topic', 'sentiment', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = manifesto_datasets.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text', 'topic_sentiment'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_shared_attention=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion_sent = nn.CrossEntropyLoss()\n",
    "criterion_topic =  nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.756312, elapsed=10.85s, remaining=1963.96s.\n",
      "Batch 200: loss=3.712485, elapsed=21.22s, remaining=1920.29s.\n",
      "Batch 300: loss=3.623567, elapsed=31.88s, remaining=1917.49s.\n",
      "Batch 400: loss=3.548142, elapsed=42.55s, remaining=1912.07s.\n",
      "Batch 500: loss=3.488375, elapsed=53.44s, remaining=1914.35s.\n",
      "Batch 600: loss=3.419642, elapsed=64.35s, remaining=1910.62s.\n",
      "Batch 700: loss=3.352318, elapsed=74.97s, remaining=1895.51s.\n",
      "Batch 800: loss=3.266815, elapsed=85.50s, remaining=1881.09s.\n",
      "Batch 900: loss=3.181134, elapsed=96.25s, remaining=1873.05s.\n",
      "Batch 1000: loss=3.090784, elapsed=106.64s, remaining=1855.91s.\n",
      "Batch 1100: loss=3.012903, elapsed=117.22s, remaining=1844.84s.\n",
      "Batch 1200: loss=2.933968, elapsed=127.76s, remaining=1833.88s.\n",
      "Batch 1300: loss=2.862694, elapsed=138.71s, remaining=1827.22s.\n",
      "Batch 1400: loss=2.798119, elapsed=149.27s, remaining=1815.74s.\n",
      "Batch 1500: loss=2.735228, elapsed=160.09s, remaining=1806.58s.\n",
      "Batch 1600: loss=2.680820, elapsed=171.07s, remaining=1798.64s.\n",
      "Batch 1700: loss=2.629391, elapsed=181.78s, remaining=1788.62s.\n",
      "Batch 1800: loss=2.583025, elapsed=192.41s, remaining=1777.41s.\n",
      "Batch 1900: loss=2.539437, elapsed=203.16s, remaining=1767.72s.\n",
      "Batch 2000: loss=2.498158, elapsed=213.61s, remaining=1754.87s.\n",
      "Batch 2100: loss=2.457094, elapsed=224.23s, remaining=1743.35s.\n",
      "Batch 2200: loss=2.420523, elapsed=234.67s, remaining=1731.32s.\n",
      "Batch 2300: loss=2.386407, elapsed=245.31s, remaining=1720.53s.\n",
      "Batch 2400: loss=2.354359, elapsed=256.28s, remaining=1712.02s.\n",
      "Batch 2500: loss=2.323627, elapsed=266.84s, remaining=1700.57s.\n",
      "Batch 2600: loss=2.295324, elapsed=277.44s, remaining=1689.20s.\n",
      "Batch 2700: loss=2.270762, elapsed=288.11s, remaining=1678.34s.\n",
      "Batch 2800: loss=2.243181, elapsed=298.68s, remaining=1667.35s.\n",
      "Batch 2900: loss=2.217878, elapsed=309.64s, remaining=1658.54s.\n",
      "Batch 3000: loss=2.197130, elapsed=320.20s, remaining=1647.68s.\n",
      "Batch 3100: loss=2.171904, elapsed=330.76s, remaining=1636.45s.\n",
      "Batch 3200: loss=2.151428, elapsed=341.45s, remaining=1625.71s.\n",
      "Batch 3300: loss=2.132544, elapsed=351.87s, remaining=1613.45s.\n",
      "Batch 3400: loss=2.113356, elapsed=362.64s, remaining=1603.12s.\n",
      "Batch 3500: loss=2.095486, elapsed=373.35s, remaining=1592.66s.\n",
      "Batch 3600: loss=2.079979, elapsed=384.14s, remaining=1582.18s.\n",
      "Batch 3700: loss=2.062843, elapsed=395.16s, remaining=1572.94s.\n",
      "Batch 3800: loss=2.045164, elapsed=406.01s, remaining=1563.01s.\n",
      "Batch 3900: loss=2.030160, elapsed=416.97s, remaining=1553.32s.\n",
      "Batch 4000: loss=2.015310, elapsed=427.92s, remaining=1543.74s.\n",
      "Batch 4100: loss=2.000662, elapsed=439.00s, remaining=1534.59s.\n",
      "Batch 4200: loss=1.986492, elapsed=450.06s, remaining=1524.77s.\n",
      "Batch 4300: loss=1.971620, elapsed=460.82s, remaining=1514.24s.\n",
      "Batch 4400: loss=1.958096, elapsed=471.64s, remaining=1504.07s.\n",
      "Batch 4500: loss=1.945803, elapsed=482.47s, remaining=1493.53s.\n",
      "Batch 4600: loss=1.935043, elapsed=492.98s, remaining=1482.09s.\n",
      "Batch 4700: loss=1.922588, elapsed=503.75s, remaining=1471.49s.\n",
      "Batch 4800: loss=1.910900, elapsed=514.04s, remaining=1459.57s.\n",
      "Batch 4900: loss=1.901335, elapsed=524.77s, remaining=1448.95s.\n",
      "Batch 5000: loss=1.892276, elapsed=535.18s, remaining=1437.46s.\n",
      "Batch 5100: loss=1.881679, elapsed=545.65s, remaining=1426.15s.\n",
      "Batch 5200: loss=1.870870, elapsed=556.09s, remaining=1414.77s.\n",
      "Batch 5300: loss=1.863083, elapsed=566.46s, remaining=1403.28s.\n",
      "Batch 5400: loss=1.854563, elapsed=576.72s, remaining=1391.57s.\n",
      "Batch 5500: loss=1.846406, elapsed=587.31s, remaining=1380.70s.\n",
      "Batch 5600: loss=1.838549, elapsed=597.74s, remaining=1369.46s.\n",
      "Batch 5700: loss=1.830418, elapsed=608.05s, remaining=1357.92s.\n",
      "Batch 5800: loss=1.821860, elapsed=618.73s, remaining=1347.33s.\n",
      "Batch 5900: loss=1.812970, elapsed=628.97s, remaining=1335.76s.\n",
      "Batch 6000: loss=1.805126, elapsed=639.47s, remaining=1324.80s.\n",
      "Batch 6100: loss=1.797508, elapsed=649.80s, remaining=1313.44s.\n",
      "Batch 6200: loss=1.789499, elapsed=660.05s, remaining=1301.97s.\n",
      "Batch 6300: loss=1.782644, elapsed=670.54s, remaining=1291.04s.\n",
      "Batch 6400: loss=1.775707, elapsed=681.10s, remaining=1280.27s.\n",
      "Batch 6500: loss=1.768903, elapsed=691.46s, remaining=1269.14s.\n",
      "Batch 6600: loss=1.762204, elapsed=701.94s, remaining=1258.23s.\n",
      "Batch 6700: loss=1.755332, elapsed=712.40s, remaining=1247.30s.\n",
      "Batch 6800: loss=1.749803, elapsed=723.06s, remaining=1236.74s.\n",
      "Batch 6900: loss=1.744277, elapsed=733.46s, remaining=1225.74s.\n",
      "Batch 7000: loss=1.738470, elapsed=744.03s, remaining=1215.07s.\n",
      "Batch 7100: loss=1.731965, elapsed=754.68s, remaining=1204.48s.\n",
      "Batch 7200: loss=1.725595, elapsed=765.13s, remaining=1193.57s.\n",
      "Batch 7300: loss=1.721656, elapsed=775.61s, remaining=1182.69s.\n",
      "Batch 7400: loss=1.715296, elapsed=786.07s, remaining=1171.85s.\n",
      "Batch 7500: loss=1.711053, elapsed=796.58s, remaining=1161.05s.\n",
      "Batch 7600: loss=1.705551, elapsed=806.97s, remaining=1150.14s.\n",
      "Batch 7700: loss=1.699887, elapsed=817.71s, remaining=1139.62s.\n",
      "Batch 7800: loss=1.694918, elapsed=828.49s, remaining=1129.13s.\n",
      "Batch 7900: loss=1.689968, elapsed=838.81s, remaining=1118.08s.\n",
      "Batch 8000: loss=1.685264, elapsed=849.06s, remaining=1106.96s.\n",
      "Batch 8100: loss=1.679936, elapsed=859.40s, remaining=1096.01s.\n",
      "Batch 8200: loss=1.675179, elapsed=869.89s, remaining=1085.28s.\n",
      "Batch 8300: loss=1.670883, elapsed=880.37s, remaining=1074.53s.\n",
      "Batch 8400: loss=1.666634, elapsed=890.78s, remaining=1063.71s.\n",
      "Batch 8500: loss=1.661493, elapsed=901.26s, remaining=1052.98s.\n",
      "Batch 8600: loss=1.656261, elapsed=911.60s, remaining=1042.06s.\n",
      "Batch 8700: loss=1.651610, elapsed=922.14s, remaining=1031.42s.\n",
      "Batch 8800: loss=1.647540, elapsed=932.41s, remaining=1020.45s.\n",
      "Batch 8900: loss=1.643400, elapsed=942.78s, remaining=1009.60s.\n",
      "Batch 9000: loss=1.639798, elapsed=953.10s, remaining=998.72s.\n",
      "Batch 9100: loss=1.636237, elapsed=963.24s, remaining=987.67s.\n",
      "Batch 9200: loss=1.632533, elapsed=973.76s, remaining=977.03s.\n",
      "Batch 9300: loss=1.628980, elapsed=984.26s, remaining=966.39s.\n",
      "Batch 9400: loss=1.624900, elapsed=994.61s, remaining=955.61s.\n",
      "Batch 9500: loss=1.620890, elapsed=1005.04s, remaining=944.90s.\n",
      "Batch 9600: loss=1.617540, elapsed=1015.62s, remaining=934.36s.\n",
      "Batch 9700: loss=1.613801, elapsed=1026.01s, remaining=923.62s.\n",
      "Batch 9800: loss=1.610276, elapsed=1036.32s, remaining=912.82s.\n",
      "Batch 9900: loss=1.607069, elapsed=1046.65s, remaining=902.04s.\n",
      "Batch 10000: loss=1.603083, elapsed=1057.26s, remaining=891.48s.\n",
      "Batch 10100: loss=1.600236, elapsed=1067.93s, remaining=880.99s.\n",
      "Batch 10200: loss=1.596596, elapsed=1078.41s, remaining=870.36s.\n",
      "Batch 10300: loss=1.593815, elapsed=1088.70s, remaining=859.57s.\n",
      "Batch 10400: loss=1.590512, elapsed=1099.04s, remaining=848.83s.\n",
      "Batch 10500: loss=1.587141, elapsed=1109.59s, remaining=838.27s.\n",
      "Batch 10600: loss=1.584016, elapsed=1120.06s, remaining=827.63s.\n",
      "Batch 10700: loss=1.581216, elapsed=1130.51s, remaining=817.00s.\n",
      "Batch 10800: loss=1.577938, elapsed=1140.84s, remaining=806.26s.\n",
      "Batch 10900: loss=1.575006, elapsed=1151.32s, remaining=795.63s.\n",
      "Batch 11000: loss=1.572051, elapsed=1161.73s, remaining=784.98s.\n",
      "Batch 11100: loss=1.568889, elapsed=1172.28s, remaining=774.43s.\n",
      "Batch 11200: loss=1.566223, elapsed=1182.95s, remaining=763.96s.\n",
      "Batch 11300: loss=1.562749, elapsed=1193.42s, remaining=753.38s.\n",
      "Batch 11400: loss=1.560160, elapsed=1204.00s, remaining=742.86s.\n",
      "Batch 11500: loss=1.557496, elapsed=1214.47s, remaining=732.25s.\n",
      "Batch 11600: loss=1.554769, elapsed=1224.85s, remaining=721.60s.\n",
      "Batch 11700: loss=1.551362, elapsed=1235.17s, remaining=710.92s.\n",
      "Batch 11800: loss=1.548495, elapsed=1245.47s, remaining=700.23s.\n",
      "Batch 11900: loss=1.545344, elapsed=1255.87s, remaining=689.60s.\n",
      "Batch 12000: loss=1.542161, elapsed=1266.19s, remaining=678.93s.\n",
      "Batch 12100: loss=1.539150, elapsed=1276.66s, remaining=668.35s.\n",
      "Batch 12200: loss=1.536311, elapsed=1287.21s, remaining=657.82s.\n",
      "Batch 12300: loss=1.533714, elapsed=1297.64s, remaining=647.20s.\n",
      "Batch 12400: loss=1.530599, elapsed=1307.82s, remaining=636.48s.\n",
      "Batch 12500: loss=1.527547, elapsed=1318.16s, remaining=625.86s.\n",
      "Batch 12600: loss=1.525248, elapsed=1328.47s, remaining=615.20s.\n",
      "Batch 12700: loss=1.522249, elapsed=1338.90s, remaining=604.62s.\n",
      "Batch 12800: loss=1.519240, elapsed=1349.42s, remaining=594.08s.\n",
      "Batch 12900: loss=1.516828, elapsed=1360.00s, remaining=583.57s.\n",
      "Batch 13000: loss=1.514351, elapsed=1370.38s, remaining=572.97s.\n",
      "Batch 13100: loss=1.511840, elapsed=1380.65s, remaining=562.33s.\n",
      "Batch 13200: loss=1.509324, elapsed=1391.07s, remaining=551.76s.\n",
      "Batch 13300: loss=1.507124, elapsed=1401.45s, remaining=541.17s.\n",
      "Batch 13400: loss=1.504632, elapsed=1411.88s, remaining=530.60s.\n",
      "Batch 13500: loss=1.502542, elapsed=1422.22s, remaining=520.00s.\n",
      "Batch 13600: loss=1.500478, elapsed=1432.67s, remaining=509.44s.\n",
      "Batch 13700: loss=1.498281, elapsed=1443.06s, remaining=498.87s.\n",
      "Batch 13800: loss=1.496137, elapsed=1453.38s, remaining=488.22s.\n",
      "Batch 13900: loss=1.493963, elapsed=1463.82s, remaining=477.66s.\n",
      "Batch 14000: loss=1.491587, elapsed=1474.30s, remaining=467.12s.\n",
      "Batch 14100: loss=1.488762, elapsed=1484.73s, remaining=456.57s.\n",
      "Batch 14200: loss=1.486384, elapsed=1494.93s, remaining=445.94s.\n",
      "Batch 14300: loss=1.484326, elapsed=1505.32s, remaining=435.37s.\n",
      "Batch 14400: loss=1.482107, elapsed=1515.91s, remaining=424.87s.\n",
      "Batch 14500: loss=1.479786, elapsed=1526.48s, remaining=414.35s.\n",
      "Batch 14600: loss=1.477168, elapsed=1536.82s, remaining=403.78s.\n",
      "Batch 14700: loss=1.474797, elapsed=1547.22s, remaining=393.22s.\n",
      "Batch 14800: loss=1.472603, elapsed=1557.75s, remaining=382.69s.\n",
      "Batch 14900: loss=1.470896, elapsed=1568.25s, remaining=372.15s.\n",
      "Batch 15000: loss=1.468676, elapsed=1578.62s, remaining=361.60s.\n",
      "Batch 15100: loss=1.466517, elapsed=1588.93s, remaining=351.05s.\n",
      "Batch 15200: loss=1.464583, elapsed=1599.33s, remaining=340.50s.\n",
      "Batch 15300: loss=1.462581, elapsed=1609.81s, remaining=329.99s.\n",
      "Batch 15400: loss=1.460188, elapsed=1620.29s, remaining=319.46s.\n",
      "Batch 15500: loss=1.457914, elapsed=1630.69s, remaining=308.93s.\n",
      "Batch 15600: loss=1.455829, elapsed=1641.08s, remaining=298.39s.\n",
      "Batch 15700: loss=1.453905, elapsed=1651.60s, remaining=287.87s.\n",
      "Batch 15800: loss=1.452164, elapsed=1662.31s, remaining=277.38s.\n",
      "Batch 15900: loss=1.450251, elapsed=1672.71s, remaining=266.84s.\n",
      "Batch 16000: loss=1.448102, elapsed=1683.00s, remaining=256.29s.\n",
      "Batch 16100: loss=1.445988, elapsed=1693.47s, remaining=245.78s.\n",
      "Batch 16200: loss=1.443852, elapsed=1703.86s, remaining=235.24s.\n",
      "Batch 16300: loss=1.441784, elapsed=1714.16s, remaining=224.69s.\n",
      "Batch 16400: loss=1.439655, elapsed=1724.26s, remaining=214.11s.\n",
      "Batch 16500: loss=1.437339, elapsed=1734.65s, remaining=203.59s.\n",
      "Batch 16600: loss=1.435221, elapsed=1745.05s, remaining=193.07s.\n",
      "Batch 16700: loss=1.433107, elapsed=1755.27s, remaining=182.53s.\n",
      "Batch 16800: loss=1.430494, elapsed=1765.77s, remaining=172.03s.\n",
      "Batch 16900: loss=1.428567, elapsed=1776.28s, remaining=161.52s.\n",
      "Batch 17000: loss=1.426761, elapsed=1786.92s, remaining=151.01s.\n",
      "Batch 17100: loss=1.424882, elapsed=1797.12s, remaining=140.48s.\n",
      "Batch 17200: loss=1.422978, elapsed=1807.44s, remaining=129.96s.\n",
      "Batch 17300: loss=1.421243, elapsed=1818.10s, remaining=119.47s.\n",
      "Batch 17400: loss=1.419014, elapsed=1828.61s, remaining=108.96s.\n",
      "Batch 17500: loss=1.417082, elapsed=1839.19s, remaining=98.46s.\n",
      "Batch 17600: loss=1.415678, elapsed=1849.74s, remaining=87.95s.\n",
      "Batch 17700: loss=1.414095, elapsed=1860.29s, remaining=77.45s.\n",
      "Batch 17800: loss=1.412557, elapsed=1870.81s, remaining=66.94s.\n",
      "Batch 17900: loss=1.411182, elapsed=1881.11s, remaining=56.44s.\n",
      "Batch 18000: loss=1.409180, elapsed=1891.51s, remaining=45.94s.\n",
      "Batch 18100: loss=1.407557, elapsed=1902.21s, remaining=35.44s.\n",
      "Batch 18200: loss=1.405553, elapsed=1912.73s, remaining=24.94s.\n",
      "Batch 18300: loss=1.403458, elapsed=1923.07s, remaining=14.42s.\n",
      "Batch 18400: loss=1.401754, elapsed=1933.60s, remaining=3.92s.\n",
      "Batch 18500: loss=1.400250, elapsed=1943.87s, remaining=-6.59s.\n",
      "Batch 18600: loss=1.398533, elapsed=1954.48s, remaining=-17.08s.\n",
      "Batch 18700: loss=1.396921, elapsed=1964.73s, remaining=-27.60s.\n",
      "Batch 18800: loss=1.395381, elapsed=1975.19s, remaining=-38.09s.\n",
      "Batch 18900: loss=1.393443, elapsed=1985.58s, remaining=-48.59s.\n",
      "\n",
      "Training epoch took: 1993.79s\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.917725, elapsed=10.28s, remaining=1861.85s.\n",
      "Batch 200: loss=0.939766, elapsed=20.57s, remaining=1863.38s.\n",
      "Batch 300: loss=0.935980, elapsed=31.13s, remaining=1873.24s.\n",
      "Batch 400: loss=0.935250, elapsed=41.80s, remaining=1878.54s.\n",
      "Batch 500: loss=0.924041, elapsed=52.27s, remaining=1870.53s.\n",
      "Batch 600: loss=0.921208, elapsed=62.56s, remaining=1856.29s.\n",
      "Batch 700: loss=0.921744, elapsed=73.04s, remaining=1847.75s.\n",
      "Batch 800: loss=0.927066, elapsed=83.50s, remaining=1838.35s.\n",
      "Batch 900: loss=0.932049, elapsed=94.03s, remaining=1829.97s.\n",
      "Batch 1000: loss=0.939018, elapsed=104.37s, remaining=1818.02s.\n",
      "Batch 1100: loss=0.938349, elapsed=114.87s, remaining=1808.99s.\n",
      "Batch 1200: loss=0.934927, elapsed=125.18s, remaining=1796.63s.\n",
      "Batch 1300: loss=0.937562, elapsed=135.78s, remaining=1788.64s.\n",
      "Batch 1400: loss=0.941360, elapsed=146.13s, remaining=1777.14s.\n",
      "Batch 1500: loss=0.941692, elapsed=156.67s, remaining=1767.78s.\n",
      "Batch 1600: loss=0.942700, elapsed=167.22s, remaining=1758.74s.\n",
      "Batch 1700: loss=0.941153, elapsed=177.54s, remaining=1747.03s.\n",
      "Batch 1800: loss=0.940045, elapsed=188.04s, remaining=1737.23s.\n",
      "Batch 1900: loss=0.940189, elapsed=198.73s, remaining=1729.10s.\n",
      "Batch 2000: loss=0.939981, elapsed=209.17s, remaining=1718.66s.\n",
      "Batch 2100: loss=0.942510, elapsed=219.65s, remaining=1708.41s.\n",
      "Batch 2200: loss=0.941046, elapsed=230.16s, remaining=1698.38s.\n",
      "Batch 2300: loss=0.942595, elapsed=240.72s, remaining=1688.59s.\n",
      "Batch 2400: loss=0.943230, elapsed=251.18s, remaining=1678.18s.\n",
      "Batch 2500: loss=0.941907, elapsed=261.72s, remaining=1668.16s.\n",
      "Batch 2600: loss=0.942060, elapsed=272.31s, remaining=1658.48s.\n",
      "Batch 2700: loss=0.940789, elapsed=282.97s, remaining=1649.16s.\n",
      "Batch 2800: loss=0.942506, elapsed=293.39s, remaining=1638.37s.\n",
      "Batch 2900: loss=0.942997, elapsed=303.68s, remaining=1626.92s.\n",
      "Batch 3000: loss=0.942781, elapsed=314.15s, remaining=1616.33s.\n",
      "Batch 3100: loss=0.942496, elapsed=324.52s, remaining=1605.35s.\n",
      "Batch 3200: loss=0.942666, elapsed=335.02s, remaining=1595.04s.\n",
      "Batch 3300: loss=0.943116, elapsed=345.46s, remaining=1584.39s.\n",
      "Batch 3400: loss=0.942398, elapsed=356.01s, remaining=1574.32s.\n",
      "Batch 3500: loss=0.943086, elapsed=366.35s, remaining=1563.30s.\n",
      "Batch 3600: loss=0.943364, elapsed=376.76s, remaining=1552.60s.\n",
      "Batch 3700: loss=0.943050, elapsed=387.05s, remaining=1541.50s.\n",
      "Batch 3800: loss=0.942355, elapsed=397.63s, remaining=1531.51s.\n",
      "Batch 3900: loss=0.942130, elapsed=408.22s, remaining=1521.57s.\n",
      "Batch 4000: loss=0.942120, elapsed=418.56s, remaining=1510.65s.\n",
      "Batch 4100: loss=0.942540, elapsed=429.32s, remaining=1501.27s.\n",
      "Batch 4200: loss=0.943490, elapsed=439.82s, remaining=1490.87s.\n",
      "Batch 4300: loss=0.942101, elapsed=450.43s, remaining=1480.88s.\n",
      "Batch 4400: loss=0.941944, elapsed=461.02s, remaining=1470.81s.\n",
      "Batch 4500: loss=0.941942, elapsed=471.59s, remaining=1460.61s.\n",
      "Batch 4600: loss=0.942122, elapsed=482.12s, remaining=1450.33s.\n",
      "Batch 4700: loss=0.943337, elapsed=492.41s, remaining=1439.31s.\n",
      "Batch 4800: loss=0.943507, elapsed=502.71s, remaining=1428.34s.\n",
      "Batch 4900: loss=0.943473, elapsed=513.13s, remaining=1417.72s.\n",
      "Batch 5000: loss=0.942906, elapsed=523.53s, remaining=1407.08s.\n",
      "Batch 5100: loss=0.942706, elapsed=534.06s, remaining=1396.76s.\n",
      "Batch 5200: loss=0.942213, elapsed=544.73s, remaining=1386.84s.\n",
      "Batch 5300: loss=0.941063, elapsed=554.89s, remaining=1375.60s.\n",
      "Batch 5400: loss=0.941451, elapsed=565.29s, remaining=1364.93s.\n",
      "Batch 5500: loss=0.941815, elapsed=575.74s, remaining=1354.44s.\n",
      "Batch 5600: loss=0.942293, elapsed=586.21s, remaining=1343.97s.\n",
      "Batch 5700: loss=0.942753, elapsed=596.62s, remaining=1333.39s.\n",
      "Batch 5800: loss=0.941129, elapsed=607.12s, remaining=1323.00s.\n",
      "Batch 5900: loss=0.941083, elapsed=617.91s, remaining=1313.24s.\n",
      "Batch 6000: loss=0.941141, elapsed=628.30s, remaining=1302.55s.\n",
      "Batch 6100: loss=0.941164, elapsed=638.68s, remaining=1291.88s.\n",
      "Batch 6200: loss=0.941102, elapsed=649.12s, remaining=1281.36s.\n",
      "Batch 6300: loss=0.940736, elapsed=659.45s, remaining=1270.61s.\n",
      "Batch 6400: loss=0.941406, elapsed=669.97s, remaining=1260.27s.\n",
      "Batch 6500: loss=0.940884, elapsed=680.49s, remaining=1249.90s.\n",
      "Batch 6600: loss=0.941108, elapsed=690.97s, remaining=1239.45s.\n",
      "Batch 6700: loss=0.941355, elapsed=701.53s, remaining=1229.15s.\n",
      "Batch 6800: loss=0.941817, elapsed=712.18s, remaining=1219.02s.\n",
      "Batch 6900: loss=0.942197, elapsed=722.62s, remaining=1208.49s.\n",
      "Batch 7000: loss=0.941374, elapsed=732.97s, remaining=1197.82s.\n",
      "Batch 7100: loss=0.940255, elapsed=743.48s, remaining=1187.30s.\n",
      "Batch 7200: loss=0.939990, elapsed=754.44s, remaining=1177.63s.\n",
      "Batch 7300: loss=0.940009, elapsed=765.59s, remaining=1168.19s.\n",
      "Batch 7400: loss=0.939740, elapsed=776.39s, remaining=1158.07s.\n",
      "Batch 7500: loss=0.940381, elapsed=787.02s, remaining=1147.73s.\n",
      "Batch 7600: loss=0.939543, elapsed=797.81s, remaining=1137.66s.\n",
      "Batch 7700: loss=0.938803, elapsed=808.57s, remaining=1127.43s.\n",
      "Batch 7800: loss=0.938808, elapsed=819.06s, remaining=1116.89s.\n",
      "Batch 7900: loss=0.940236, elapsed=829.76s, remaining=1106.62s.\n",
      "Batch 8000: loss=0.939414, elapsed=840.65s, remaining=1096.51s.\n",
      "Batch 8100: loss=0.939198, elapsed=851.32s, remaining=1086.21s.\n",
      "Batch 8200: loss=0.938641, elapsed=862.03s, remaining=1075.93s.\n",
      "Batch 8300: loss=0.939251, elapsed=872.83s, remaining=1065.69s.\n",
      "Batch 8400: loss=0.938298, elapsed=883.40s, remaining=1055.19s.\n",
      "Batch 8500: loss=0.937749, elapsed=894.12s, remaining=1044.90s.\n",
      "Batch 8600: loss=0.937399, elapsed=905.03s, remaining=1034.76s.\n",
      "Batch 8700: loss=0.937329, elapsed=915.59s, remaining=1024.27s.\n",
      "Batch 8800: loss=0.937132, elapsed=926.41s, remaining=1014.08s.\n",
      "Batch 8900: loss=0.936615, elapsed=937.13s, remaining=1003.79s.\n",
      "Batch 9000: loss=0.935144, elapsed=947.77s, remaining=993.34s.\n",
      "Batch 9100: loss=0.935123, elapsed=958.57s, remaining=983.12s.\n",
      "Batch 9200: loss=0.934383, elapsed=969.25s, remaining=972.65s.\n",
      "Batch 9300: loss=0.933470, elapsed=980.00s, remaining=962.26s.\n",
      "Batch 9400: loss=0.933009, elapsed=990.90s, remaining=952.04s.\n",
      "Batch 9500: loss=0.932854, elapsed=1001.62s, remaining=941.65s.\n",
      "Batch 9600: loss=0.932336, elapsed=1012.18s, remaining=931.13s.\n",
      "Batch 9700: loss=0.931060, elapsed=1022.97s, remaining=920.81s.\n",
      "Batch 9800: loss=0.931324, elapsed=1033.65s, remaining=910.36s.\n",
      "Batch 9900: loss=0.931260, elapsed=1044.42s, remaining=900.03s.\n",
      "Batch 10000: loss=0.930769, elapsed=1055.23s, remaining=889.65s.\n",
      "Batch 10100: loss=0.930262, elapsed=1065.77s, remaining=879.04s.\n",
      "Batch 10200: loss=0.930128, elapsed=1076.48s, remaining=868.56s.\n",
      "Batch 10300: loss=0.930054, elapsed=1087.15s, remaining=858.13s.\n",
      "Batch 10400: loss=0.930476, elapsed=1097.79s, remaining=847.62s.\n",
      "Batch 10500: loss=0.930063, elapsed=1108.60s, remaining=837.26s.\n",
      "Batch 10600: loss=0.929219, elapsed=1119.17s, remaining=826.65s.\n",
      "Batch 10700: loss=0.928853, elapsed=1129.70s, remaining=816.04s.\n",
      "Batch 10800: loss=0.928043, elapsed=1140.30s, remaining=805.44s.\n",
      "Batch 10900: loss=0.927559, elapsed=1150.94s, remaining=794.91s.\n",
      "Batch 11000: loss=0.928039, elapsed=1161.63s, remaining=784.39s.\n",
      "Batch 11100: loss=0.928210, elapsed=1172.45s, remaining=773.86s.\n",
      "Batch 11200: loss=0.928398, elapsed=1183.12s, remaining=763.36s.\n",
      "Batch 11300: loss=0.927963, elapsed=1193.77s, remaining=752.92s.\n",
      "Batch 11400: loss=0.927421, elapsed=1204.85s, remaining=742.63s.\n",
      "Batch 11500: loss=0.926954, elapsed=1215.52s, remaining=732.15s.\n",
      "Batch 11600: loss=0.926622, elapsed=1226.29s, remaining=721.58s.\n",
      "Batch 11700: loss=0.926177, elapsed=1236.87s, remaining=710.98s.\n",
      "Batch 11800: loss=0.925786, elapsed=1247.52s, remaining=700.57s.\n",
      "Batch 11900: loss=0.925339, elapsed=1257.90s, remaining=689.85s.\n",
      "Batch 12000: loss=0.924788, elapsed=1268.50s, remaining=679.18s.\n",
      "Batch 12100: loss=0.924528, elapsed=1278.93s, remaining=668.52s.\n",
      "Batch 12200: loss=0.924094, elapsed=1289.55s, remaining=658.00s.\n",
      "Batch 12300: loss=0.924187, elapsed=1300.24s, remaining=647.43s.\n",
      "Batch 12400: loss=0.923418, elapsed=1310.93s, remaining=636.95s.\n",
      "Batch 12500: loss=0.923378, elapsed=1321.79s, remaining=626.57s.\n",
      "Batch 12600: loss=0.922887, elapsed=1332.54s, remaining=616.09s.\n",
      "Batch 12700: loss=0.922932, elapsed=1343.07s, remaining=605.45s.\n",
      "Batch 12800: loss=0.922504, elapsed=1353.74s, remaining=594.87s.\n",
      "Batch 12900: loss=0.921909, elapsed=1364.50s, remaining=584.34s.\n",
      "Batch 13000: loss=0.921794, elapsed=1374.96s, remaining=573.74s.\n",
      "Batch 13100: loss=0.921099, elapsed=1385.66s, remaining=563.19s.\n",
      "Batch 13200: loss=0.920402, elapsed=1396.17s, remaining=552.69s.\n",
      "Batch 13300: loss=0.920306, elapsed=1406.58s, remaining=541.99s.\n",
      "Batch 13400: loss=0.919992, elapsed=1417.16s, remaining=531.42s.\n",
      "Batch 13500: loss=0.919711, elapsed=1427.59s, remaining=520.72s.\n",
      "Batch 13600: loss=0.919568, elapsed=1438.18s, remaining=510.19s.\n",
      "Batch 13700: loss=0.919387, elapsed=1448.73s, remaining=499.61s.\n",
      "Batch 13800: loss=0.919010, elapsed=1459.32s, remaining=488.96s.\n",
      "Batch 13900: loss=0.918698, elapsed=1469.88s, remaining=478.38s.\n",
      "Batch 14000: loss=0.918213, elapsed=1480.46s, remaining=467.85s.\n",
      "Batch 14100: loss=0.917272, elapsed=1491.25s, remaining=457.25s.\n",
      "Batch 14200: loss=0.916787, elapsed=1501.98s, remaining=446.75s.\n",
      "Batch 14300: loss=0.916703, elapsed=1512.87s, remaining=436.25s.\n",
      "Batch 14400: loss=0.916401, elapsed=1523.76s, remaining=425.71s.\n",
      "Batch 14500: loss=0.916391, elapsed=1534.38s, remaining=415.24s.\n",
      "Batch 14600: loss=0.915830, elapsed=1545.01s, remaining=404.58s.\n",
      "Batch 14700: loss=0.915489, elapsed=1555.70s, remaining=393.97s.\n",
      "Batch 14800: loss=0.915463, elapsed=1566.50s, remaining=383.38s.\n",
      "Batch 14900: loss=0.915506, elapsed=1577.15s, remaining=372.82s.\n",
      "Batch 15000: loss=0.915107, elapsed=1587.87s, remaining=362.20s.\n",
      "Batch 15100: loss=0.914703, elapsed=1598.55s, remaining=351.62s.\n",
      "Batch 15200: loss=0.914328, elapsed=1609.41s, remaining=340.89s.\n",
      "Batch 15300: loss=0.913818, elapsed=1620.12s, remaining=330.27s.\n",
      "Batch 15400: loss=0.913391, elapsed=1630.73s, remaining=319.63s.\n",
      "Batch 15500: loss=0.913084, elapsed=1641.26s, remaining=308.95s.\n",
      "Batch 15600: loss=0.912931, elapsed=1652.37s, remaining=298.43s.\n",
      "Batch 15700: loss=0.912711, elapsed=1663.11s, remaining=287.87s.\n",
      "Batch 15800: loss=0.912410, elapsed=1673.93s, remaining=277.36s.\n",
      "Batch 15900: loss=0.911996, elapsed=1684.69s, remaining=266.74s.\n",
      "Batch 16000: loss=0.911792, elapsed=1695.15s, remaining=256.18s.\n",
      "Batch 16100: loss=0.910901, elapsed=1705.73s, remaining=245.57s.\n",
      "Batch 16200: loss=0.910514, elapsed=1716.67s, remaining=234.93s.\n",
      "Batch 16300: loss=0.910037, elapsed=1727.50s, remaining=224.34s.\n",
      "Batch 16400: loss=0.910102, elapsed=1738.22s, remaining=213.74s.\n",
      "Batch 16500: loss=0.909891, elapsed=1749.21s, remaining=203.21s.\n",
      "Batch 16600: loss=0.909459, elapsed=1760.11s, remaining=192.72s.\n",
      "Batch 16700: loss=0.909054, elapsed=1770.75s, remaining=182.15s.\n",
      "Batch 16800: loss=0.908361, elapsed=1781.40s, remaining=171.56s.\n",
      "Batch 16900: loss=0.908005, elapsed=1791.92s, remaining=160.95s.\n",
      "Batch 17000: loss=0.907828, elapsed=1802.50s, remaining=150.33s.\n",
      "Batch 17100: loss=0.907391, elapsed=1813.01s, remaining=139.83s.\n",
      "Batch 17200: loss=0.907097, elapsed=1823.37s, remaining=129.27s.\n",
      "Batch 17300: loss=0.906924, elapsed=1834.12s, remaining=118.65s.\n",
      "Batch 17400: loss=0.906463, elapsed=1844.89s, remaining=108.07s.\n",
      "Batch 17500: loss=0.906006, elapsed=1855.38s, remaining=97.53s.\n",
      "Batch 17600: loss=0.905395, elapsed=1865.93s, remaining=86.94s.\n",
      "Batch 17700: loss=0.905052, elapsed=1876.82s, remaining=76.29s.\n",
      "Batch 17800: loss=0.904795, elapsed=1887.31s, remaining=65.66s.\n",
      "Batch 17900: loss=0.904535, elapsed=1897.89s, remaining=55.07s.\n",
      "Batch 18000: loss=0.904284, elapsed=1908.29s, remaining=44.48s.\n",
      "Batch 18100: loss=0.903986, elapsed=1918.77s, remaining=33.88s.\n",
      "Batch 18200: loss=0.903612, elapsed=1929.59s, remaining=23.33s.\n",
      "Batch 18300: loss=0.903426, elapsed=1940.37s, remaining=12.72s.\n",
      "Batch 18400: loss=0.903267, elapsed=1950.81s, remaining=2.14s.\n",
      "Batch 18500: loss=0.903104, elapsed=1961.12s, remaining=-8.52s.\n",
      "Batch 18600: loss=0.902398, elapsed=1971.42s, remaining=-19.09s.\n",
      "Batch 18700: loss=0.901874, elapsed=1982.03s, remaining=-29.74s.\n",
      "Batch 18800: loss=0.901540, elapsed=1992.81s, remaining=-40.43s.\n",
      "Batch 18900: loss=0.901345, elapsed=2003.46s, remaining=-51.02s.\n",
      "\n",
      "Training epoch took: 2012.02s\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.691172, elapsed=10.62s, remaining=1922.44s.\n",
      "Batch 200: loss=0.652314, elapsed=21.03s, remaining=1898.46s.\n",
      "Batch 300: loss=0.655744, elapsed=31.51s, remaining=1890.13s.\n",
      "Batch 400: loss=0.649328, elapsed=42.37s, remaining=1897.33s.\n",
      "Batch 500: loss=0.658675, elapsed=52.84s, remaining=1884.77s.\n",
      "Batch 600: loss=0.662356, elapsed=63.45s, remaining=1876.77s.\n",
      "Batch 700: loss=0.667183, elapsed=73.91s, remaining=1864.47s.\n",
      "Batch 800: loss=0.663497, elapsed=84.50s, remaining=1856.43s.\n",
      "Batch 900: loss=0.671051, elapsed=95.27s, remaining=1850.36s.\n",
      "Batch 1000: loss=0.669585, elapsed=105.67s, remaining=1836.38s.\n",
      "Batch 1100: loss=0.673031, elapsed=116.22s, remaining=1827.48s.\n",
      "Batch 1200: loss=0.674845, elapsed=126.83s, remaining=1817.40s.\n",
      "Batch 1300: loss=0.672584, elapsed=137.41s, remaining=1806.51s.\n",
      "Batch 1400: loss=0.677108, elapsed=148.08s, remaining=1797.38s.\n",
      "Batch 1500: loss=0.677097, elapsed=158.89s, remaining=1789.98s.\n",
      "Batch 1600: loss=0.678026, elapsed=169.56s, remaining=1779.85s.\n",
      "Batch 1700: loss=0.678100, elapsed=180.00s, remaining=1767.27s.\n",
      "Batch 1800: loss=0.676765, elapsed=190.67s, remaining=1757.53s.\n",
      "Batch 1900: loss=0.676376, elapsed=201.45s, remaining=1748.42s.\n",
      "Batch 2000: loss=0.673810, elapsed=212.29s, remaining=1738.75s.\n",
      "Batch 2100: loss=0.674857, elapsed=222.98s, remaining=1728.56s.\n",
      "Batch 2200: loss=0.673044, elapsed=233.69s, remaining=1719.11s.\n",
      "Batch 2300: loss=0.672617, elapsed=244.44s, remaining=1709.29s.\n",
      "Batch 2400: loss=0.672207, elapsed=254.98s, remaining=1698.37s.\n",
      "Batch 2500: loss=0.672202, elapsed=265.63s, remaining=1688.48s.\n",
      "Batch 2600: loss=0.674325, elapsed=276.23s, remaining=1678.07s.\n",
      "Batch 2700: loss=0.675641, elapsed=286.62s, remaining=1666.09s.\n",
      "Batch 2800: loss=0.676345, elapsed=297.31s, remaining=1655.94s.\n",
      "Batch 2900: loss=0.677099, elapsed=307.92s, remaining=1645.56s.\n",
      "Batch 3000: loss=0.678039, elapsed=318.28s, remaining=1633.87s.\n",
      "Batch 3100: loss=0.679034, elapsed=328.84s, remaining=1623.13s.\n",
      "Batch 3200: loss=0.676831, elapsed=339.40s, remaining=1612.15s.\n",
      "Batch 3300: loss=0.677589, elapsed=349.88s, remaining=1600.86s.\n",
      "Batch 3400: loss=0.678406, elapsed=360.61s, remaining=1590.68s.\n",
      "Batch 3500: loss=0.678031, elapsed=371.28s, remaining=1580.80s.\n",
      "Batch 3600: loss=0.676076, elapsed=381.72s, remaining=1569.67s.\n",
      "Batch 3700: loss=0.676383, elapsed=392.35s, remaining=1559.45s.\n",
      "Batch 3800: loss=0.675429, elapsed=403.13s, remaining=1549.58s.\n",
      "Batch 3900: loss=0.674723, elapsed=414.23s, remaining=1540.66s.\n",
      "Batch 4000: loss=0.675944, elapsed=424.89s, remaining=1529.93s.\n",
      "Batch 4100: loss=0.675387, elapsed=435.44s, remaining=1519.05s.\n",
      "Batch 4200: loss=0.676178, elapsed=445.97s, remaining=1508.05s.\n",
      "Batch 4300: loss=0.675771, elapsed=456.60s, remaining=1497.74s.\n",
      "Batch 4400: loss=0.675633, elapsed=467.41s, remaining=1487.81s.\n",
      "Batch 4500: loss=0.675306, elapsed=478.05s, remaining=1477.40s.\n",
      "Batch 4600: loss=0.675738, elapsed=488.65s, remaining=1466.97s.\n",
      "Batch 4700: loss=0.675561, elapsed=499.15s, remaining=1455.92s.\n",
      "Batch 4800: loss=0.675435, elapsed=509.87s, remaining=1445.62s.\n",
      "Batch 4900: loss=0.675306, elapsed=520.49s, remaining=1435.11s.\n",
      "Batch 5000: loss=0.674729, elapsed=530.98s, remaining=1424.28s.\n",
      "Batch 5100: loss=0.673707, elapsed=541.67s, remaining=1413.99s.\n",
      "Batch 5200: loss=0.674438, elapsed=552.25s, remaining=1403.27s.\n",
      "Batch 5300: loss=0.674980, elapsed=562.87s, remaining=1392.45s.\n",
      "Batch 5400: loss=0.674268, elapsed=573.54s, remaining=1382.20s.\n",
      "Batch 5500: loss=0.675472, elapsed=584.16s, remaining=1371.62s.\n",
      "Batch 5600: loss=0.675558, elapsed=595.04s, remaining=1361.78s.\n",
      "Batch 5700: loss=0.675420, elapsed=605.50s, remaining=1350.86s.\n",
      "Batch 5800: loss=0.676233, elapsed=615.92s, remaining=1339.61s.\n",
      "Batch 5900: loss=0.675509, elapsed=626.65s, remaining=1329.02s.\n",
      "Batch 6000: loss=0.674549, elapsed=637.36s, remaining=1318.61s.\n",
      "Batch 6100: loss=0.674801, elapsed=648.07s, remaining=1308.09s.\n",
      "Batch 6200: loss=0.673793, elapsed=658.87s, remaining=1297.71s.\n",
      "Batch 6300: loss=0.673426, elapsed=669.58s, remaining=1287.23s.\n",
      "Batch 6400: loss=0.673340, elapsed=680.18s, remaining=1276.71s.\n",
      "Batch 6500: loss=0.673276, elapsed=690.68s, remaining=1265.88s.\n",
      "Batch 6600: loss=0.672630, elapsed=701.23s, remaining=1255.17s.\n",
      "Batch 6700: loss=0.671281, elapsed=711.76s, remaining=1244.42s.\n",
      "Batch 6800: loss=0.670858, elapsed=722.53s, remaining=1234.14s.\n",
      "Batch 6900: loss=0.670777, elapsed=733.08s, remaining=1223.44s.\n",
      "Batch 7000: loss=0.670945, elapsed=743.55s, remaining=1212.37s.\n",
      "Batch 7100: loss=0.670294, elapsed=754.16s, remaining=1201.71s.\n",
      "Batch 7200: loss=0.669876, elapsed=764.90s, remaining=1191.40s.\n",
      "Batch 7300: loss=0.669688, elapsed=775.74s, remaining=1181.31s.\n",
      "Batch 7400: loss=0.669725, elapsed=786.59s, remaining=1170.99s.\n",
      "Batch 7500: loss=0.669758, elapsed=797.20s, remaining=1160.26s.\n",
      "Batch 7600: loss=0.669298, elapsed=807.75s, remaining=1149.49s.\n",
      "Batch 7700: loss=0.668753, elapsed=818.76s, remaining=1139.34s.\n",
      "Batch 7800: loss=0.668755, elapsed=829.59s, remaining=1128.84s.\n",
      "Batch 7900: loss=0.668832, elapsed=840.27s, remaining=1118.30s.\n",
      "Batch 8000: loss=0.668626, elapsed=850.96s, remaining=1107.73s.\n",
      "Batch 8100: loss=0.669448, elapsed=861.36s, remaining=1096.77s.\n",
      "Batch 8200: loss=0.669321, elapsed=872.16s, remaining=1086.30s.\n",
      "Batch 8300: loss=0.669161, elapsed=883.07s, remaining=1076.04s.\n",
      "Batch 8400: loss=0.668901, elapsed=893.59s, remaining=1065.27s.\n",
      "Batch 8500: loss=0.668402, elapsed=904.47s, remaining=1054.89s.\n",
      "Batch 8600: loss=0.668837, elapsed=915.07s, remaining=1044.18s.\n",
      "Batch 8700: loss=0.668716, elapsed=925.74s, remaining=1033.41s.\n",
      "Batch 8800: loss=0.668066, elapsed=936.41s, remaining=1022.86s.\n",
      "Batch 8900: loss=0.667676, elapsed=947.06s, remaining=1012.24s.\n",
      "Batch 9000: loss=0.667074, elapsed=957.44s, remaining=1001.40s.\n",
      "Batch 9100: loss=0.666866, elapsed=968.16s, remaining=990.84s.\n",
      "Batch 9200: loss=0.666637, elapsed=978.88s, remaining=980.50s.\n",
      "Batch 9300: loss=0.666619, elapsed=989.35s, remaining=969.87s.\n",
      "Batch 9400: loss=0.666867, elapsed=1000.15s, remaining=959.18s.\n",
      "Batch 9500: loss=0.666986, elapsed=1010.53s, remaining=948.34s.\n",
      "Batch 9600: loss=0.666474, elapsed=1020.84s, remaining=937.29s.\n",
      "Batch 9700: loss=0.666138, elapsed=1031.43s, remaining=926.45s.\n",
      "Batch 9800: loss=0.666324, elapsed=1042.00s, remaining=915.89s.\n",
      "Batch 9900: loss=0.666025, elapsed=1052.63s, remaining=905.15s.\n",
      "Batch 10000: loss=0.665711, elapsed=1063.35s, remaining=894.63s.\n",
      "Batch 10100: loss=0.665127, elapsed=1073.82s, remaining=883.90s.\n",
      "Batch 10200: loss=0.664586, elapsed=1084.42s, remaining=873.29s.\n",
      "Batch 10300: loss=0.664126, elapsed=1095.14s, remaining=862.65s.\n",
      "Batch 10400: loss=0.663643, elapsed=1106.14s, remaining=852.44s.\n",
      "Batch 10500: loss=0.663492, elapsed=1116.74s, remaining=841.81s.\n",
      "Batch 10600: loss=0.663557, elapsed=1127.35s, remaining=831.06s.\n",
      "Batch 10700: loss=0.663095, elapsed=1138.07s, remaining=820.62s.\n",
      "Batch 10800: loss=0.663302, elapsed=1148.89s, remaining=810.03s.\n",
      "Batch 10900: loss=0.663226, elapsed=1159.67s, remaining=799.35s.\n",
      "Batch 11000: loss=0.663063, elapsed=1170.27s, remaining=788.70s.\n",
      "Batch 11100: loss=0.663287, elapsed=1180.89s, remaining=778.07s.\n",
      "Batch 11200: loss=0.663172, elapsed=1191.48s, remaining=767.53s.\n",
      "Batch 11300: loss=0.662987, elapsed=1202.30s, remaining=756.93s.\n",
      "Batch 11400: loss=0.662328, elapsed=1213.01s, remaining=746.36s.\n",
      "Batch 11500: loss=0.662439, elapsed=1223.72s, remaining=735.71s.\n",
      "Batch 11600: loss=0.662925, elapsed=1234.52s, remaining=725.22s.\n",
      "Batch 11700: loss=0.662808, elapsed=1245.29s, remaining=714.56s.\n",
      "Batch 11800: loss=0.663003, elapsed=1255.65s, remaining=703.85s.\n",
      "Batch 11900: loss=0.662908, elapsed=1266.39s, remaining=693.31s.\n",
      "Batch 12000: loss=0.662944, elapsed=1277.24s, remaining=682.67s.\n",
      "Batch 12100: loss=0.663483, elapsed=1287.87s, remaining=672.01s.\n",
      "Batch 12200: loss=0.663458, elapsed=1299.02s, remaining=661.63s.\n",
      "Batch 12300: loss=0.663122, elapsed=1309.94s, remaining=651.04s.\n",
      "Batch 12400: loss=0.662817, elapsed=1320.88s, remaining=640.61s.\n",
      "Batch 12500: loss=0.662618, elapsed=1331.76s, remaining=630.08s.\n",
      "Batch 12600: loss=0.662753, elapsed=1342.65s, remaining=619.53s.\n",
      "Batch 12700: loss=0.662107, elapsed=1353.41s, remaining=608.99s.\n",
      "Batch 12800: loss=0.661880, elapsed=1364.30s, remaining=598.38s.\n",
      "Batch 12900: loss=0.662020, elapsed=1375.13s, remaining=587.81s.\n",
      "Batch 13000: loss=0.662278, elapsed=1385.92s, remaining=577.23s.\n",
      "Batch 13100: loss=0.662110, elapsed=1396.91s, remaining=566.75s.\n",
      "Batch 13200: loss=0.662144, elapsed=1407.88s, remaining=556.26s.\n",
      "Batch 13300: loss=0.661809, elapsed=1418.85s, remaining=545.78s.\n",
      "Batch 13400: loss=0.661632, elapsed=1429.79s, remaining=535.27s.\n",
      "Batch 13500: loss=0.661539, elapsed=1440.76s, remaining=524.75s.\n",
      "Batch 13600: loss=0.661288, elapsed=1451.63s, remaining=514.12s.\n",
      "Batch 13700: loss=0.661131, elapsed=1462.45s, remaining=503.43s.\n",
      "Batch 13800: loss=0.660955, elapsed=1473.25s, remaining=492.85s.\n",
      "Batch 13900: loss=0.660531, elapsed=1484.03s, remaining=482.24s.\n",
      "Batch 14000: loss=0.660637, elapsed=1494.96s, remaining=471.72s.\n",
      "Batch 14100: loss=0.660419, elapsed=1505.90s, remaining=461.13s.\n",
      "Batch 14200: loss=0.660272, elapsed=1516.64s, remaining=450.46s.\n",
      "Batch 14300: loss=0.660403, elapsed=1527.29s, remaining=439.78s.\n",
      "Batch 14400: loss=0.660159, elapsed=1538.21s, remaining=429.18s.\n",
      "Batch 14500: loss=0.660434, elapsed=1549.13s, remaining=418.59s.\n",
      "Batch 14600: loss=0.660438, elapsed=1559.84s, remaining=407.97s.\n",
      "Batch 14700: loss=0.659967, elapsed=1570.77s, remaining=397.29s.\n",
      "Batch 14800: loss=0.659753, elapsed=1581.24s, remaining=386.60s.\n",
      "Batch 14900: loss=0.659938, elapsed=1591.67s, remaining=375.91s.\n",
      "Batch 15000: loss=0.660027, elapsed=1602.36s, remaining=365.26s.\n",
      "Batch 15100: loss=0.659831, elapsed=1612.94s, remaining=354.47s.\n",
      "Batch 15200: loss=0.659656, elapsed=1623.75s, remaining=343.84s.\n",
      "Batch 15300: loss=0.659262, elapsed=1634.39s, remaining=333.15s.\n",
      "Batch 15400: loss=0.658914, elapsed=1645.03s, remaining=322.49s.\n",
      "Batch 15500: loss=0.658538, elapsed=1655.97s, remaining=311.79s.\n",
      "Batch 15600: loss=0.658457, elapsed=1666.99s, remaining=301.17s.\n",
      "Batch 15700: loss=0.658351, elapsed=1677.52s, remaining=290.54s.\n",
      "Batch 15800: loss=0.658170, elapsed=1688.36s, remaining=280.01s.\n",
      "Batch 15900: loss=0.657863, elapsed=1699.00s, remaining=269.27s.\n",
      "Batch 16000: loss=0.657368, elapsed=1709.84s, remaining=258.59s.\n",
      "Batch 16100: loss=0.657572, elapsed=1720.57s, remaining=247.90s.\n",
      "Batch 16200: loss=0.657073, elapsed=1731.48s, remaining=237.27s.\n",
      "Batch 16300: loss=0.656923, elapsed=1741.98s, remaining=226.57s.\n",
      "Batch 16400: loss=0.656620, elapsed=1752.72s, remaining=215.91s.\n",
      "Batch 16500: loss=0.656251, elapsed=1763.68s, remaining=205.29s.\n",
      "Batch 16600: loss=0.656509, elapsed=1774.50s, remaining=194.63s.\n",
      "Batch 16700: loss=0.656394, elapsed=1784.82s, remaining=183.92s.\n",
      "Batch 16800: loss=0.656066, elapsed=1795.58s, remaining=173.26s.\n",
      "Batch 16900: loss=0.656132, elapsed=1806.10s, remaining=162.59s.\n",
      "Batch 17000: loss=0.655845, elapsed=1816.74s, remaining=151.91s.\n",
      "Batch 17100: loss=0.655710, elapsed=1827.42s, remaining=141.24s.\n",
      "Batch 17200: loss=0.655334, elapsed=1837.96s, remaining=130.56s.\n",
      "Batch 17300: loss=0.655499, elapsed=1848.51s, remaining=119.88s.\n",
      "Batch 17400: loss=0.655107, elapsed=1859.04s, remaining=109.20s.\n",
      "Batch 17500: loss=0.655142, elapsed=1869.85s, remaining=98.54s.\n",
      "Batch 17600: loss=0.654824, elapsed=1880.41s, remaining=87.87s.\n",
      "Batch 17700: loss=0.654632, elapsed=1890.89s, remaining=77.18s.\n",
      "Batch 17800: loss=0.654124, elapsed=1901.64s, remaining=66.52s.\n",
      "Batch 17900: loss=0.654112, elapsed=1912.26s, remaining=55.85s.\n",
      "Batch 18000: loss=0.654082, elapsed=1922.95s, remaining=45.19s.\n",
      "Batch 18100: loss=0.654041, elapsed=1933.60s, remaining=34.53s.\n",
      "Batch 18200: loss=0.653922, elapsed=1943.98s, remaining=23.86s.\n",
      "Batch 18300: loss=0.653554, elapsed=1954.62s, remaining=13.20s.\n",
      "Batch 18400: loss=0.653276, elapsed=1965.08s, remaining=2.53s.\n",
      "Batch 18500: loss=0.652857, elapsed=1975.89s, remaining=-8.13s.\n",
      "Batch 18600: loss=0.652657, elapsed=1986.57s, remaining=-18.80s.\n",
      "Batch 18700: loss=0.652643, elapsed=1997.06s, remaining=-29.46s.\n",
      "Batch 18800: loss=0.652459, elapsed=2007.83s, remaining=-40.13s.\n",
      "Batch 18900: loss=0.652302, elapsed=2018.84s, remaining=-50.78s.\n",
      "\n",
      "Training epoch took: 2027.06s\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.466219, elapsed=10.54s, remaining=1910.71s.\n",
      "Batch 200: loss=0.438456, elapsed=21.15s, remaining=1918.93s.\n",
      "Batch 300: loss=0.450494, elapsed=31.99s, remaining=1928.57s.\n",
      "Batch 400: loss=0.447639, elapsed=42.53s, remaining=1913.73s.\n",
      "Batch 500: loss=0.455132, elapsed=53.25s, remaining=1907.81s.\n",
      "Batch 600: loss=0.450990, elapsed=63.74s, remaining=1892.98s.\n",
      "Batch 700: loss=0.451474, elapsed=74.42s, remaining=1884.38s.\n",
      "Batch 800: loss=0.451727, elapsed=85.13s, remaining=1876.06s.\n",
      "Batch 900: loss=0.455476, elapsed=95.70s, remaining=1864.32s.\n",
      "Batch 1000: loss=0.459610, elapsed=106.27s, remaining=1852.95s.\n",
      "Batch 1100: loss=0.460503, elapsed=116.95s, remaining=1843.30s.\n",
      "Batch 1200: loss=0.464595, elapsed=127.61s, remaining=1833.06s.\n",
      "Batch 1300: loss=0.464571, elapsed=138.33s, remaining=1823.74s.\n",
      "Batch 1400: loss=0.460692, elapsed=148.87s, remaining=1811.88s.\n",
      "Batch 1500: loss=0.461377, elapsed=159.42s, remaining=1800.36s.\n",
      "Batch 1600: loss=0.463654, elapsed=170.37s, remaining=1793.26s.\n",
      "Batch 1700: loss=0.462885, elapsed=180.77s, remaining=1780.21s.\n",
      "Batch 1800: loss=0.463535, elapsed=191.45s, remaining=1770.02s.\n",
      "Batch 1900: loss=0.464576, elapsed=202.12s, remaining=1759.76s.\n",
      "Batch 2000: loss=0.466475, elapsed=212.98s, remaining=1751.04s.\n",
      "Batch 2100: loss=0.467960, elapsed=223.55s, remaining=1739.77s.\n",
      "Batch 2200: loss=0.467427, elapsed=234.28s, remaining=1729.82s.\n",
      "Batch 2300: loss=0.466776, elapsed=244.90s, remaining=1718.99s.\n",
      "Batch 2400: loss=0.468696, elapsed=255.56s, remaining=1708.44s.\n",
      "Batch 2500: loss=0.468820, elapsed=266.27s, remaining=1698.28s.\n",
      "Batch 2600: loss=0.467952, elapsed=277.04s, remaining=1688.40s.\n",
      "Batch 2700: loss=0.469236, elapsed=287.61s, remaining=1677.30s.\n",
      "Batch 2800: loss=0.470252, elapsed=298.24s, remaining=1666.61s.\n",
      "Batch 2900: loss=0.469414, elapsed=309.03s, remaining=1656.69s.\n",
      "Batch 3000: loss=0.470369, elapsed=319.85s, remaining=1646.92s.\n",
      "Batch 3100: loss=0.470557, elapsed=330.35s, remaining=1635.50s.\n",
      "Batch 3200: loss=0.472286, elapsed=341.12s, remaining=1625.40s.\n",
      "Batch 3300: loss=0.471200, elapsed=352.03s, remaining=1615.95s.\n",
      "Batch 3400: loss=0.472896, elapsed=362.73s, remaining=1605.44s.\n",
      "Batch 3500: loss=0.474027, elapsed=373.31s, remaining=1594.45s.\n",
      "Batch 3600: loss=0.474948, elapsed=383.93s, remaining=1583.57s.\n",
      "Batch 3700: loss=0.475941, elapsed=394.47s, remaining=1572.42s.\n",
      "Batch 3800: loss=0.475952, elapsed=405.19s, remaining=1562.00s.\n",
      "Batch 3900: loss=0.477886, elapsed=415.73s, remaining=1550.89s.\n",
      "Batch 4000: loss=0.477247, elapsed=426.29s, remaining=1539.89s.\n",
      "Batch 4100: loss=0.477844, elapsed=436.82s, remaining=1528.77s.\n",
      "Batch 4200: loss=0.476336, elapsed=447.73s, remaining=1519.06s.\n",
      "Batch 4300: loss=0.476076, elapsed=458.71s, remaining=1509.46s.\n",
      "Batch 4400: loss=0.475169, elapsed=469.39s, remaining=1498.83s.\n",
      "Batch 4500: loss=0.474681, elapsed=479.91s, remaining=1487.72s.\n",
      "Batch 4600: loss=0.474556, elapsed=490.65s, remaining=1477.29s.\n",
      "Batch 4700: loss=0.474369, elapsed=501.49s, remaining=1467.13s.\n",
      "Batch 4800: loss=0.475034, elapsed=512.09s, remaining=1456.23s.\n",
      "Batch 4900: loss=0.474937, elapsed=522.77s, remaining=1445.65s.\n",
      "Batch 5000: loss=0.474629, elapsed=533.47s, remaining=1435.05s.\n",
      "Batch 5100: loss=0.474103, elapsed=544.08s, remaining=1424.25s.\n",
      "Batch 5200: loss=0.474145, elapsed=554.56s, remaining=1413.09s.\n",
      "Batch 5300: loss=0.473719, elapsed=565.27s, remaining=1402.54s.\n",
      "Batch 5400: loss=0.473089, elapsed=575.76s, remaining=1391.41s.\n",
      "Batch 5500: loss=0.473481, elapsed=586.35s, remaining=1380.57s.\n",
      "Batch 5600: loss=0.473206, elapsed=596.92s, remaining=1369.71s.\n",
      "Batch 5700: loss=0.473578, elapsed=607.53s, remaining=1358.92s.\n",
      "Batch 5800: loss=0.472525, elapsed=618.33s, remaining=1348.59s.\n",
      "Batch 5900: loss=0.471755, elapsed=629.02s, remaining=1338.01s.\n",
      "Batch 6000: loss=0.472141, elapsed=639.82s, remaining=1327.64s.\n",
      "Batch 6100: loss=0.472314, elapsed=650.38s, remaining=1316.78s.\n",
      "Batch 6200: loss=0.471962, elapsed=661.07s, remaining=1306.15s.\n",
      "Batch 6300: loss=0.472620, elapsed=671.64s, remaining=1295.32s.\n",
      "Batch 6400: loss=0.472994, elapsed=682.31s, remaining=1284.67s.\n",
      "Batch 6500: loss=0.472597, elapsed=692.91s, remaining=1273.90s.\n",
      "Batch 6600: loss=0.472289, elapsed=703.49s, remaining=1263.08s.\n",
      "Batch 6700: loss=0.473423, elapsed=714.08s, remaining=1252.27s.\n",
      "Batch 6800: loss=0.473325, elapsed=724.67s, remaining=1241.49s.\n",
      "Batch 6900: loss=0.472695, elapsed=735.24s, remaining=1230.69s.\n",
      "Batch 7000: loss=0.472761, elapsed=745.94s, remaining=1220.12s.\n",
      "Batch 7100: loss=0.471952, elapsed=756.59s, remaining=1209.44s.\n",
      "Batch 7200: loss=0.471990, elapsed=767.06s, remaining=1198.50s.\n",
      "Batch 7300: loss=0.472023, elapsed=777.70s, remaining=1187.82s.\n",
      "Batch 7400: loss=0.472497, elapsed=788.20s, remaining=1176.93s.\n",
      "Batch 7500: loss=0.472744, elapsed=798.63s, remaining=1165.94s.\n",
      "Batch 7600: loss=0.472950, elapsed=809.35s, remaining=1155.40s.\n",
      "Batch 7700: loss=0.473243, elapsed=820.14s, remaining=1144.94s.\n",
      "Batch 7800: loss=0.473453, elapsed=830.86s, remaining=1134.42s.\n",
      "Batch 7900: loss=0.473646, elapsed=841.47s, remaining=1123.71s.\n",
      "Batch 8000: loss=0.472719, elapsed=852.08s, remaining=1113.01s.\n",
      "Batch 8100: loss=0.472795, elapsed=862.71s, remaining=1102.34s.\n",
      "Batch 8200: loss=0.472644, elapsed=873.31s, remaining=1091.61s.\n",
      "Batch 8300: loss=0.472667, elapsed=883.91s, remaining=1080.91s.\n",
      "Batch 8400: loss=0.473067, elapsed=894.47s, remaining=1070.16s.\n",
      "Batch 8500: loss=0.473096, elapsed=904.97s, remaining=1059.33s.\n",
      "Batch 8600: loss=0.473016, elapsed=915.54s, remaining=1048.60s.\n",
      "Batch 8700: loss=0.472625, elapsed=926.07s, remaining=1037.83s.\n",
      "Batch 8800: loss=0.472736, elapsed=936.96s, remaining=1027.47s.\n",
      "Batch 8900: loss=0.472479, elapsed=947.60s, remaining=1016.82s.\n",
      "Batch 9000: loss=0.472521, elapsed=958.48s, remaining=1006.43s.\n",
      "Batch 9100: loss=0.472131, elapsed=969.18s, remaining=995.83s.\n",
      "Batch 9200: loss=0.472070, elapsed=979.72s, remaining=985.06s.\n",
      "Batch 9300: loss=0.472192, elapsed=990.18s, remaining=974.23s.\n",
      "Batch 9400: loss=0.472061, elapsed=1000.90s, remaining=963.65s.\n",
      "Batch 9500: loss=0.472129, elapsed=1011.36s, remaining=952.82s.\n",
      "Batch 9600: loss=0.471742, elapsed=1021.83s, remaining=942.00s.\n",
      "Batch 9700: loss=0.471392, elapsed=1032.50s, remaining=931.37s.\n",
      "Batch 9800: loss=0.471224, elapsed=1043.05s, remaining=920.64s.\n",
      "Batch 9900: loss=0.471046, elapsed=1053.98s, remaining=910.25s.\n",
      "Batch 10000: loss=0.470484, elapsed=1064.73s, remaining=899.70s.\n",
      "Batch 10100: loss=0.470613, elapsed=1075.42s, remaining=889.08s.\n",
      "Batch 10200: loss=0.470568, elapsed=1086.02s, remaining=878.40s.\n",
      "Batch 10300: loss=0.471017, elapsed=1096.77s, remaining=867.84s.\n",
      "Batch 10400: loss=0.470684, elapsed=1107.27s, remaining=857.08s.\n",
      "Batch 10500: loss=0.470604, elapsed=1118.27s, remaining=846.72s.\n",
      "Batch 10600: loss=0.470362, elapsed=1129.06s, remaining=836.19s.\n",
      "Batch 10700: loss=0.470019, elapsed=1139.73s, remaining=825.55s.\n",
      "Batch 10800: loss=0.469939, elapsed=1150.49s, remaining=814.98s.\n",
      "Batch 10900: loss=0.469988, elapsed=1161.08s, remaining=804.27s.\n",
      "Batch 11000: loss=0.470146, elapsed=1171.76s, remaining=793.65s.\n",
      "Batch 11100: loss=0.470173, elapsed=1182.68s, remaining=783.18s.\n",
      "Batch 11200: loss=0.470060, elapsed=1193.26s, remaining=772.47s.\n",
      "Batch 11300: loss=0.469890, elapsed=1203.90s, remaining=761.80s.\n",
      "Batch 11400: loss=0.469619, elapsed=1214.62s, remaining=751.19s.\n",
      "Batch 11500: loss=0.469483, elapsed=1225.00s, remaining=740.38s.\n",
      "Batch 11600: loss=0.469763, elapsed=1235.50s, remaining=729.64s.\n",
      "Batch 11700: loss=0.469450, elapsed=1246.22s, remaining=719.04s.\n",
      "Batch 11800: loss=0.469191, elapsed=1257.06s, remaining=708.50s.\n",
      "Batch 11900: loss=0.469263, elapsed=1267.65s, remaining=697.80s.\n",
      "Batch 12000: loss=0.469450, elapsed=1278.15s, remaining=687.07s.\n",
      "Batch 12100: loss=0.469486, elapsed=1288.84s, remaining=676.44s.\n",
      "Batch 12200: loss=0.469321, elapsed=1299.61s, remaining=665.86s.\n",
      "Batch 12300: loss=0.469499, elapsed=1310.63s, remaining=655.38s.\n",
      "Batch 12400: loss=0.469216, elapsed=1321.31s, remaining=644.73s.\n",
      "Batch 12500: loss=0.469298, elapsed=1331.97s, remaining=634.07s.\n",
      "Batch 12600: loss=0.469334, elapsed=1342.55s, remaining=623.37s.\n",
      "Batch 12700: loss=0.469133, elapsed=1353.16s, remaining=612.68s.\n",
      "Batch 12800: loss=0.468804, elapsed=1363.67s, remaining=601.95s.\n",
      "Batch 12900: loss=0.468599, elapsed=1374.40s, remaining=591.33s.\n",
      "Batch 13000: loss=0.468153, elapsed=1384.88s, remaining=580.59s.\n",
      "Batch 13100: loss=0.467771, elapsed=1395.69s, remaining=570.00s.\n",
      "Batch 13200: loss=0.467659, elapsed=1406.52s, remaining=559.42s.\n",
      "Batch 13300: loss=0.467567, elapsed=1417.51s, remaining=548.91s.\n",
      "Batch 13400: loss=0.467520, elapsed=1427.99s, remaining=538.17s.\n",
      "Batch 13500: loss=0.467496, elapsed=1438.62s, remaining=527.50s.\n",
      "Batch 13600: loss=0.467801, elapsed=1449.37s, remaining=516.88s.\n",
      "Batch 13700: loss=0.467597, elapsed=1460.07s, remaining=506.23s.\n",
      "Batch 13800: loss=0.467689, elapsed=1470.47s, remaining=495.48s.\n",
      "Batch 13900: loss=0.467457, elapsed=1481.25s, remaining=484.86s.\n",
      "Batch 14000: loss=0.467106, elapsed=1491.97s, remaining=474.22s.\n",
      "Batch 14100: loss=0.467225, elapsed=1502.63s, remaining=463.55s.\n",
      "Batch 14200: loss=0.466858, elapsed=1513.18s, remaining=452.85s.\n",
      "Batch 14300: loss=0.466694, elapsed=1523.67s, remaining=442.14s.\n",
      "Batch 14400: loss=0.466739, elapsed=1534.21s, remaining=431.44s.\n",
      "Batch 14500: loss=0.466505, elapsed=1545.13s, remaining=420.87s.\n",
      "Batch 14600: loss=0.466510, elapsed=1555.96s, remaining=410.27s.\n",
      "Batch 14700: loss=0.466292, elapsed=1566.50s, remaining=399.57s.\n",
      "Batch 14800: loss=0.466198, elapsed=1576.99s, remaining=388.86s.\n",
      "Batch 14900: loss=0.465781, elapsed=1587.81s, remaining=378.24s.\n",
      "Batch 15000: loss=0.465841, elapsed=1598.54s, remaining=367.59s.\n",
      "Batch 15100: loss=0.465831, elapsed=1609.10s, remaining=356.90s.\n",
      "Batch 15200: loss=0.465961, elapsed=1619.87s, remaining=346.27s.\n",
      "Batch 15300: loss=0.466141, elapsed=1630.75s, remaining=335.67s.\n",
      "Batch 15400: loss=0.465640, elapsed=1641.51s, remaining=325.03s.\n",
      "Batch 15500: loss=0.465470, elapsed=1652.19s, remaining=314.37s.\n",
      "Batch 15600: loss=0.465278, elapsed=1662.90s, remaining=303.73s.\n",
      "Batch 15700: loss=0.465001, elapsed=1673.65s, remaining=293.09s.\n",
      "Batch 15800: loss=0.464590, elapsed=1684.00s, remaining=282.37s.\n",
      "Batch 15900: loss=0.464657, elapsed=1694.68s, remaining=271.70s.\n",
      "Batch 16000: loss=0.464474, elapsed=1705.38s, remaining=261.03s.\n",
      "Batch 16100: loss=0.464635, elapsed=1716.12s, remaining=250.38s.\n",
      "Batch 16200: loss=0.464423, elapsed=1726.95s, remaining=239.74s.\n",
      "Batch 16300: loss=0.464374, elapsed=1737.80s, remaining=229.11s.\n",
      "Batch 16400: loss=0.464290, elapsed=1748.59s, remaining=218.46s.\n",
      "Batch 16500: loss=0.464264, elapsed=1759.50s, remaining=207.83s.\n",
      "Batch 16600: loss=0.464028, elapsed=1770.12s, remaining=197.16s.\n",
      "Batch 16700: loss=0.464265, elapsed=1780.73s, remaining=186.50s.\n",
      "Batch 16800: loss=0.464277, elapsed=1791.59s, remaining=175.86s.\n",
      "Batch 16900: loss=0.463965, elapsed=1802.30s, remaining=165.20s.\n",
      "Batch 17000: loss=0.463909, elapsed=1812.90s, remaining=154.53s.\n",
      "Batch 17100: loss=0.463759, elapsed=1823.42s, remaining=143.85s.\n",
      "Batch 17200: loss=0.463739, elapsed=1833.79s, remaining=133.15s.\n",
      "Batch 17300: loss=0.463534, elapsed=1844.56s, remaining=122.50s.\n",
      "Batch 17400: loss=0.463503, elapsed=1855.12s, remaining=111.83s.\n",
      "Batch 17500: loss=0.463696, elapsed=1865.67s, remaining=101.15s.\n",
      "Batch 17600: loss=0.463664, elapsed=1876.18s, remaining=90.48s.\n",
      "Batch 17700: loss=0.463263, elapsed=1886.71s, remaining=79.82s.\n",
      "Batch 17800: loss=0.463176, elapsed=1897.29s, remaining=69.14s.\n",
      "Batch 17900: loss=0.462994, elapsed=1907.91s, remaining=58.47s.\n",
      "Batch 18000: loss=0.463077, elapsed=1918.65s, remaining=47.81s.\n",
      "Batch 18100: loss=0.462800, elapsed=1929.45s, remaining=37.14s.\n",
      "Batch 18200: loss=0.462288, elapsed=1940.16s, remaining=26.47s.\n",
      "Batch 18300: loss=0.462101, elapsed=1951.04s, remaining=15.78s.\n",
      "Batch 18400: loss=0.461958, elapsed=1961.91s, remaining=5.10s.\n",
      "Batch 18500: loss=0.461900, elapsed=1972.35s, remaining=-5.56s.\n",
      "Batch 18600: loss=0.461657, elapsed=1982.77s, remaining=-16.23s.\n",
      "Batch 18700: loss=0.461587, elapsed=1993.81s, remaining=-27.01s.\n",
      "Batch 18800: loss=0.461445, elapsed=2004.41s, remaining=-37.68s.\n",
      "Batch 18900: loss=0.461256, elapsed=2014.92s, remaining=-48.34s.\n",
      "\n",
      "Training epoch took: 2023.37s\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.336086, elapsed=10.95s, remaining=1987.59s.\n",
      "Batch 200: loss=0.327354, elapsed=21.71s, remaining=1969.52s.\n",
      "Batch 300: loss=0.326096, elapsed=32.34s, remaining=1949.22s.\n",
      "Batch 400: loss=0.323657, elapsed=43.30s, remaining=1948.19s.\n",
      "Batch 500: loss=0.328688, elapsed=53.89s, remaining=1929.65s.\n",
      "Batch 600: loss=0.336058, elapsed=64.67s, remaining=1919.93s.\n",
      "Batch 700: loss=0.341951, elapsed=75.14s, remaining=1901.66s.\n",
      "Batch 800: loss=0.341670, elapsed=85.77s, remaining=1888.89s.\n",
      "Batch 900: loss=0.339701, elapsed=96.22s, remaining=1872.98s.\n",
      "Batch 1000: loss=0.335911, elapsed=107.07s, remaining=1865.64s.\n",
      "Batch 1100: loss=0.337965, elapsed=117.57s, remaining=1851.84s.\n",
      "Batch 1200: loss=0.340499, elapsed=128.31s, remaining=1842.06s.\n",
      "Batch 1300: loss=0.338535, elapsed=139.01s, remaining=1831.61s.\n",
      "Batch 1400: loss=0.339132, elapsed=149.72s, remaining=1821.16s.\n",
      "Batch 1500: loss=0.340275, elapsed=160.47s, remaining=1811.29s.\n",
      "Batch 1600: loss=0.340072, elapsed=171.17s, remaining=1800.65s.\n",
      "Batch 1700: loss=0.341617, elapsed=182.00s, remaining=1791.36s.\n",
      "Batch 1800: loss=0.341715, elapsed=192.65s, remaining=1780.27s.\n",
      "Batch 1900: loss=0.342416, elapsed=203.15s, remaining=1767.81s.\n",
      "Batch 2000: loss=0.342406, elapsed=213.88s, remaining=1757.63s.\n",
      "Batch 2100: loss=0.341792, elapsed=224.45s, remaining=1746.08s.\n",
      "Batch 2200: loss=0.344362, elapsed=235.22s, remaining=1736.02s.\n",
      "Batch 2300: loss=0.342724, elapsed=245.80s, remaining=1724.51s.\n",
      "Batch 2400: loss=0.341597, elapsed=256.55s, remaining=1714.28s.\n",
      "Batch 2500: loss=0.343987, elapsed=267.16s, remaining=1703.13s.\n",
      "Batch 2600: loss=0.341288, elapsed=277.52s, remaining=1690.44s.\n",
      "Batch 2700: loss=0.341456, elapsed=288.29s, remaining=1680.38s.\n",
      "Batch 2800: loss=0.339822, elapsed=299.03s, remaining=1670.15s.\n",
      "Batch 2900: loss=0.339028, elapsed=309.41s, remaining=1657.81s.\n",
      "Batch 3000: loss=0.339680, elapsed=320.33s, remaining=1648.41s.\n",
      "Batch 3100: loss=0.341220, elapsed=331.19s, remaining=1638.64s.\n",
      "Batch 3200: loss=0.340498, elapsed=341.88s, remaining=1628.05s.\n",
      "Batch 3300: loss=0.340718, elapsed=352.58s, remaining=1617.41s.\n",
      "Batch 3400: loss=0.341428, elapsed=363.31s, remaining=1606.99s.\n",
      "Batch 3500: loss=0.341255, elapsed=374.05s, remaining=1596.59s.\n",
      "Batch 3600: loss=0.340281, elapsed=384.40s, remaining=1584.51s.\n",
      "Batch 3700: loss=0.339813, elapsed=395.12s, remaining=1574.03s.\n",
      "Batch 3800: loss=0.340246, elapsed=405.74s, remaining=1563.10s.\n",
      "Batch 3900: loss=0.340646, elapsed=416.11s, remaining=1551.24s.\n",
      "Batch 4000: loss=0.340897, elapsed=426.71s, remaining=1540.36s.\n",
      "Batch 4100: loss=0.340331, elapsed=437.30s, remaining=1529.45s.\n",
      "Batch 4200: loss=0.340596, elapsed=447.96s, remaining=1518.76s.\n",
      "Batch 4300: loss=0.340216, elapsed=458.72s, remaining=1508.39s.\n",
      "Batch 4400: loss=0.339782, elapsed=469.37s, remaining=1497.64s.\n",
      "Batch 4500: loss=0.338676, elapsed=480.05s, remaining=1487.00s.\n",
      "Batch 4600: loss=0.337906, elapsed=490.63s, remaining=1476.03s.\n",
      "Batch 4700: loss=0.337688, elapsed=501.27s, remaining=1465.27s.\n",
      "Batch 4800: loss=0.337953, elapsed=512.02s, remaining=1454.88s.\n",
      "Batch 4900: loss=0.338065, elapsed=522.53s, remaining=1443.79s.\n",
      "Batch 5000: loss=0.338228, elapsed=533.08s, remaining=1432.81s.\n",
      "Batch 5100: loss=0.338369, elapsed=543.64s, remaining=1421.87s.\n",
      "Batch 5200: loss=0.337377, elapsed=554.18s, remaining=1410.90s.\n",
      "Batch 5300: loss=0.337379, elapsed=565.06s, remaining=1400.78s.\n",
      "Batch 5400: loss=0.337566, elapsed=576.01s, remaining=1390.82s.\n",
      "Batch 5500: loss=0.337147, elapsed=586.70s, remaining=1380.19s.\n",
      "Batch 5600: loss=0.337157, elapsed=597.39s, remaining=1369.59s.\n",
      "Batch 5700: loss=0.337042, elapsed=607.88s, remaining=1358.54s.\n",
      "Batch 5800: loss=0.336584, elapsed=618.54s, remaining=1347.85s.\n",
      "Batch 5900: loss=0.335884, elapsed=629.22s, remaining=1337.20s.\n",
      "Batch 6000: loss=0.335890, elapsed=639.94s, remaining=1326.66s.\n",
      "Batch 6100: loss=0.336037, elapsed=650.73s, remaining=1316.25s.\n",
      "Batch 6200: loss=0.336076, elapsed=661.40s, remaining=1305.61s.\n",
      "Batch 6300: loss=0.335629, elapsed=671.91s, remaining=1294.62s.\n",
      "Batch 6400: loss=0.335293, elapsed=682.67s, remaining=1284.15s.\n",
      "Batch 6500: loss=0.335234, elapsed=693.23s, remaining=1273.29s.\n",
      "Batch 6600: loss=0.335516, elapsed=703.96s, remaining=1262.77s.\n",
      "Batch 6700: loss=0.335118, elapsed=714.71s, remaining=1252.25s.\n",
      "Batch 6800: loss=0.334779, elapsed=725.17s, remaining=1241.20s.\n",
      "Batch 6900: loss=0.334543, elapsed=736.11s, remaining=1231.00s.\n",
      "Batch 7000: loss=0.333763, elapsed=746.82s, remaining=1220.40s.\n",
      "Batch 7100: loss=0.334120, elapsed=757.33s, remaining=1209.47s.\n",
      "Batch 7200: loss=0.334032, elapsed=768.20s, remaining=1199.15s.\n",
      "Batch 7300: loss=0.334426, elapsed=779.10s, remaining=1188.85s.\n",
      "Batch 7400: loss=0.334777, elapsed=790.02s, remaining=1178.58s.\n",
      "Batch 7500: loss=0.334506, elapsed=800.61s, remaining=1167.78s.\n",
      "Batch 7600: loss=0.334185, elapsed=811.25s, remaining=1157.07s.\n",
      "Batch 7700: loss=0.333611, elapsed=822.02s, remaining=1146.54s.\n",
      "Batch 7800: loss=0.333479, elapsed=832.73s, remaining=1135.93s.\n",
      "Batch 7900: loss=0.332912, elapsed=843.33s, remaining=1125.15s.\n",
      "Batch 8000: loss=0.333222, elapsed=853.98s, remaining=1114.42s.\n",
      "Batch 8100: loss=0.333425, elapsed=864.65s, remaining=1103.74s.\n",
      "Batch 8200: loss=0.333464, elapsed=875.07s, remaining=1092.74s.\n",
      "Batch 8300: loss=0.333182, elapsed=885.93s, remaining=1082.30s.\n",
      "Batch 8400: loss=0.332847, elapsed=896.63s, remaining=1071.66s.\n",
      "Batch 8500: loss=0.332717, elapsed=907.35s, remaining=1061.03s.\n",
      "Batch 8600: loss=0.332365, elapsed=917.88s, remaining=1050.18s.\n",
      "Batch 8700: loss=0.331899, elapsed=928.41s, remaining=1039.37s.\n",
      "Batch 8800: loss=0.331743, elapsed=939.22s, remaining=1028.83s.\n",
      "Batch 8900: loss=0.332032, elapsed=949.86s, remaining=1018.12s.\n",
      "Batch 9000: loss=0.331825, elapsed=960.75s, remaining=1007.69s.\n",
      "Batch 9100: loss=0.331939, elapsed=971.79s, remaining=997.41s.\n",
      "Batch 9200: loss=0.331735, elapsed=982.81s, remaining=987.09s.\n",
      "Batch 9300: loss=0.331830, elapsed=993.48s, remaining=976.40s.\n",
      "Batch 9400: loss=0.331808, elapsed=1004.21s, remaining=965.77s.\n",
      "Batch 9500: loss=0.331104, elapsed=1014.64s, remaining=954.84s.\n",
      "Batch 9600: loss=0.331104, elapsed=1025.00s, remaining=943.86s.\n",
      "Batch 9700: loss=0.331229, elapsed=1035.70s, remaining=933.19s.\n",
      "Batch 9800: loss=0.331603, elapsed=1046.37s, remaining=922.51s.\n",
      "Batch 9900: loss=0.331385, elapsed=1056.99s, remaining=911.79s.\n",
      "Batch 10000: loss=0.331146, elapsed=1067.48s, remaining=900.95s.\n",
      "Batch 10100: loss=0.331312, elapsed=1078.18s, remaining=890.29s.\n",
      "Batch 10200: loss=0.330792, elapsed=1088.67s, remaining=879.46s.\n",
      "Batch 10300: loss=0.330575, elapsed=1099.29s, remaining=868.74s.\n",
      "Batch 10400: loss=0.330936, elapsed=1110.22s, remaining=858.28s.\n",
      "Batch 10500: loss=0.331095, elapsed=1120.67s, remaining=847.42s.\n",
      "Batch 10600: loss=0.330899, elapsed=1131.20s, remaining=836.65s.\n",
      "Batch 10700: loss=0.331180, elapsed=1141.70s, remaining=825.86s.\n",
      "Batch 10800: loss=0.330930, elapsed=1152.39s, remaining=815.20s.\n",
      "Batch 10900: loss=0.330958, elapsed=1163.13s, remaining=804.59s.\n",
      "Batch 11000: loss=0.330973, elapsed=1173.77s, remaining=793.90s.\n",
      "Batch 11100: loss=0.330725, elapsed=1184.39s, remaining=783.18s.\n",
      "Batch 11200: loss=0.330619, elapsed=1194.91s, remaining=772.40s.\n",
      "Batch 11300: loss=0.330461, elapsed=1205.80s, remaining=761.88s.\n",
      "Batch 11400: loss=0.330696, elapsed=1216.43s, remaining=751.17s.\n",
      "Batch 11500: loss=0.330940, elapsed=1227.11s, remaining=740.51s.\n",
      "Batch 11600: loss=0.330986, elapsed=1237.76s, remaining=729.82s.\n",
      "Batch 11700: loss=0.330788, elapsed=1248.39s, remaining=719.12s.\n",
      "Batch 11800: loss=0.330348, elapsed=1258.87s, remaining=708.34s.\n",
      "Batch 11900: loss=0.330495, elapsed=1269.49s, remaining=697.64s.\n",
      "Batch 12000: loss=0.330470, elapsed=1280.00s, remaining=686.89s.\n",
      "Batch 12100: loss=0.330184, elapsed=1290.65s, remaining=676.22s.\n",
      "Batch 12200: loss=0.329995, elapsed=1301.49s, remaining=665.64s.\n",
      "Batch 12300: loss=0.329964, elapsed=1312.28s, remaining=655.03s.\n",
      "Batch 12400: loss=0.329614, elapsed=1322.72s, remaining=644.24s.\n",
      "Batch 12500: loss=0.329211, elapsed=1333.24s, remaining=633.51s.\n",
      "Batch 12600: loss=0.329292, elapsed=1343.91s, remaining=622.85s.\n",
      "Batch 12700: loss=0.329366, elapsed=1354.48s, remaining=612.15s.\n",
      "Batch 12800: loss=0.329363, elapsed=1365.19s, remaining=601.50s.\n",
      "Batch 12900: loss=0.329081, elapsed=1375.98s, remaining=590.89s.\n",
      "Batch 13000: loss=0.329359, elapsed=1386.82s, remaining=580.30s.\n",
      "Batch 13100: loss=0.329511, elapsed=1397.63s, remaining=569.69s.\n",
      "Batch 13200: loss=0.329220, elapsed=1408.16s, remaining=558.96s.\n",
      "Batch 13300: loss=0.329142, elapsed=1418.75s, remaining=548.27s.\n",
      "Batch 13400: loss=0.329172, elapsed=1429.29s, remaining=537.55s.\n",
      "Batch 13500: loss=0.328972, elapsed=1439.92s, remaining=526.87s.\n",
      "Batch 13600: loss=0.328910, elapsed=1450.68s, remaining=516.26s.\n",
      "Batch 13700: loss=0.328261, elapsed=1461.30s, remaining=505.58s.\n",
      "Batch 13800: loss=0.328749, elapsed=1471.84s, remaining=494.87s.\n",
      "Batch 13900: loss=0.329025, elapsed=1482.56s, remaining=484.22s.\n",
      "Batch 14000: loss=0.328902, elapsed=1493.15s, remaining=473.53s.\n",
      "Batch 14100: loss=0.328927, elapsed=1503.79s, remaining=462.87s.\n",
      "Batch 14200: loss=0.329060, elapsed=1514.42s, remaining=452.19s.\n",
      "Batch 14300: loss=0.329218, elapsed=1525.15s, remaining=441.55s.\n",
      "Batch 14400: loss=0.329287, elapsed=1535.60s, remaining=430.81s.\n",
      "Batch 14500: loss=0.329390, elapsed=1546.03s, remaining=420.07s.\n",
      "Batch 14600: loss=0.329243, elapsed=1556.74s, remaining=409.43s.\n",
      "Batch 14700: loss=0.329143, elapsed=1567.07s, remaining=398.68s.\n",
      "Batch 14800: loss=0.329201, elapsed=1577.72s, remaining=388.02s.\n",
      "Batch 14900: loss=0.328838, elapsed=1588.28s, remaining=377.34s.\n",
      "Batch 15000: loss=0.328863, elapsed=1599.14s, remaining=366.73s.\n",
      "Batch 15100: loss=0.329088, elapsed=1609.85s, remaining=356.08s.\n",
      "Batch 15200: loss=0.329019, elapsed=1620.50s, remaining=345.42s.\n",
      "Batch 15300: loss=0.328915, elapsed=1630.99s, remaining=334.72s.\n",
      "Batch 15400: loss=0.328883, elapsed=1641.71s, remaining=324.07s.\n",
      "Batch 15500: loss=0.328777, elapsed=1652.55s, remaining=313.45s.\n",
      "Batch 15600: loss=0.328755, elapsed=1663.16s, remaining=302.76s.\n",
      "Batch 15700: loss=0.328839, elapsed=1673.73s, remaining=292.09s.\n",
      "Batch 15800: loss=0.328855, elapsed=1684.25s, remaining=281.41s.\n",
      "Batch 15900: loss=0.328707, elapsed=1694.95s, remaining=270.75s.\n",
      "Batch 16000: loss=0.328534, elapsed=1705.62s, remaining=260.08s.\n",
      "Batch 16100: loss=0.328528, elapsed=1716.39s, remaining=249.45s.\n",
      "Batch 16200: loss=0.328469, elapsed=1727.20s, remaining=238.81s.\n",
      "Batch 16300: loss=0.328063, elapsed=1737.78s, remaining=228.14s.\n",
      "Batch 16400: loss=0.328015, elapsed=1748.46s, remaining=217.48s.\n",
      "Batch 16500: loss=0.327966, elapsed=1759.14s, remaining=206.82s.\n",
      "Batch 16600: loss=0.327580, elapsed=1769.68s, remaining=196.14s.\n",
      "Batch 16700: loss=0.327548, elapsed=1780.62s, remaining=185.51s.\n",
      "Batch 16800: loss=0.327259, elapsed=1791.17s, remaining=174.83s.\n",
      "Batch 16900: loss=0.327117, elapsed=1801.72s, remaining=164.15s.\n",
      "Batch 17000: loss=0.327105, elapsed=1812.32s, remaining=153.49s.\n",
      "Batch 17100: loss=0.326778, elapsed=1822.95s, remaining=142.83s.\n",
      "Batch 17200: loss=0.326679, elapsed=1833.61s, remaining=132.17s.\n",
      "Batch 17300: loss=0.326483, elapsed=1844.17s, remaining=121.50s.\n",
      "Batch 17400: loss=0.326415, elapsed=1854.85s, remaining=110.85s.\n",
      "Batch 17500: loss=0.326308, elapsed=1865.54s, remaining=100.20s.\n",
      "Batch 17600: loss=0.326195, elapsed=1876.05s, remaining=89.53s.\n",
      "Batch 17700: loss=0.326016, elapsed=1886.77s, remaining=78.87s.\n",
      "Batch 17800: loss=0.325656, elapsed=1897.57s, remaining=68.22s.\n",
      "Batch 17900: loss=0.325652, elapsed=1908.46s, remaining=57.57s.\n",
      "Batch 18000: loss=0.325570, elapsed=1919.15s, remaining=46.91s.\n",
      "Batch 18100: loss=0.325494, elapsed=1929.66s, remaining=36.24s.\n",
      "Batch 18200: loss=0.325522, elapsed=1940.20s, remaining=25.58s.\n",
      "Batch 18300: loss=0.325366, elapsed=1950.74s, remaining=14.92s.\n",
      "Batch 18400: loss=0.325251, elapsed=1961.56s, remaining=4.27s.\n",
      "Batch 18500: loss=0.325136, elapsed=1972.18s, remaining=-6.39s.\n",
      "Batch 18600: loss=0.325235, elapsed=1982.81s, remaining=-17.05s.\n",
      "Batch 18700: loss=0.324860, elapsed=1993.68s, remaining=-27.71s.\n",
      "Batch 18800: loss=0.324971, elapsed=2004.42s, remaining=-38.38s.\n",
      "Batch 18900: loss=0.325053, elapsed=2015.07s, remaining=-49.04s.\n",
      "\n",
      "Training epoch took: 2023.48s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    timing_log = train_loop(train_dataloader, model,optimizer, scheduler, device, criterion_sent, criterion_topic, sentiment_var='sentiment',\n",
    "               topic_var='topic', timing_log=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/manifesto_ContextScalePrediction_main/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 47.78s, Estimated remaining time: 52.99s\n",
      "Elapsed time: 94.13s, Estimated remaining time: 5.13s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_main = scale_func(test_dataloader, \n",
    "               model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  precision  recall  accuracy\n",
       "0   0.85       0.82    0.87      0.87\n",
       "1   0.88       0.88    0.88      0.88\n",
       "2   0.91       0.90    0.92      0.92\n",
       "3   0.91       0.91    0.91      0.91\n",
       "4   0.88       0.87    0.89      0.89\n",
       "5   0.85       0.86    0.85      0.85\n",
       "6   0.82       0.82    0.83      0.83\n",
       "7   0.88       0.88    0.89      0.89\n",
       "8   0.91       0.90    0.91      0.91\n",
       "9   0.91       0.91    0.92      0.92\n",
       "10  0.85       0.86    0.84      0.84\n",
       "11  0.85       0.86    0.85      0.85"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_main['res_table_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_main['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  precision  recall  accuracy\n",
       "0  0.92       0.92    0.93      0.93\n",
       "1  0.87       0.88    0.86      0.86\n",
       "2  0.85       0.85    0.85      0.85"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_main['res_table_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_main['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_main'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_main, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_main['res_table_sentiment'].to_csv('results/classification results/main_sentiment.csv', index=False)\n",
    "outputs_main['res_table_topic'].to_csv('results/classification results/main_topic.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling party positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto = pd.read_csv('data/temps/manifesto.csv', encoding='utf-8', dtype={2:'str',18: 'str'})\n",
    "manifesto_regrouped = pd.read_csv('data/temps/manifesto_regrouped.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For all country-party-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 337412 entries, 0 to 337411\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   text       337412 non-null  object \n",
      " 1   idx        337412 non-null  object \n",
      " 2   country    337412 non-null  object \n",
      " 3   election   337412 non-null  int64  \n",
      " 4   party      337412 non-null  int64  \n",
      " 5   cmp_code   337412 non-null  float64\n",
      " 6   sentiment  337412 non-null  object \n",
      " 7   topic      337412 non-null  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 20.6+ MB\n"
     ]
    }
   ],
   "source": [
    "manifesto_regrouped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_kws = dict({'left': 'left', 'right': 'right'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "absscale, relscale, logscale, name_ls = cmp_scale(manifesto, text_var = 'text', group_vars=['countryname','party','election'], lr_kws = lr_kws, sent_var='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 12\n",
    "num_sentiments = 3\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, num_topics=num_topics, num_sentiments=num_sentiments,lora=False,\n",
    "                                       use_shared_attention=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_main/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7133dab13d34f468b953821ed3c46a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/337412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['topic', 'sentiment', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = manifesto_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text', 'topic_sentiment'])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 34.33s, Estimated remaining time: 689.66s\n",
      "Elapsed time: 77.89s, Estimated remaining time: 743.39s\n",
      "Elapsed time: 120.18s, Estimated remaining time: 724.67s\n",
      "Elapsed time: 163.04s, Estimated remaining time: 696.55s\n",
      "Elapsed time: 203.13s, Estimated remaining time: 653.62s\n",
      "Elapsed time: 237.77s, Estimated remaining time: 597.96s\n",
      "Elapsed time: 271.47s, Estimated remaining time: 546.39s\n",
      "Elapsed time: 308.70s, Estimated remaining time: 505.07s\n",
      "Elapsed time: 354.65s, Estimated remaining time: 476.38s\n",
      "Elapsed time: 394.39s, Estimated remaining time: 437.34s\n",
      "Elapsed time: 435.36s, Estimated remaining time: 399.30s\n",
      "Elapsed time: 467.14s, Estimated remaining time: 353.82s\n",
      "Elapsed time: 498.68s, Estimated remaining time: 310.30s\n",
      "Elapsed time: 530.14s, Estimated remaining time: 268.44s\n",
      "Elapsed time: 561.77s, Estimated remaining time: 228.04s\n",
      "Elapsed time: 607.49s, Estimated remaining time: 193.22s\n",
      "Elapsed time: 653.22s, Estimated remaining time: 157.12s\n",
      "Elapsed time: 704.25s, Estimated remaining time: 120.86s\n",
      "Elapsed time: 755.84s, Estimated remaining time: 83.10s\n",
      "Elapsed time: 793.14s, Estimated remaining time: 43.19s\n",
      "Elapsed time: 826.56s, Estimated remaining time: 3.50s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_all = scale_func(pred_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_names = manifesto_dataset.features['sentiment'].names\n",
    "name_sentiment_dict = dict([(x,y) for x,y in enumerate(list_names)])\n",
    "list_names = manifesto_dataset.features['topic'].names\n",
    "name_topic_dict = dict([(x,y) for x,y in enumerate(list_names)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Agriculture - Protectionism',\n",
       " 1: 'Economics',\n",
       " 2: 'Education',\n",
       " 3: 'Environment - Growth',\n",
       " 4: 'European Integration',\n",
       " 5: 'Fabrics of Society',\n",
       " 6: 'Immigration',\n",
       " 7: 'International Relations',\n",
       " 8: 'Labour and Social Welfare',\n",
       " 9: 'Military',\n",
       " 10: 'Other',\n",
       " 11: 'Political System'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.loc[:,'position_scores'] = outputs_all['position_scores'].flatten()\n",
    "manifesto_regrouped.loc[:,'pred_sentiment_index'] = outputs_all['pred_sentiment']\n",
    "manifesto_regrouped.loc[:,'pred_sentiment'] = manifesto_regrouped.pred_sentiment_index.map(name_sentiment_dict)\n",
    "manifesto_regrouped.loc[:,'pred_topic_index'] = outputs_all['pred_topics']\n",
    "manifesto_regrouped.loc[:,'pred_topic'] = manifesto_regrouped.pred_topic_index.map(name_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  =['country','party', 'election','mean_score', 'se_score']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for name, group in manifesto_regrouped.groupby(['country','party','election']):\n",
    "    mean_score = group['position_scores'].mean()\n",
    "    se_score = group['position_scores'].std()/np.sqrt(len(group))\n",
    "    df_temp = pd.DataFrame([[str(group.iloc[0,group.columns.get_loc('country')]),\n",
    "                             str(group.iloc[0,group.columns.get_loc('party')]), \n",
    "                    str(group.iloc[0,group.columns.get_loc('election')]), \n",
    "               mean_score, se_score]], columns = columns)\n",
    "    df = (df_temp if df.empty else pd.concat([df, df_temp], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lr_log'] = logscale\n",
    "df['lr_abs'] = absscale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.90054232],\n",
       "       [0.90054232, 1.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df['lr_log'], df['mean_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save df \n",
    "df.to_csv('data/py_outputs/cs_gen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>idx</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>position_scores</th>\n",
       "      <th>pred_sentiment_index</th>\n",
       "      <th>pred_sentiment</th>\n",
       "      <th>pred_topic_index</th>\n",
       "      <th>pred_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statt dessen soll ein Freiwilligen-Milizheer g...</td>\n",
       "      <td>Austria;1999;42110;104</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>104.0</td>\n",
       "      <td>right</td>\n",
       "      <td>Military</td>\n",
       "      <td>0.755618</td>\n",
       "      <td>2</td>\n",
       "      <td>right</td>\n",
       "      <td>9</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weg von der Sicherheit durch RÃ¼stung, Unsere S...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.884001</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "      <td>9</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Militarisierung der EU bringt mehr RÃ¼stung...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.846234</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "      <td>9</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.860604</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "      <td>9</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.864315</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "      <td>9</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                     idx  \\\n",
       "0  Statt dessen soll ein Freiwilligen-Milizheer g...  Austria;1999;42110;104   \n",
       "1  Weg von der Sicherheit durch RÃ¼stung, Unsere S...  Austria;1999;42110;105   \n",
       "2  Die Militarisierung der EU bringt mehr RÃ¼stung...  Austria;1999;42110;105   \n",
       "3  Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...  Austria;1999;42110;105   \n",
       "4  Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...  Austria;1999;42110;105   \n",
       "\n",
       "   country  election  party  cmp_code sentiment     topic  position_scores  \\\n",
       "0  Austria      1999  42110     104.0     right  Military         0.755618   \n",
       "1  Austria      1999  42110     105.0      left  Military        -0.884001   \n",
       "2  Austria      1999  42110     105.0      left  Military        -0.846234   \n",
       "3  Austria      1999  42110     105.0      left  Military        -0.860604   \n",
       "4  Austria      1999  42110     105.0      left  Military        -0.864315   \n",
       "\n",
       "   pred_sentiment_index pred_sentiment  pred_topic_index pred_topic  \n",
       "0                     2          right                 9   Military  \n",
       "1                     0           left                 9   Military  \n",
       "2                     0           left                 9   Military  \n",
       "3                     0           left                 9   Military  \n",
       "4                     0           left                 9   Military  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  precision  recall  accuracy\n",
       "0  0.97       0.97    0.98      0.98\n",
       "1  0.96       0.96    0.95      0.95\n",
       "2  0.95       0.95    0.95      0.95"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_all['res_table_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  precision  recall  accuracy\n",
       "0   0.95       0.94    0.96      0.96\n",
       "1   0.96       0.96    0.96      0.96\n",
       "2   0.98       0.97    0.98      0.98\n",
       "3   0.97       0.97    0.97      0.97\n",
       "4   0.96       0.96    0.97      0.97\n",
       "5   0.95       0.95    0.94      0.94\n",
       "6   0.94       0.94    0.94      0.94\n",
       "7   0.96       0.95    0.96      0.96\n",
       "8   0.97       0.97    0.97      0.97\n",
       "9   0.97       0.96    0.98      0.98\n",
       "10  0.95       0.96    0.94      0.94\n",
       "11  0.96       0.96    0.96      0.96"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_all['res_table_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.to_csv(\"data/py_outputs/cs_topic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Germany only - recording scaling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_rgr_deu = manifesto_regrouped[manifesto_regrouped.country == 'Germany'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30987 entries, 0 to 30986\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   text                  30987 non-null  object \n",
      " 1   idx                   30987 non-null  object \n",
      " 2   country               30987 non-null  object \n",
      " 3   election              30987 non-null  int64  \n",
      " 4   party                 30987 non-null  int64  \n",
      " 5   cmp_code              30987 non-null  float64\n",
      " 6   sentiment             30987 non-null  object \n",
      " 7   topic                 30987 non-null  object \n",
      " 8   position_scores       30987 non-null  float64\n",
      " 9   pred_sentiment_index  30987 non-null  int64  \n",
      " 10  pred_sentiment        30987 non-null  object \n",
      " 11  pred_topic_index      30987 non-null  int64  \n",
      " 12  pred_topic            30987 non-null  object \n",
      "dtypes: float64(2), int64(4), object(7)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "manifesto_rgr_deu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 12\n",
    "num_sentiments = 3\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, num_topics=num_topics, num_sentiments=num_sentiments,lora=False,\n",
    "                                       use_shared_attention=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_main/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14118691d1dd4319b49bc73806f20f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/30987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d94207756bc4d8ea4ef7eaea3a0adf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/30987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manifesto_dataset = Dataset.from_pandas(manifesto_rgr_deu[['text','topic','sentiment']])\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('topic')\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6b841754294a2194dd3845d1e0cc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['topic', 'sentiment', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = manifesto_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text'])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 33.71s, Estimated remaining time: 31.59s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_germany = scale_func(pred_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on test languages unseen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_test = pd.read_csv(os.path.join(\"data\", \"r_outputs\",\"pulled_manifestoes_test.csv\"), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>eu_code</th>\n",
       "      <th>pos</th>\n",
       "      <th>manifesto_id</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>translation_en</th>\n",
       "      <th>country</th>\n",
       "      <th>party_code</th>\n",
       "      <th>countryname</th>\n",
       "      <th>abbrev</th>\n",
       "      <th>name</th>\n",
       "      <th>edate</th>\n",
       "      <th>parfam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>å®‰å€æ”¿æ¨©ã®æš´èµ°ã‚¹ãƒˆãƒƒãƒ—ï¼</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>71220_201412</td>\n",
       "      <td>71220</td>\n",
       "      <td>201412</td>\n",
       "      <td>japanese</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>71</td>\n",
       "      <td>71220</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JCP</td>\n",
       "      <td>Nihon KyÅsan-tÅ</td>\n",
       "      <td>14/12/2014</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>å›½æ°‘ã®å£°ãŒç”Ÿãã‚‹æ–°ã—ã„æ”¿æ²»ã‚’</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>71220_201412</td>\n",
       "      <td>71220</td>\n",
       "      <td>201412</td>\n",
       "      <td>japanese</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>71</td>\n",
       "      <td>71220</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JCP</td>\n",
       "      <td>Nihon KyÅsan-tÅ</td>\n",
       "      <td>14/12/2014</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>æ—¥æœ¬å…±ç”£å…šã®ç·é¸æŒ™æ”¿ç­–</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>71220_201412</td>\n",
       "      <td>71220</td>\n",
       "      <td>201412</td>\n",
       "      <td>japanese</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>71</td>\n",
       "      <td>71220</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JCP</td>\n",
       "      <td>Nihon KyÅsan-tÅ</td>\n",
       "      <td>14/12/2014</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>æ—¥æœ¬å…±ç”£å…š</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>71220_201412</td>\n",
       "      <td>71220</td>\n",
       "      <td>201412</td>\n",
       "      <td>japanese</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>71</td>\n",
       "      <td>71220</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JCP</td>\n",
       "      <td>Nihon KyÅsan-tÅ</td>\n",
       "      <td>14/12/2014</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>å®‰å€æ”¿æ¨©ã®æš´èµ°ã‚¹ãƒˆãƒƒãƒ—ã€æ”¿æ²»ã‚’å¤‰ãˆã‚‹ãƒãƒ£ãƒ³ã‚¹ã§ã™â€¦â€¦</td>\n",
       "      <td>305.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>71220_201412</td>\n",
       "      <td>71220</td>\n",
       "      <td>201412</td>\n",
       "      <td>japanese</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>71</td>\n",
       "      <td>71220</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JCP</td>\n",
       "      <td>Nihon KyÅsan-tÅ</td>\n",
       "      <td>14/12/2014</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text cmp_code  eu_code  pos  manifesto_id  party  \\\n",
       "0                å®‰å€æ”¿æ¨©ã®æš´èµ°ã‚¹ãƒˆãƒƒãƒ—ï¼        H      NaN    1  71220_201412  71220   \n",
       "1              å›½æ°‘ã®å£°ãŒç”Ÿãã‚‹æ–°ã—ã„æ”¿æ²»ã‚’        H      NaN    2  71220_201412  71220   \n",
       "2                 æ—¥æœ¬å…±ç”£å…šã®ç·é¸æŒ™æ”¿ç­–        H      NaN    3  71220_201412  71220   \n",
       "3                       æ—¥æœ¬å…±ç”£å…š        H      NaN    4  71220_201412  71220   \n",
       "4  å®‰å€æ”¿æ¨©ã®æš´èµ°ã‚¹ãƒˆãƒƒãƒ—ã€æ”¿æ²»ã‚’å¤‰ãˆã‚‹ãƒãƒ£ãƒ³ã‚¹ã§ã™â€¦â€¦    305.1      NaN    5  71220_201412  71220   \n",
       "\n",
       "     date  language  annotations  translation_en  country  party_code  \\\n",
       "0  201412  japanese         True           False       71       71220   \n",
       "1  201412  japanese         True           False       71       71220   \n",
       "2  201412  japanese         True           False       71       71220   \n",
       "3  201412  japanese         True           False       71       71220   \n",
       "4  201412  japanese         True           False       71       71220   \n",
       "\n",
       "  countryname abbrev             name       edate  parfam  \n",
       "0       Japan    JCP  Nihon KyÅsan-tÅ  14/12/2014    20.0  \n",
       "1       Japan    JCP  Nihon KyÅsan-tÅ  14/12/2014    20.0  \n",
       "2       Japan    JCP  Nihon KyÅsan-tÅ  14/12/2014    20.0  \n",
       "3       Japan    JCP  Nihon KyÅsan-tÅ  14/12/2014    20.0  \n",
       "4       Japan    JCP  Nihon KyÅsan-tÅ  14/12/2014    20.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_test = manifesto_test[(manifesto_test.cmp_code.notna()) & ~(manifesto_test.cmp_code == 'H')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_test['sentiment'] = manifesto['cmp_code'].apply(sentiment_code)\n",
    "manifesto_test['topic'] = manifesto['cmp_code'].apply(topic_code)\n",
    "manifesto_test['election'] = manifesto['date'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = group_texts(manifesto_test, \n",
    "                      ['countryname','election','party','cmp_code'], 'text', \n",
    "                      max_group_factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped = pd.DataFrame(results)\n",
    "manifesto_regrouped = manifesto_regrouped.explode('text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = manifesto_regrouped['labels'].str.split(';', expand=True)\n",
    "manifesto_regrouped = pd.concat([manifesto_regrouped, df_cols], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.columns = ['text', 'country_election_party_code', 'country','election', 'party', 'cmp_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>country_election_party_code</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RopnÃ½ zlom je moment, kdy nastane vrchol svÄ›to...</td>\n",
       "      <td>Czech Republic;2013;82110;000</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>2013</td>\n",
       "      <td>82110</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NejzranitelnÄ›jÅ¡Ã­ zÂ ekonomik je trh SpojenÃ½ch s...</td>\n",
       "      <td>Czech Republic;2013;82110;000</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>2013</td>\n",
       "      <td>82110</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Na konci tohoto pÅ™Ã­stupu mohou bÃ½t prÃ¡zdnÃ© nÃ¡d...</td>\n",
       "      <td>Czech Republic;2013;82110;000</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>2013</td>\n",
       "      <td>82110</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KdyÅ¾ jsme pÅ™ed ÄtyÅ™mi lety vstupovali do Posla...</td>\n",
       "      <td>Czech Republic;2013;82110;000</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>2013</td>\n",
       "      <td>82110</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZmÄ›ny, kterÃ© se sice vÄ›tÅ¡inou nedostanou na pr...</td>\n",
       "      <td>Czech Republic;2013;82110;000</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>2013</td>\n",
       "      <td>82110</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RopnÃ½ zlom je moment, kdy nastane vrchol svÄ›to...   \n",
       "1  NejzranitelnÄ›jÅ¡Ã­ zÂ ekonomik je trh SpojenÃ½ch s...   \n",
       "2  Na konci tohoto pÅ™Ã­stupu mohou bÃ½t prÃ¡zdnÃ© nÃ¡d...   \n",
       "3  KdyÅ¾ jsme pÅ™ed ÄtyÅ™mi lety vstupovali do Posla...   \n",
       "4  ZmÄ›ny, kterÃ© se sice vÄ›tÅ¡inou nedostanou na pr...   \n",
       "\n",
       "     country_election_party_code         country election  party cmp_code  \n",
       "0  Czech Republic;2013;82110;000  Czech Republic     2013  82110      000  \n",
       "1  Czech Republic;2013;82110;000  Czech Republic     2013  82110      000  \n",
       "2  Czech Republic;2013;82110;000  Czech Republic     2013  82110      000  \n",
       "3  Czech Republic;2013;82110;000  Czech Republic     2013  82110      000  \n",
       "4  Czech Republic;2013;82110;000  Czech Republic     2013  82110      000  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.loc[:,'sentiment'] = manifesto_regrouped['cmp_code'].apply(sentiment_code)\n",
    "manifesto_regrouped.loc[:,'topic'] = manifesto_regrouped['cmp_code'].apply(topic_code)\n",
    "manifesto_regrouped = manifesto_regrouped.drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>country_election_party_code</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Agriculture - Protectionism</th>\n",
       "      <th>left</th>\n",
       "      <td>1354</td>\n",
       "      <td>1354</td>\n",
       "      <td>1354</td>\n",
       "      <td>1354</td>\n",
       "      <td>1354</td>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Economics</th>\n",
       "      <th>left</th>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>4174</td>\n",
       "      <td>4174</td>\n",
       "      <td>4174</td>\n",
       "      <td>4174</td>\n",
       "      <td>4174</td>\n",
       "      <td>4174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Education</th>\n",
       "      <th>left</th>\n",
       "      <td>1571</td>\n",
       "      <td>1571</td>\n",
       "      <td>1571</td>\n",
       "      <td>1571</td>\n",
       "      <td>1571</td>\n",
       "      <td>1571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Environment - Growth</th>\n",
       "      <th>left</th>\n",
       "      <td>1235</td>\n",
       "      <td>1235</td>\n",
       "      <td>1235</td>\n",
       "      <td>1235</td>\n",
       "      <td>1235</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1105</td>\n",
       "      <td>1105</td>\n",
       "      <td>1105</td>\n",
       "      <td>1105</td>\n",
       "      <td>1105</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">European Integration</th>\n",
       "      <th>left</th>\n",
       "      <td>233</td>\n",
       "      <td>233</td>\n",
       "      <td>233</td>\n",
       "      <td>233</td>\n",
       "      <td>233</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Fabrics of Society</th>\n",
       "      <th>left</th>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1837</td>\n",
       "      <td>1837</td>\n",
       "      <td>1837</td>\n",
       "      <td>1837</td>\n",
       "      <td>1837</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Immigration</th>\n",
       "      <th>left</th>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>882</td>\n",
       "      <td>882</td>\n",
       "      <td>882</td>\n",
       "      <td>882</td>\n",
       "      <td>882</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">International Relations</th>\n",
       "      <th>left</th>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Labour and Social Welfare</th>\n",
       "      <th>left</th>\n",
       "      <td>4333</td>\n",
       "      <td>4333</td>\n",
       "      <td>4333</td>\n",
       "      <td>4333</td>\n",
       "      <td>4333</td>\n",
       "      <td>4333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Military</th>\n",
       "      <th>left</th>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>neutral</th>\n",
       "      <td>4298</td>\n",
       "      <td>4298</td>\n",
       "      <td>4298</td>\n",
       "      <td>4298</td>\n",
       "      <td>4298</td>\n",
       "      <td>4298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Political System</th>\n",
       "      <th>left</th>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1739</td>\n",
       "      <td>1739</td>\n",
       "      <td>1739</td>\n",
       "      <td>1739</td>\n",
       "      <td>1739</td>\n",
       "      <td>1739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text  country_election_party_code  \\\n",
       "topic                       sentiment                                      \n",
       "Agriculture - Protectionism left       1354                         1354   \n",
       "                            right       128                          128   \n",
       "Economics                   left       2060                         2060   \n",
       "                            neutral    4174                         4174   \n",
       "                            right      2282                         2282   \n",
       "Education                   left       1571                         1571   \n",
       "                            right         4                            4   \n",
       "Environment - Growth        left       1235                         1235   \n",
       "                            neutral     159                          159   \n",
       "                            right      1105                         1105   \n",
       "European Integration        left        233                          233   \n",
       "                            right        69                           69   \n",
       "Fabrics of Society          left       1050                         1050   \n",
       "                            neutral     494                          494   \n",
       "                            right      1837                         1837   \n",
       "Immigration                 left        193                          193   \n",
       "                            right       882                          882   \n",
       "International Relations     left       1094                         1094   \n",
       "                            neutral     266                          266   \n",
       "                            right        68                           68   \n",
       "Labour and Social Welfare   left       4333                         4333   \n",
       "                            neutral      88                           88   \n",
       "                            right        92                           92   \n",
       "Military                    left        156                          156   \n",
       "                            right       581                          581   \n",
       "Other                       neutral    4298                         4298   \n",
       "Political System            left         56                           56   \n",
       "                            neutral    1739                         1739   \n",
       "                            right       747                          747   \n",
       "\n",
       "                                       country  election  party  cmp_code  \n",
       "topic                       sentiment                                      \n",
       "Agriculture - Protectionism left          1354      1354   1354      1354  \n",
       "                            right          128       128    128       128  \n",
       "Economics                   left          2060      2060   2060      2060  \n",
       "                            neutral       4174      4174   4174      4174  \n",
       "                            right         2282      2282   2282      2282  \n",
       "Education                   left          1571      1571   1571      1571  \n",
       "                            right            4         4      4         4  \n",
       "Environment - Growth        left          1235      1235   1235      1235  \n",
       "                            neutral        159       159    159       159  \n",
       "                            right         1105      1105   1105      1105  \n",
       "European Integration        left           233       233    233       233  \n",
       "                            right           69        69     69        69  \n",
       "Fabrics of Society          left          1050      1050   1050      1050  \n",
       "                            neutral        494       494    494       494  \n",
       "                            right         1837      1837   1837      1837  \n",
       "Immigration                 left           193       193    193       193  \n",
       "                            right          882       882    882       882  \n",
       "International Relations     left          1094      1094   1094      1094  \n",
       "                            neutral        266       266    266       266  \n",
       "                            right           68        68     68        68  \n",
       "Labour and Social Welfare   left          4333      4333   4333      4333  \n",
       "                            neutral         88        88     88        88  \n",
       "                            right           92        92     92        92  \n",
       "Military                    left           156       156    156       156  \n",
       "                            right          581       581    581       581  \n",
       "Other                       neutral       4298      4298   4298      4298  \n",
       "Political System            left            56        56     56        56  \n",
       "                            neutral       1739      1739   1739      1739  \n",
       "                            right          747       747    747       747  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.groupby(['topic','sentiment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_reduced = manifesto_regrouped[['topic','sentiment','text']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd30c2d620734605931d22eccd055660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/32348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a8266e718140bf89ec8f9045758a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/32348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manifesto_dataset = Dataset.from_pandas(manifesto_reduced)\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('topic')\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('sentiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65162be53b84c30b38d64a58057ab0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = manifesto_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "pred_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, num_topics=num_topics, num_sentiments=num_sentiments,lora=False,\n",
    "                                       use_shared_attention=True).to(device)\n",
    "\n",
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_main/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 36.93s, Estimated remaining time: 37.74s\n",
      "Elapsed time: 74.30s, Estimated remaining time: 0.82s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_dl = scale_func(pred_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1           0.76\n",
       "precision    0.75\n",
       "recall       0.78\n",
       "accuracy     0.78\n",
       "dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dl['res_table_topic'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1           0.76\n",
       "precision    0.76\n",
       "recall       0.76\n",
       "accuracy     0.76\n",
       "dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dl['res_table_sentiment'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_dl'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_dl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dl['res_table_sentiment'].to_csv('results/classification results/dl_sentiment.csv', index=False)\n",
    "outputs_dl['res_table_topic'].to_csv('results/classification results/dl_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model using only 10% of labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_reduced['topic_sentiment'] = manifesto_reduced['topic'] + '_' + manifesto_reduced['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf9eb79de54528bb123d3fb66bd78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/32348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ff9f112e044e3d8b7a3de1fcaae998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/32348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432197017a9649ab855e12063ee95e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/32348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manifesto_dataset = Dataset.from_pandas(manifesto_reduced)\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('topic')\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('sentiment')\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('topic_sentiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = manifesto_dataset.train_test_split(test_size=0.9, stratify_by_column='topic_sentiment', seed=seed_val)\n",
    "train_eval = train_test['train'].train_test_split(test_size=0.3, stratify_by_column='topic_sentiment', seed=seed_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 2263\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 29114\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['topic', 'sentiment', 'text', 'topic_sentiment'],\n",
       "        num_rows: 971\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_datasets = DatasetDict({\n",
    "    'train': train_eval['train'],\n",
    "    'test': train_test['test'],\n",
    "    'eval': train_eval['test']\n",
    "})\n",
    "manifesto_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80b25029fb64c46b6fb90127d80aaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01568c0597374b848a4297228d44b547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86f466618f9429e9ab695ccf8693496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['topic', 'sentiment', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = manifesto_datasets.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text', 'topic_sentiment'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=16, shuffle=False, collate_fn = data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['eval'], batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_shared_attention=True).to(device)\n",
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_main/model.safetensors')\n",
    "model.load_state_dict(loaded_tensors)\n",
    "scaling_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion_sent = nn.CrossEntropyLoss()\n",
    "criterion_topic =  nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=1.523980, elapsed=10.65s, remaining=3.91s.\n",
      "\n",
      "Training epoch took: 14.99s\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 1.205339 \n",
      "\n",
      "Accuracy - Sentiment: 78.3%, Avg loss: 1.205339 \n",
      "\n",
      "Accuracy - Topic: 79.5%, Avg loss: 1.205339 \n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.783501, elapsed=10.44s, remaining=3.81s.\n",
      "\n",
      "Training epoch took: 14.44s\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 1.258797 \n",
      "\n",
      "Accuracy - Sentiment: 79.9%, Avg loss: 1.258797 \n",
      "\n",
      "Accuracy - Topic: 80.5%, Avg loss: 1.258797 \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.397384, elapsed=10.19s, remaining=3.72s.\n",
      "\n",
      "Training epoch took: 14.39s\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 1.397560 \n",
      "\n",
      "Accuracy - Sentiment: 80.6%, Avg loss: 1.397560 \n",
      "\n",
      "Accuracy - Topic: 80.9%, Avg loss: 1.397560 \n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.223953, elapsed=10.24s, remaining=3.74s.\n",
      "\n",
      "Training epoch took: 14.47s\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 1.592707 \n",
      "\n",
      "Accuracy - Sentiment: 81.1%, Avg loss: 1.592707 \n",
      "\n",
      "Accuracy - Topic: 80.4%, Avg loss: 1.592707 \n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.170274, elapsed=10.31s, remaining=3.76s.\n",
      "\n",
      "Training epoch took: 14.48s\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 1.616914 \n",
      "\n",
      "Accuracy - Sentiment: 81.7%, Avg loss: 1.616914 \n",
      "\n",
      "Accuracy - Topic: 81.7%, Avg loss: 1.616914 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    timing_log = train_loop(train_dataloader, model,optimizer, scheduler, device, criterion_sent, criterion_topic, sentiment_var='sentiment',\n",
    "               topic_var='topic', timing_log=True)\n",
    "    eval_loop(eval_dataloader, model, device, criterion_sent, criterion_topic, sentiment_var='sentiment', topic_var='topic')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/manifesto_ContextScalePrediction_dl_10/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, num_topics=num_topics, num_sentiments=num_sentiments,lora=False,\n",
    "                                       use_shared_attention=True).to(device)\n",
    "\n",
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_dl_10/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 42.09s, Estimated remaining time: 34.51s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_dl_10 = scale_func(test_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  precision  recall  accuracy\n",
       "0   0.83       0.84    0.82      0.82\n",
       "1   0.86       0.85    0.87      0.87\n",
       "2   0.90       0.89    0.91      0.91\n",
       "3   0.80       0.82    0.79      0.79\n",
       "4   0.84       0.78    0.90      0.90\n",
       "5   0.76       0.76    0.77      0.77\n",
       "6   0.76       0.72    0.81      0.81\n",
       "7   0.81       0.85    0.78      0.78\n",
       "8   0.83       0.83    0.84      0.84\n",
       "9   0.82       0.77    0.89      0.89\n",
       "10  0.75       0.77    0.73      0.73\n",
       "11  0.78       0.80    0.76      0.76"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dl_10['res_table_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dl_10['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  precision  recall  accuracy\n",
       "0  0.85       0.85    0.86      0.86\n",
       "1  0.81       0.81    0.80      0.80\n",
       "2  0.76       0.76    0.77      0.77"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dl_10['res_table_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dl_10['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_dl_10'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_dl_10, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dl_10['res_table_sentiment'].to_csv('results/classification results/dl_10_sentiment.csv', index=False)\n",
    "outputs_dl_10['res_table_topic'].to_csv('results/classification results/dl_10_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 36.73s, Estimated remaining time: 37.54s\n",
      "Elapsed time: 74.11s, Estimated remaining time: 0.82s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_dl_10_all = scale_func(pred_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.loc[:,'position_scores'] = outputs_dl_10_all['position_scores'].flatten()\n",
    "manifesto_regrouped.loc[:,'pred_sentiment_index'] = outputs_dl_10_all['pred_sentiment']\n",
    "manifesto_regrouped.loc[:,'pred_sentiment'] = manifesto_regrouped.pred_sentiment_index.map(name_sentiment_dict)\n",
    "manifesto_regrouped.loc[:,'pred_topic_index'] = outputs_dl_10_all['pred_topics']\n",
    "manifesto_regrouped.loc[:,'pred_topic'] = manifesto_regrouped.pred_topic_index.map(name_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.to_csv('data/py_outputs/manifesto_dl_10_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COALITIONAGREE, same coding style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalitionagree = pd.read_csv('data/r_outputs/coalitionagree_texts.csv', encoding='utf-8', index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>domain</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>level</th>\n",
       "      <th>id</th>\n",
       "      <th>country_init</th>\n",
       "      <th>cabinet_year</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Abkommen vom Dezember 1945</td>\n",
       "      <td>8</td>\n",
       "      <td>800</td>\n",
       "      <td>80000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AT</td>\n",
       "      <td>1945</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Der Proporz soll nicht nur bei der Bildung der...</td>\n",
       "      <td>9</td>\n",
       "      <td>900</td>\n",
       "      <td>90001</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>AT</td>\n",
       "      <td>1945</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StaatssekretÃ¤re sollen nur in AusnahmefÃ¤llen n...</td>\n",
       "      <td>9</td>\n",
       "      <td>900</td>\n",
       "      <td>90004</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>AT</td>\n",
       "      <td>1945</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Das Programm der Parteien soll in der ErklÃ¤run...</td>\n",
       "      <td>9</td>\n",
       "      <td>900</td>\n",
       "      <td>90001</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>AT</td>\n",
       "      <td>1945</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Ã–sterreichische Volkspartei bietet den Soz...</td>\n",
       "      <td>9</td>\n",
       "      <td>900</td>\n",
       "      <td>90002</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>AT</td>\n",
       "      <td>1945</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  domain  category2  \\\n",
       "0                     1. Abkommen vom Dezember 1945        8        800   \n",
       "1  Der Proporz soll nicht nur bei der Bildung der...       9        900   \n",
       "2  StaatssekretÃ¤re sollen nur in AusnahmefÃ¤llen n...       9        900   \n",
       "3  Das Programm der Parteien soll in der ErklÃ¤run...       9        900   \n",
       "4  Die Ã–sterreichische Volkspartei bietet den Soz...       9        900   \n",
       "\n",
       "   category3  level  id country_init  cabinet_year  country  \n",
       "0      80000      0   1           AT          1945  Austria  \n",
       "1      90001      0   2           AT          1945  Austria  \n",
       "2      90004      0   3           AT          1945  Austria  \n",
       "3      90001      0   4           AT          1945  Austria  \n",
       "4      90002      0   5           AT          1945  Austria  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalitionagree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = group_texts(coalitionagree, ['country','cabinet_year','category2','category3'], 'sentence', max_group_factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped = pd.DataFrame(results)\n",
    "coalition_regrouped = coalition_regrouped.explode('text').reset_index(drop=True)\n",
    "df_cols = coalition_regrouped['labels'].str.split(';', expand=True)\n",
    "coalition_regrouped = pd.concat([coalition_regrouped, df_cols], axis=1)\n",
    "coalition_regrouped.columns =['text','labels', 'country','year', 'cmp_short','cmp_long']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>cmp_short</th>\n",
       "      <th>cmp_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abmachungen Ã¼ber die Beamtenbesoldung,  ebenso...</td>\n",
       "      <td>Austria;1945;303;30301</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>303</td>\n",
       "      <td>30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Der Gemeinde Wien soll, wenn die Sozialistisch...</td>\n",
       "      <td>Austria;1945;303;30303</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>303</td>\n",
       "      <td>30303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ã¼ber die Behandlung der Nationalsozialisten</td>\n",
       "      <td>Austria;1945;305;30506</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>305</td>\n",
       "      <td>30506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sowie Ã¼ber die Verstaatlichung werden in Aussi...</td>\n",
       "      <td>Austria;1945;413;41301</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>413</td>\n",
       "      <td>41301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Schaffung eines einheitlichen Dienst- und ...</td>\n",
       "      <td>Austria;1945;506;50602</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>506</td>\n",
       "      <td>50602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  labels  \\\n",
       "0  Abmachungen Ã¼ber die Beamtenbesoldung,  ebenso...  Austria;1945;303;30301   \n",
       "1  Der Gemeinde Wien soll, wenn die Sozialistisch...  Austria;1945;303;30303   \n",
       "2       Ã¼ber die Behandlung der Nationalsozialisten   Austria;1945;305;30506   \n",
       "3  sowie Ã¼ber die Verstaatlichung werden in Aussi...  Austria;1945;413;41301   \n",
       "4  Die Schaffung eines einheitlichen Dienst- und ...  Austria;1945;506;50602   \n",
       "\n",
       "   country  year cmp_short cmp_long  \n",
       "0  Austria  1945       303    30301  \n",
       "1  Austria  1945       303    30303  \n",
       "2  Austria  1945       305    30506  \n",
       "3  Austria  1945       413    41301  \n",
       "4  Austria  1945       506    50602  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalition_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped['sentiment'] = coalition_regrouped.apply(lambda x: sentiment_code_coalition(x['cmp_short'], x['cmp_long']), axis=1)\n",
    "coalition_regrouped['topic'] = coalition_regrouped['cmp_short'].apply(topic_code_coalition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>cmp_short</th>\n",
       "      <th>cmp_long</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Agriculture - Protectionism</th>\n",
       "      <th>left</th>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Economics</th>\n",
       "      <th>left</th>\n",
       "      <td>2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>3564</td>\n",
       "      <td>3564</td>\n",
       "      <td>3564</td>\n",
       "      <td>3564</td>\n",
       "      <td>3564</td>\n",
       "      <td>3564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2592</td>\n",
       "      <td>2592</td>\n",
       "      <td>2592</td>\n",
       "      <td>2592</td>\n",
       "      <td>2592</td>\n",
       "      <td>2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Education</th>\n",
       "      <th>left</th>\n",
       "      <td>1883</td>\n",
       "      <td>1883</td>\n",
       "      <td>1883</td>\n",
       "      <td>1883</td>\n",
       "      <td>1883</td>\n",
       "      <td>1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Environment - Growth</th>\n",
       "      <th>left</th>\n",
       "      <td>2045</td>\n",
       "      <td>2045</td>\n",
       "      <td>2045</td>\n",
       "      <td>2045</td>\n",
       "      <td>2045</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">European Integration</th>\n",
       "      <th>left</th>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Fabrics of Society</th>\n",
       "      <th>left</th>\n",
       "      <td>831</td>\n",
       "      <td>831</td>\n",
       "      <td>831</td>\n",
       "      <td>831</td>\n",
       "      <td>831</td>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>769</td>\n",
       "      <td>769</td>\n",
       "      <td>769</td>\n",
       "      <td>769</td>\n",
       "      <td>769</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2115</td>\n",
       "      <td>2115</td>\n",
       "      <td>2115</td>\n",
       "      <td>2115</td>\n",
       "      <td>2115</td>\n",
       "      <td>2115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Immigration</th>\n",
       "      <th>left</th>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>429</td>\n",
       "      <td>429</td>\n",
       "      <td>429</td>\n",
       "      <td>429</td>\n",
       "      <td>429</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">International Relations</th>\n",
       "      <th>left</th>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Labour and Social Welfare</th>\n",
       "      <th>left</th>\n",
       "      <td>6767</td>\n",
       "      <td>6767</td>\n",
       "      <td>6767</td>\n",
       "      <td>6767</td>\n",
       "      <td>6767</td>\n",
       "      <td>6767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Military</th>\n",
       "      <th>left</th>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>685</td>\n",
       "      <td>685</td>\n",
       "      <td>685</td>\n",
       "      <td>685</td>\n",
       "      <td>685</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>neutral</th>\n",
       "      <td>5633</td>\n",
       "      <td>5633</td>\n",
       "      <td>5633</td>\n",
       "      <td>5633</td>\n",
       "      <td>5633</td>\n",
       "      <td>5633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Political System</th>\n",
       "      <th>left</th>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>2541</td>\n",
       "      <td>2541</td>\n",
       "      <td>2541</td>\n",
       "      <td>2541</td>\n",
       "      <td>2541</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text  labels  country  year  cmp_short  \\\n",
       "topic                       sentiment                                           \n",
       "Agriculture - Protectionism left        877     877      877   877        877   \n",
       "                            right        86      86       86    86         86   \n",
       "Economics                   left       2025    2025     2025  2025       2025   \n",
       "                            neutral    3564    3564     3564  3564       3564   \n",
       "                            right      2592    2592     2592  2592       2592   \n",
       "Education                   left       1883    1883     1883  1883       1883   \n",
       "                            right        37      37       37    37         37   \n",
       "Environment - Growth        left       2045    2045     2045  2045       2045   \n",
       "                            neutral     337     337      337   337        337   \n",
       "                            right       754     754      754   754        754   \n",
       "European Integration        left        862     862      862   862        862   \n",
       "                            right        90      90       90    90         90   \n",
       "Fabrics of Society          left        831     831      831   831        831   \n",
       "                            neutral     769     769      769   769        769   \n",
       "                            right      2115    2115     2115  2115       2115   \n",
       "Immigration                 left        485     485      485   485        485   \n",
       "                            right       429     429      429   429        429   \n",
       "International Relations     left       1236    1236     1236  1236       1236   \n",
       "                            neutral     202     202      202   202        202   \n",
       "                            right        36      36       36    36         36   \n",
       "Labour and Social Welfare   left       6767    6767     6767  6767       6767   \n",
       "                            neutral      30      30       30    30         30   \n",
       "                            right       502     502      502   502        502   \n",
       "Military                    left        211     211      211   211        211   \n",
       "                            right       685     685      685   685        685   \n",
       "Other                       neutral    5633    5633     5633  5633       5633   \n",
       "Political System            left        121     121      121   121        121   \n",
       "                            neutral    2541    2541     2541  2541       2541   \n",
       "                            right      1542    1542     1542  1542       1542   \n",
       "\n",
       "                                       cmp_long  \n",
       "topic                       sentiment            \n",
       "Agriculture - Protectionism left            877  \n",
       "                            right            86  \n",
       "Economics                   left           2025  \n",
       "                            neutral        3564  \n",
       "                            right          2592  \n",
       "Education                   left           1883  \n",
       "                            right            37  \n",
       "Environment - Growth        left           2045  \n",
       "                            neutral         337  \n",
       "                            right           754  \n",
       "European Integration        left            862  \n",
       "                            right            90  \n",
       "Fabrics of Society          left            831  \n",
       "                            neutral         769  \n",
       "                            right          2115  \n",
       "Immigration                 left            485  \n",
       "                            right           429  \n",
       "International Relations     left           1236  \n",
       "                            neutral         202  \n",
       "                            right            36  \n",
       "Labour and Social Welfare   left           6767  \n",
       "                            neutral          30  \n",
       "                            right           502  \n",
       "Military                    left            211  \n",
       "                            right           685  \n",
       "Other                       neutral        5633  \n",
       "Political System            left            121  \n",
       "                            neutral        2541  \n",
       "                            right          1542  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalition_regrouped.groupby(['topic','sentiment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped.to_csv('data/temps/coalitionagree_regrouped_processed.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagree_reduced = coalition_regrouped[['sentiment', 'topic','text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698ca9f0896a4db1a98efb08faa9ddfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42292cf5cac4e39b84cc1ba5825d339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cagree_dataset = Dataset.from_pandas(cagree_reduced)\n",
    "cagree_dataset = cagree_dataset.class_encode_column('sentiment')\n",
    "cagree_dataset = cagree_dataset.class_encode_column('topic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f145a1e87d47f496e5e3601a03ad27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = cagree_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, num_topics=num_topics, num_sentiments=num_sentiments,lora=False,\n",
    "                                       use_shared_attention=True).to(device)\n",
    "\n",
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_main/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 46.61s, Estimated remaining time: 67.86s\n",
      "Elapsed time: 86.18s, Estimated remaining time: 19.65s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_ca_test = scale_func(pred_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_test['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_test['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_ca_test'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_ca_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_ca_test['res_table_sentiment'].to_csv('results/classification results/cagree_noft_sentiment.csv', index=False)\n",
    "outputs_ca_test['res_table_topic'].to_csv('results/classification results/cagree_noft_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10% supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagree_reduced.loc[:,'topic_sentiment'] = cagree_reduced.loc[:,'topic'] + '_' + cagree_reduced.loc[:,'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05ed5ea2a8540c2ba2341b7706635ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f365d222c3374332a639592a8985c4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f06b992bd7f447281e3b27515d35c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cagree_dataset = Dataset.from_pandas(cagree_reduced)\n",
    "cagree_dataset = cagree_dataset.class_encode_column('sentiment')\n",
    "cagree_dataset = cagree_dataset.class_encode_column('topic')\n",
    "cagree_dataset = cagree_dataset.class_encode_column('topic_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = cagree_dataset.train_test_split(test_size=0.9, stratify_by_column='topic_sentiment', seed=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentiment', 'topic', 'text', 'topic_sentiment'],\n",
       "        num_rows: 3928\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentiment', 'topic', 'text', 'topic_sentiment'],\n",
       "        num_rows: 35359\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cagree_datasets = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test'],\n",
    "})\n",
    "cagree_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efffc362cc6447c9b5df53b8a4126d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3928 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bd75e793664cb8b01337db410d3736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['sentiment', 'topic', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = cagree_datasets.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text','topic_sentiment'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=16, shuffle=False, collate_fn = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "num_topics = len(set(manifesto_reduced['topic']))\n",
    "num_sentiments = len(set(manifesto_reduced['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_shared_attention=True).to(device)\n",
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_main/model.safetensors')\n",
    "model.load_state_dict(loaded_tensors)\n",
    "scaling_model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) ## Recommended for LoRA. Without LoRA, can use 2e-5 instead.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion_sentiment = nn.CrossEntropyLoss()\n",
    "criterion_topic =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=1.865123, elapsed=11.59s, remaining=15.84s.\n",
      "Batch 200: loss=1.559479, elapsed=22.85s, remaining=4.34s.\n",
      "\n",
      "Training epoch took: 27.80s\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.709372, elapsed=11.39s, remaining=15.60s.\n",
      "Batch 200: loss=0.707168, elapsed=22.74s, remaining=4.33s.\n",
      "\n",
      "Training epoch took: 27.86s\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.451099, elapsed=11.34s, remaining=15.50s.\n",
      "Batch 200: loss=0.443708, elapsed=22.74s, remaining=4.32s.\n",
      "\n",
      "Training epoch took: 27.64s\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.260907, elapsed=11.32s, remaining=15.48s.\n",
      "Batch 200: loss=0.261992, elapsed=22.65s, remaining=4.27s.\n",
      "\n",
      "Training epoch took: 27.52s\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.171426, elapsed=11.35s, remaining=15.50s.\n",
      "Batch 200: loss=0.182020, elapsed=22.52s, remaining=4.27s.\n",
      "\n",
      "Training epoch took: 27.66s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    train_loop(train_dataloader, model,optimizer, scheduler, device, criterion_sentiment, criterion_topic, sentiment_var='sentiment',\n",
    "               topic_var='topic', timing_log=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/coalitionagree_ContextScalePrediction_10/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "num_topics = 12\n",
    "num_sentiments = 3\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_shared_attention=True).to(device)\n",
    "loaded_tensors = load_file('results/models/coalitionagree_ContextScalePrediction_10/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 47.84s, Estimated remaining time: 57.88s\n",
      "Elapsed time: 95.11s, Estimated remaining time: 9.99s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_ca_10 = scale_func(test_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_10['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_10['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_ca_10'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_ca_10, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_ca_10['res_table_sentiment'].to_csv('results/classification results/cagree_10ft_sentiment.csv', index=False)\n",
    "outputs_ca_10['res_table_topic'].to_csv('results/classification results/cagree_10ft_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the entire corpus with 10% training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b75f5668d04b388c82be59fefcebda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fce4c156dd46cda1e071c89c26585c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cagree_dataset = Dataset.from_pandas(cagree_reduced)\n",
    "cagree_dataset = cagree_dataset.class_encode_column('sentiment')\n",
    "cagree_dataset = cagree_dataset.class_encode_column('topic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da35b98798b411cb3c3babc7a0c968b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = cagree_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text', 'topic_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 46.85s, Estimated remaining time: 68.21s\n",
      "Elapsed time: 86.74s, Estimated remaining time: 19.78s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_ca_10_all = scale_func(pred_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_10_all['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_10_all['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_ca_10_all'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_ca_10, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped.loc[:,'position_scores'] = outputs_ca_10_all['position_scores'].flatten()\n",
    "coalition_regrouped.loc[:,'pred_sentiment_index'] = outputs_ca_10_all['pred_sentiment']\n",
    "coalition_regrouped.loc[:,'pred_sentiment'] = coalition_regrouped.pred_sentiment_index.map(name_sentiment_dict)\n",
    "coalition_regrouped.loc[:,'pred_topic_index'] = outputs_ca_10_all['pred_topics']\n",
    "coalition_regrouped.loc[:,'pred_topic'] = coalition_regrouped.pred_topic_index.map(name_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped.to_csv('data/py_outputs/cagree_10ft_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the COALITIONAGREE corpus with full labels information (for official release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped = pd.read_csv('data/temps/coalitionagree_regrouped_processed.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>cmp_short</th>\n",
       "      <th>cmp_long</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abmachungen Ã¼ber die Beamtenbesoldung,  ebenso...</td>\n",
       "      <td>Austria;1945;303;30301</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>303</td>\n",
       "      <td>30301</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Political System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Der Gemeinde Wien soll, wenn die Sozialistisch...</td>\n",
       "      <td>Austria;1945;303;30303</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>303</td>\n",
       "      <td>30303</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Political System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ã¼ber die Behandlung der Nationalsozialisten</td>\n",
       "      <td>Austria;1945;305;30506</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>305</td>\n",
       "      <td>30506</td>\n",
       "      <td>right</td>\n",
       "      <td>Political System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sowie Ã¼ber die Verstaatlichung werden in Aussi...</td>\n",
       "      <td>Austria;1945;413;41301</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>413</td>\n",
       "      <td>41301</td>\n",
       "      <td>left</td>\n",
       "      <td>Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Schaffung eines einheitlichen Dienst- und ...</td>\n",
       "      <td>Austria;1945;506;50602</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>506</td>\n",
       "      <td>50602</td>\n",
       "      <td>left</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  labels  \\\n",
       "0  Abmachungen Ã¼ber die Beamtenbesoldung,  ebenso...  Austria;1945;303;30301   \n",
       "1  Der Gemeinde Wien soll, wenn die Sozialistisch...  Austria;1945;303;30303   \n",
       "2       Ã¼ber die Behandlung der Nationalsozialisten   Austria;1945;305;30506   \n",
       "3  sowie Ã¼ber die Verstaatlichung werden in Aussi...  Austria;1945;413;41301   \n",
       "4  Die Schaffung eines einheitlichen Dienst- und ...  Austria;1945;506;50602   \n",
       "\n",
       "   country  year  cmp_short  cmp_long sentiment             topic  \n",
       "0  Austria  1945        303     30301   neutral  Political System  \n",
       "1  Austria  1945        303     30303   neutral  Political System  \n",
       "2  Austria  1945        305     30506     right  Political System  \n",
       "3  Austria  1945        413     41301      left         Economics  \n",
       "4  Austria  1945        506     50602      left         Education  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalition_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagree_reduced = coalition_regrouped[['sentiment', 'topic','text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de21075e67944c1866fe80dae298b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade7145a922045a9bb35886be939ec99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cagree_dataset = Dataset.from_pandas(cagree_reduced)\n",
    "cagree_dataset = cagree_dataset.class_encode_column('sentiment')\n",
    "cagree_dataset = cagree_dataset.class_encode_column('topic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "num_topics = 12\n",
    "num_sentiments = 3\n",
    "model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_shared_attention=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5510316532e04cba85cbf016fa380277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['sentiment', 'topic', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = cagree_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text'])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "pred_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False, collate_fn = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) ## Recommended for LoRA. Without LoRA, can use 2e-5 instead.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion =  nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.708248, elapsed=10.72s, remaining=242.13s.\n",
      "Batch 200: loss=3.548744, elapsed=21.19s, remaining=229.93s.\n",
      "Batch 300: loss=3.412370, elapsed=31.74s, remaining=219.61s.\n",
      "Batch 400: loss=3.186755, elapsed=42.29s, remaining=209.18s.\n",
      "Batch 500: loss=2.949784, elapsed=52.86s, remaining=198.77s.\n",
      "Batch 600: loss=2.755245, elapsed=63.60s, remaining=188.83s.\n",
      "Batch 700: loss=2.596298, elapsed=74.13s, remaining=178.11s.\n",
      "Batch 800: loss=2.457945, elapsed=84.63s, remaining=167.41s.\n",
      "Batch 900: loss=2.353059, elapsed=95.08s, remaining=156.65s.\n",
      "Batch 1000: loss=2.258474, elapsed=105.55s, remaining=145.97s.\n",
      "Batch 1100: loss=2.185417, elapsed=115.92s, remaining=135.24s.\n",
      "Batch 1200: loss=2.109768, elapsed=126.59s, remaining=124.86s.\n",
      "Batch 1300: loss=2.049764, elapsed=136.97s, remaining=114.18s.\n",
      "Batch 1400: loss=1.994587, elapsed=147.62s, remaining=103.74s.\n",
      "Batch 1500: loss=1.950611, elapsed=158.01s, remaining=93.11s.\n",
      "Batch 1600: loss=1.900779, elapsed=168.63s, remaining=82.64s.\n",
      "Batch 1700: loss=1.862804, elapsed=179.18s, remaining=72.13s.\n",
      "Batch 1800: loss=1.824948, elapsed=189.64s, remaining=61.58s.\n",
      "Batch 1900: loss=1.791327, elapsed=200.33s, remaining=51.09s.\n",
      "Batch 2000: loss=1.759663, elapsed=210.83s, remaining=40.55s.\n",
      "Batch 2100: loss=1.732791, elapsed=221.27s, remaining=30.00s.\n",
      "Batch 2200: loss=1.708807, elapsed=231.89s, remaining=19.48s.\n",
      "Batch 2300: loss=1.686355, elapsed=242.61s, remaining=8.96s.\n",
      "Batch 2400: loss=1.664779, elapsed=253.26s, remaining=-1.58s.\n",
      "\n",
      "Training epoch took: 258.98s\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.975112, elapsed=10.66s, remaining=240.86s.\n",
      "Batch 200: loss=0.963805, elapsed=21.03s, remaining=228.46s.\n",
      "Batch 300: loss=0.936781, elapsed=31.71s, remaining=219.68s.\n",
      "Batch 400: loss=0.935890, elapsed=42.25s, remaining=209.15s.\n",
      "Batch 500: loss=0.940536, elapsed=52.70s, remaining=198.30s.\n",
      "Batch 600: loss=0.939546, elapsed=63.24s, remaining=187.86s.\n",
      "Batch 700: loss=0.931244, elapsed=74.09s, remaining=178.14s.\n",
      "Batch 800: loss=0.921175, elapsed=84.59s, remaining=167.37s.\n",
      "Batch 900: loss=0.916430, elapsed=94.96s, remaining=156.41s.\n",
      "Batch 1000: loss=0.910164, elapsed=105.29s, remaining=145.58s.\n",
      "Batch 1100: loss=0.910724, elapsed=115.32s, remaining=134.48s.\n",
      "Batch 1200: loss=0.911224, elapsed=125.43s, remaining=123.64s.\n",
      "Batch 1300: loss=0.911151, elapsed=135.69s, remaining=113.04s.\n",
      "Batch 1400: loss=0.906042, elapsed=145.91s, remaining=102.45s.\n",
      "Batch 1500: loss=0.903605, elapsed=156.41s, remaining=92.05s.\n",
      "Batch 1600: loss=0.900725, elapsed=166.63s, remaining=81.50s.\n",
      "Batch 1700: loss=0.898177, elapsed=176.83s, remaining=71.02s.\n",
      "Batch 1800: loss=0.891379, elapsed=187.14s, remaining=60.60s.\n",
      "Batch 1900: loss=0.886063, elapsed=197.37s, remaining=50.17s.\n",
      "Batch 2000: loss=0.886532, elapsed=207.46s, remaining=39.73s.\n",
      "Batch 2100: loss=0.881225, elapsed=217.48s, remaining=29.31s.\n",
      "Batch 2200: loss=0.877703, elapsed=227.45s, remaining=18.93s.\n",
      "Batch 2300: loss=0.874398, elapsed=238.39s, remaining=8.56s.\n",
      "Batch 2400: loss=0.871151, elapsed=248.55s, remaining=-1.79s.\n",
      "\n",
      "Training epoch took: 254.07s\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.639530, elapsed=10.27s, remaining=232.07s.\n",
      "Batch 200: loss=0.619525, elapsed=20.71s, remaining=224.86s.\n",
      "Batch 300: loss=0.603809, elapsed=30.83s, remaining=213.40s.\n",
      "Batch 400: loss=0.603417, elapsed=41.00s, remaining=202.78s.\n",
      "Batch 500: loss=0.599052, elapsed=51.29s, remaining=192.80s.\n",
      "Batch 600: loss=0.594025, elapsed=61.57s, remaining=182.73s.\n",
      "Batch 700: loss=0.599615, elapsed=71.66s, remaining=172.07s.\n",
      "Batch 800: loss=0.597656, elapsed=81.75s, remaining=161.57s.\n",
      "Batch 900: loss=0.602980, elapsed=91.92s, remaining=151.33s.\n",
      "Batch 1000: loss=0.600712, elapsed=102.11s, remaining=141.11s.\n",
      "Batch 1100: loss=0.600005, elapsed=112.30s, remaining=130.89s.\n",
      "Batch 1200: loss=0.596664, elapsed=122.53s, remaining=120.72s.\n",
      "Batch 1300: loss=0.596252, elapsed=132.84s, remaining=110.63s.\n",
      "Batch 1400: loss=0.595098, elapsed=142.99s, remaining=100.39s.\n",
      "Batch 1500: loss=0.593916, elapsed=153.40s, remaining=90.31s.\n",
      "Batch 1600: loss=0.595446, elapsed=163.58s, remaining=80.06s.\n",
      "Batch 1700: loss=0.593401, elapsed=173.49s, remaining=69.72s.\n",
      "Batch 1800: loss=0.591295, elapsed=183.73s, remaining=59.54s.\n",
      "Batch 1900: loss=0.589778, elapsed=193.81s, remaining=49.29s.\n",
      "Batch 2000: loss=0.587668, elapsed=204.02s, remaining=39.09s.\n",
      "Batch 2100: loss=0.586601, elapsed=214.09s, remaining=28.89s.\n",
      "Batch 2200: loss=0.588427, elapsed=224.22s, remaining=18.69s.\n",
      "Batch 2300: loss=0.586040, elapsed=234.37s, remaining=8.50s.\n",
      "Batch 2400: loss=0.584084, elapsed=244.63s, remaining=-1.69s.\n",
      "\n",
      "Training epoch took: 250.05s\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.390809, elapsed=10.27s, remaining=232.31s.\n",
      "Batch 200: loss=0.402988, elapsed=20.40s, remaining=221.62s.\n",
      "Batch 300: loss=0.412758, elapsed=30.59s, remaining=211.68s.\n",
      "Batch 400: loss=0.413509, elapsed=40.80s, remaining=201.74s.\n",
      "Batch 500: loss=0.410744, elapsed=51.06s, remaining=191.89s.\n",
      "Batch 600: loss=0.412953, elapsed=61.29s, remaining=181.81s.\n",
      "Batch 700: loss=0.405539, elapsed=71.40s, remaining=171.42s.\n",
      "Batch 800: loss=0.407403, elapsed=81.67s, remaining=161.40s.\n",
      "Batch 900: loss=0.407386, elapsed=91.81s, remaining=151.10s.\n",
      "Batch 1000: loss=0.402594, elapsed=102.12s, remaining=141.09s.\n",
      "Batch 1100: loss=0.399346, elapsed=112.45s, remaining=131.04s.\n",
      "Batch 1200: loss=0.396007, elapsed=122.61s, remaining=120.79s.\n",
      "Batch 1300: loss=0.398715, elapsed=132.72s, remaining=110.50s.\n",
      "Batch 1400: loss=0.398383, elapsed=142.92s, remaining=100.29s.\n",
      "Batch 1500: loss=0.400984, elapsed=152.84s, remaining=89.92s.\n",
      "Batch 1600: loss=0.398660, elapsed=163.08s, remaining=79.78s.\n",
      "Batch 1700: loss=0.398500, elapsed=173.22s, remaining=69.58s.\n",
      "Batch 1800: loss=0.397604, elapsed=183.58s, remaining=59.44s.\n",
      "Batch 1900: loss=0.396849, elapsed=193.94s, remaining=49.30s.\n",
      "Batch 2000: loss=0.396712, elapsed=203.94s, remaining=39.06s.\n",
      "Batch 2100: loss=0.395486, elapsed=214.33s, remaining=28.90s.\n",
      "Batch 2200: loss=0.394876, elapsed=224.63s, remaining=18.71s.\n",
      "Batch 2300: loss=0.393808, elapsed=234.70s, remaining=8.49s.\n",
      "Batch 2400: loss=0.394330, elapsed=244.91s, remaining=-1.71s.\n",
      "\n",
      "Training epoch took: 250.31s\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.280032, elapsed=10.06s, remaining=227.13s.\n",
      "Batch 200: loss=0.284617, elapsed=20.34s, remaining=220.69s.\n",
      "Batch 300: loss=0.284351, elapsed=30.58s, remaining=211.53s.\n",
      "Batch 400: loss=0.284568, elapsed=40.84s, remaining=201.93s.\n",
      "Batch 500: loss=0.283030, elapsed=50.99s, remaining=191.60s.\n",
      "Batch 600: loss=0.284705, elapsed=61.09s, remaining=181.20s.\n",
      "Batch 700: loss=0.284981, elapsed=71.23s, remaining=170.95s.\n",
      "Batch 800: loss=0.279731, elapsed=81.43s, remaining=160.89s.\n",
      "Batch 900: loss=0.277751, elapsed=91.67s, remaining=150.86s.\n",
      "Batch 1000: loss=0.280764, elapsed=101.87s, remaining=140.73s.\n",
      "Batch 1100: loss=0.282785, elapsed=111.97s, remaining=130.46s.\n",
      "Batch 1200: loss=0.283976, elapsed=122.02s, remaining=120.18s.\n",
      "Batch 1300: loss=0.281014, elapsed=132.30s, remaining=110.13s.\n",
      "Batch 1400: loss=0.281301, elapsed=142.54s, remaining=100.00s.\n",
      "Batch 1500: loss=0.280818, elapsed=152.80s, remaining=89.88s.\n",
      "Batch 1600: loss=0.277453, elapsed=163.06s, remaining=79.74s.\n",
      "Batch 1700: loss=0.279114, elapsed=173.52s, remaining=69.68s.\n",
      "Batch 1800: loss=0.279130, elapsed=183.60s, remaining=59.45s.\n",
      "Batch 1900: loss=0.280060, elapsed=193.63s, remaining=49.22s.\n",
      "Batch 2000: loss=0.280178, elapsed=203.90s, remaining=39.06s.\n",
      "Batch 2100: loss=0.278721, elapsed=214.10s, remaining=28.87s.\n",
      "Batch 2200: loss=0.277451, elapsed=224.22s, remaining=18.66s.\n",
      "Batch 2300: loss=0.274867, elapsed=234.30s, remaining=8.47s.\n",
      "Batch 2400: loss=0.272221, elapsed=244.45s, remaining=-1.71s.\n",
      "\n",
      "Training epoch took: 249.95s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    train_loop(train_dataloader, model,optimizer, scheduler, device, criterion, criterion, sentiment_var='sentiment',\n",
    "               topic_var='topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/coalitionagree_ContextScalePrediction_full/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "num_topics = 12\n",
    "num_sentiments = 3\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, \n",
    "                               num_topics=num_topics, \n",
    "                               num_sentiments=num_sentiments,\n",
    "                               lora=False,\n",
    "                               use_shared_attention=True).to(device)\n",
    "loaded_tensors = load_file('results/models/coalitionagree_ContextScalePrediction_full/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 41.76s, Estimated remaining time: 60.81s\n",
      "Elapsed time: 77.46s, Estimated remaining time: 17.66s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_ca_all = scale_func(pred_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_all['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_ca_all['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_ca_all'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_ca_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_ca_all['res_table_sentiment'].to_csv('results/classification results/cagree_all_sentiment.csv', index=False)\n",
    "outputs_ca_all['res_table_topic'].to_csv('results/classification results/cagree_all_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped.loc[:,'position_scores'] = outputs_ca_all['position_scores'].flatten()\n",
    "coalition_regrouped.loc[:,'pred_sentiment_index'] = outputs_ca_all['pred_sentiment']\n",
    "coalition_regrouped.loc[:,'pred_sentiment'] = coalition_regrouped.pred_sentiment_index.map(name_sentiment_dict)\n",
    "coalition_regrouped.loc[:,'pred_topic_index'] = outputs_ca_all['pred_topics']\n",
    "coalition_regrouped.loc[:,'pred_topic'] = coalition_regrouped.pred_topic_index.map(name_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped.to_csv('data/py_outputs/cagree_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting to twitter data (Sentiment is not Stance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptation training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_trump = pd.read_csv('data/MOTN/MOTN_responses_groundtruth.csv', encoding='utf-8')\n",
    "tw_kav = pd.read_csv('data/MOTN/kavanaugh_tweets_groundtruth.csv', encoding='utf-8')\n",
    "tw_wm = pd.read_csv('data/MOTN/WM_tweets_groundtruth.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wavenum</th>\n",
       "      <th>ideo5</th>\n",
       "      <th>edits_clean_text</th>\n",
       "      <th>qpos</th>\n",
       "      <th>trump_stance_auto</th>\n",
       "      <th>lexicoder_sentiment</th>\n",
       "      <th>fold</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>SVM_sentiment</th>\n",
       "      <th>BERT_sentiment</th>\n",
       "      <th>SVM_stance</th>\n",
       "      <th>BERT_stance</th>\n",
       "      <th>vader_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>the recent election of donald trump the freedo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Very conservative</td>\n",
       "      <td>donald trump won</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>that donald trump beat hillary clinton</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>donald trump was elected president</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>the american people saw through the obfuscatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wavenum              ideo5  \\\n",
       "0        3           Moderate   \n",
       "1        3  Very conservative   \n",
       "2        3       Conservative   \n",
       "3        3       Conservative   \n",
       "4        3       Conservative   \n",
       "\n",
       "                                    edits_clean_text  qpos  trump_stance_auto  \\\n",
       "0  the recent election of donald trump the freedo...     1                  1   \n",
       "1                                   donald trump won     1                  1   \n",
       "2             that donald trump beat hillary clinton     1                  1   \n",
       "3                 donald trump was elected president     1                  1   \n",
       "4  the american people saw through the obfuscatio...     1                  1   \n",
       "\n",
       "   lexicoder_sentiment  fold  vader_sentiment  SVM_sentiment  BERT_sentiment  \\\n",
       "0                  1.0     3              1.0              1               1   \n",
       "1                  1.0     1              1.0              1               1   \n",
       "2                  NaN     5              NaN              1               1   \n",
       "3                  NaN     5              NaN              1               1   \n",
       "4                  1.0     3              1.0              0               1   \n",
       "\n",
       "   SVM_stance  BERT_stance  vader_scores  \n",
       "0           1            0        0.6369  \n",
       "1           1            1        0.5719  \n",
       "2           1            1        0.0000  \n",
       "3           1            1        0.0000  \n",
       "4           1            1        0.4019  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "      <th>fold</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>SVM_sentiment</th>\n",
       "      <th>BERT_sentiment</th>\n",
       "      <th>SVM_stance</th>\n",
       "      <th>BERT_stance</th>\n",
       "      <th>lexicoder_sentiment</th>\n",
       "      <th>vader_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @willchamberlain Ms. Ford sent an anonymou...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.7579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @dbongino Is there ever going to come a da...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @SuzeOrmanShow He violates every one of my...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @funder Dear Judge Kavanaugh-  We request ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @BrianKarem BREAKING: Montgomery MD  PD Ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  stance  fold  \\\n",
       "0   RT @willchamberlain Ms. Ford sent an anonymou...          0       1     3   \n",
       "1   RT @dbongino Is there ever going to come a da...          0       1     1   \n",
       "2   RT @SuzeOrmanShow He violates every one of my...          0       0     5   \n",
       "3   RT @funder Dear Judge Kavanaugh-  We request ...          0       0     5   \n",
       "4   RT @BrianKarem BREAKING: Montgomery MD  PD Ch...          0       0     3   \n",
       "\n",
       "   vader_sentiment  SVM_sentiment  BERT_sentiment  SVM_stance  BERT_stance  \\\n",
       "0              0.0              0               0           1            1   \n",
       "1              0.0              0               0           1            1   \n",
       "2              1.0              0               0           0            0   \n",
       "3              0.0              0               0           0            0   \n",
       "4              0.0              0               0           0            0   \n",
       "\n",
       "   lexicoder_sentiment  vader_scores  \n",
       "0                  0.0       -0.7579  \n",
       "1                  0.0       -0.4767  \n",
       "2                  0.0        0.5423  \n",
       "3                  0.0       -0.8020  \n",
       "4                  0.0       -0.2960  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_kav.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stance</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>balanced_train</th>\n",
       "      <th>vader_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YES! I'm still with her and always will be. ht...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pics or it didn't happen. https://t.co/o1GddSmwk2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love this nasty woman. @MaribethMonroe #wome...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YiawayYeh: Marching for love.  Nashville #...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These people are just Sad. https://t.co/0LK6iG...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stance  sentiment  \\\n",
       "0  YES! I'm still with her and always will be. ht...       1        1.0   \n",
       "1  Pics or it didn't happen. https://t.co/o1GddSmwk2       1        0.0   \n",
       "2  I love this nasty woman. @MaribethMonroe #wome...       1        1.0   \n",
       "3  RT @YiawayYeh: Marching for love.  Nashville #...       1        1.0   \n",
       "4  These people are just Sad. https://t.co/0LK6iG...       0        0.0   \n",
       "\n",
       "   balanced_train  vader_scores  \n",
       "0             0.0        0.5754  \n",
       "1             0.0        0.0000  \n",
       "2             1.0       -0.0129  \n",
       "3             1.0        0.6369  \n",
       "4             1.0       -0.4767  "
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_wm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_trump = tw_trump[['edits_clean_text','trump_stance_auto']].copy()\n",
    "tw_trump = tw_trump.rename(columns={'edits_clean_text': 'text', 'trump_stance_auto': 'stance'})\n",
    "tw_trump['topic'] = 'trump'\n",
    "tw_kav = tw_kav[['text', 'stance']].copy()\n",
    "tw_kav['topic'] = 'kavanaugh'\n",
    "tw_wm = tw_wm[['text','stance']].copy()\n",
    "tw_wm['topic'] = 'women march'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df = pd.concat([tw_trump, tw_kav,tw_wm]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df.loc[:,'lr'] = tw_df.apply(lambda x: recode_tw(x['topic'], x['stance']), axis=1)\n",
    "tw_df.loc[:,'topic_lr'] = tw_df['topic'] + '_' + tw_df['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stance</th>\n",
       "      <th>topic_lr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th>lr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">kavanaugh</th>\n",
       "      <th>left</th>\n",
       "      <td>1672</td>\n",
       "      <td>1672</td>\n",
       "      <td>1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1988</td>\n",
       "      <td>1988</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">trump</th>\n",
       "      <th>left</th>\n",
       "      <td>4312</td>\n",
       "      <td>4312</td>\n",
       "      <td>4312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2834</td>\n",
       "      <td>2834</td>\n",
       "      <td>2834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">women march</th>\n",
       "      <th>left</th>\n",
       "      <td>16965</td>\n",
       "      <td>16965</td>\n",
       "      <td>16965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2647</td>\n",
       "      <td>2647</td>\n",
       "      <td>2647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  stance  topic_lr\n",
       "topic       lr                            \n",
       "kavanaugh   left    1672    1672      1672\n",
       "            right   1988    1988      1988\n",
       "trump       left    4312    4312      4312\n",
       "            right   2834    2834      2834\n",
       "women march left   16965   16965     16965\n",
       "            right   2647    2647      2647"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_df.groupby(['topic','lr']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30418 entries, 0 to 30417\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      30418 non-null  object\n",
      " 1   stance    30418 non-null  int64 \n",
      " 2   topic     30418 non-null  object\n",
      " 3   lr        30418 non-null  object\n",
      " 4   topic_lr  30418 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "tw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baf03cb8694415b9011e2c2ea89b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/30418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff15ce85f26434cb95462bb92ad5c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/30418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b42ec4ddff640e19f01ad9ce7cfc087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/30418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tw_dataset = Dataset.from_pandas(tw_df[['text','lr','topic', 'topic_lr']].copy())\n",
    "tw_dataset = tw_dataset.class_encode_column('lr')\n",
    "tw_dataset = tw_dataset.class_encode_column('topic')\n",
    "tw_dataset = tw_dataset.class_encode_column('topic_lr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = tw_dataset.train_test_split(test_size=0.9, stratify_by_column='topic_lr',seed=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'lr', 'topic', 'topic_lr'],\n",
       "        num_rows: 3041\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'lr', 'topic', 'topic_lr'],\n",
       "        num_rows: 27377\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_datasets = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test'],\n",
    "})\n",
    "tw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0156764c640a98dc0069d63b666a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10392ded229a464a8567ef0fd5b77558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['lr', 'topic', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tw_datasets.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length':512}, \n",
    "                                            remove_columns=['text', 'topic_lr'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=16, shuffle=False, collate_fn = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained models\n",
    "source_model = ContextScalePrediction(roberta_model=model_name, num_topics=12, num_sentiments=3,lora=False, use_shared_attention=True).to(device)\n",
    "loaded_tensors = load_file('results/models/manifesto_ContextScalePrediction_main/model.safetensors')\n",
    "source_model.load_state_dict(loaded_tensors)\n",
    "model=None\n",
    "scaling_model=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = ContextScalePrediction(roberta_model=model_name, num_topics=3, num_sentiments=2,lora=False, use_shared_attention=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextScalePrediction(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): XLMRobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (intermediate): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (intermediate_topic): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (intermediate_sentiment): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (shared_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "  )\n",
       "  (topic): Linear(in_features=384, out_features=3, bias=True)\n",
       "  (sentiment): Linear(in_features=384, out_features=2, bias=True)\n",
       "  (attention_regularizer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture1 = get_architecture_details(target_model)\n",
    "architecture2 = get_architecture_details(source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models have the same number of layers/modules.\n",
      "Difference found in layer topic:\n",
      "Model 1: {'name': 'topic', 'type': 'Linear', 'params': 1155}\n",
      "Model 2: {'name': 'topic', 'type': 'Linear', 'params': 4620}\n",
      "Difference found in layer sentiment:\n",
      "Model 1: {'name': 'sentiment', 'type': 'Linear', 'params': 770}\n",
      "Model 2: {'name': 'sentiment', 'type': 'Linear', 'params': 1155}\n"
     ]
    }
   ],
   "source": [
    "compare_architectures(architecture1, architecture2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping topic.weight as it is not present or should be skipped in the scaling model.\n",
      "Skipping topic.bias as it is not present or should be skipped in the scaling model.\n",
      "Skipping sentiment.weight as it is not present or should be skipped in the scaling model.\n",
      "Skipping sentiment.bias as it is not present or should be skipped in the scaling model.\n",
      "Trainable Parameters after copying:\n",
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "roberta.pooler.dense.weight\n",
      "roberta.pooler.dense.bias\n",
      "intermediate.0.weight\n",
      "intermediate.0.bias\n",
      "intermediate.1.weight\n",
      "intermediate.1.bias\n",
      "intermediate_topic.0.weight\n",
      "intermediate_topic.0.bias\n",
      "intermediate_topic.1.weight\n",
      "intermediate_topic.1.bias\n",
      "intermediate_sentiment.0.weight\n",
      "intermediate_sentiment.0.bias\n",
      "intermediate_sentiment.1.weight\n",
      "intermediate_sentiment.1.bias\n",
      "shared_attention.in_proj_weight\n",
      "shared_attention.in_proj_bias\n",
      "shared_attention.out_proj.weight\n",
      "shared_attention.out_proj.bias\n",
      "topic.weight\n",
      "topic.bias\n",
      "sentiment.weight\n",
      "sentiment.bias\n",
      "attention_regularizer.weight\n",
      "attention_regularizer.bias\n"
     ]
    }
   ],
   "source": [
    "copy_weights(source_model, target_model, patterns=('topic','sentiment'), freeze_copied=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters copied successfully.\n"
     ]
    }
   ],
   "source": [
    "check_weights_similar(source_model, target_model, patterns=('topic','sentiment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(target_model.parameters(), lr=2e-5) ## Recommended for LoRA. Without LoRA, can use 2e-5 instead.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nullify existing models (if any)\n",
    "scaling_model=None\n",
    "source_model=None\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=1.029016, elapsed=9.27s, remaining=7.67s.\n",
      "\n",
      "Training epoch took: 17.41s\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.486419 \n",
      "\n",
      "Accuracy - Sentiment: 76.9%, Avg loss: 0.486419 \n",
      "\n",
      "Accuracy - Topic: 99.6%, Avg loss: 0.486419 \n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.462686, elapsed=8.50s, remaining=7.07s.\n",
      "\n",
      "Training epoch took: 16.24s\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.451708 \n",
      "\n",
      "Accuracy - Sentiment: 82.0%, Avg loss: 0.451708 \n",
      "\n",
      "Accuracy - Topic: 99.3%, Avg loss: 0.451708 \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.319896, elapsed=8.55s, remaining=7.12s.\n",
      "\n",
      "Training epoch took: 16.18s\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.400162 \n",
      "\n",
      "Accuracy - Sentiment: 83.9%, Avg loss: 0.400162 \n",
      "\n",
      "Accuracy - Topic: 98.9%, Avg loss: 0.400162 \n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.214013, elapsed=8.46s, remaining=7.05s.\n",
      "\n",
      "Training epoch took: 16.17s\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.445855 \n",
      "\n",
      "Accuracy - Sentiment: 84.2%, Avg loss: 0.445855 \n",
      "\n",
      "Accuracy - Topic: 99.5%, Avg loss: 0.445855 \n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.145229, elapsed=8.65s, remaining=7.19s.\n",
      "\n",
      "Training epoch took: 16.12s\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.490335 \n",
      "\n",
      "Accuracy - Sentiment: 83.9%, Avg loss: 0.490335 \n",
      "\n",
      "Accuracy - Topic: 99.6%, Avg loss: 0.490335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    train_loop(train_dataloader, target_model,optimizer, scheduler, device, criterion_sent=criterion, criterion_topic=criterion, sentiment_var='lr', topic_var='topic', timing_log=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = target_model.state_dict()\n",
    "save_file(state_dict, 'results/models/tw_ContextScalePrediction/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Elapsed time: 20.07s, Estimated remaining time: 14.29s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_tw_10 = scale_func(test_dataloader, \n",
    "               target_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='lr', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tw_10['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tw_10['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_tw_10'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_tw_10, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_tw_10['res_table_sentiment'].to_csv('results/classification results/tw_10ft_sentiment.csv', index=False)\n",
    "outputs_tw_10['res_table_topic'].to_csv('results/classification results/tw_10ft_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load pre-trained models\n",
    "target_model = ContextScalePrediction(roberta_model=model_name, num_topics=3, num_sentiments=2,lora=False, use_shared_attention=True).to(device)\n",
    "loaded_tensors = load_file('results/models/tw_ContextScalePrediction/model.safetensors')\n",
    "target_model.load_state_dict(loaded_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eae52ac38294530a6083f8ac514e712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = tw_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text','topic_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 18.78s, Estimated remaining time: 16.94s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "outputs_tw_10_all = scale_func(pred_dataloader, \n",
    "               target_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='lr', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tw_10_all['res_table_sentiment']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tw_10_all['res_table_topic']['f1'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/outputs_tw_10_all'\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(outputs_tw_10_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_names = tw_dataset.features['lr'].names\n",
    "name_sentiment_dict = dict([(x,y) for x,y in enumerate(list_names)])\n",
    "list_names = tw_dataset.features['topic'].names\n",
    "name_topic_dict = dict([(x,y) for x,y in enumerate(list_names)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df.loc[:,'position_scores'] = outputs_tw_10_all['position_scores'].flatten()\n",
    "tw_df.loc[:,'pred_sentiment_index'] = outputs_tw_10_all['pred_sentiment']\n",
    "tw_df.loc[:,'pred_sentiment'] = tw_df.pred_sentiment_index.map(name_sentiment_dict)\n",
    "tw_df.loc[:,'pred_topic_index'] = outputs_tw_10_all['pred_topics']\n",
    "tw_df.loc[:,'pred_topic'] = tw_df.pred_topic_index.map(name_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df.to_csv('data/py_outputs/tw_10_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_d2v = pd.read_csv('data/temps/manifesto.csv', encoding='utf-8', dtype={'cmp_code':'str', 'is_copy_of':'str'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the 0th sentence\n",
      "Cleaning the 10000th sentence\n",
      "Cleaning the 20000th sentence\n",
      "Cleaning the 30000th sentence\n",
      "Cleaning the 40000th sentence\n",
      "Cleaning the 50000th sentence\n",
      "Cleaning the 60000th sentence\n",
      "Cleaning the 70000th sentence\n",
      "Cleaning the 80000th sentence\n",
      "Cleaning the 90000th sentence\n",
      "Cleaning the 100000th sentence\n",
      "Cleaning the 110000th sentence\n",
      "Cleaning the 120000th sentence\n",
      "Cleaning the 130000th sentence\n",
      "Cleaning the 140000th sentence\n",
      "Cleaning the 150000th sentence\n",
      "Cleaning the 160000th sentence\n",
      "Cleaning the 170000th sentence\n",
      "Cleaning the 180000th sentence\n",
      "Cleaning the 190000th sentence\n",
      "Cleaning the 200000th sentence\n",
      "Cleaning the 210000th sentence\n",
      "Cleaning the 220000th sentence\n",
      "Cleaning the 230000th sentence\n",
      "Cleaning the 240000th sentence\n",
      "Cleaning the 250000th sentence\n",
      "Cleaning the 260000th sentence\n",
      "Cleaning the 270000th sentence\n",
      "Cleaning the 280000th sentence\n",
      "Cleaning the 290000th sentence\n",
      "Cleaning the 300000th sentence\n",
      "Cleaning the 310000th sentence\n",
      "Cleaning the 320000th sentence\n",
      "Cleaning the 330000th sentence\n",
      "Cleaning the 340000th sentence\n",
      "Cleaning the 350000th sentence\n",
      "Cleaning the 360000th sentence\n",
      "Cleaning the 370000th sentence\n",
      "Cleaning the 380000th sentence\n",
      "Cleaning the 390000th sentence\n",
      "Cleaning the 400000th sentence\n",
      "Cleaning the 410000th sentence\n",
      "Cleaning the 420000th sentence\n",
      "Cleaning the 430000th sentence\n",
      "Cleaning the 440000th sentence\n",
      "Cleaning the 450000th sentence\n",
      "Cleaning the 460000th sentence\n",
      "Cleaning the 470000th sentence\n",
      "Cleaning the 480000th sentence\n",
      "Cleaning the 490000th sentence\n",
      "Cleaning the 500000th sentence\n",
      "Cleaning the 510000th sentence\n",
      "Cleaning the 520000th sentence\n",
      "Cleaning the 530000th sentence\n",
      "Cleaning the 540000th sentence\n",
      "Cleaning the 550000th sentence\n",
      "Cleaning the 560000th sentence\n",
      "Cleaning the 570000th sentence\n",
      "Cleaning the 580000th sentence\n",
      "Cleaning the 590000th sentence\n",
      "Cleaning the 600000th sentence\n",
      "Cleaning the 610000th sentence\n",
      "Cleaning the 620000th sentence\n",
      "Cleaning the 630000th sentence\n",
      "Cleaning the 640000th sentence\n",
      "Cleaning the 650000th sentence\n",
      "Cleaning the 660000th sentence\n",
      "Cleaning the 670000th sentence\n",
      "Cleaning the 680000th sentence\n",
      "Cleaning the 690000th sentence\n",
      "Cleaning the 700000th sentence\n",
      "Cleaning the 710000th sentence\n",
      "Cleaning the 720000th sentence\n",
      "Cleaning the 730000th sentence\n",
      "Cleaning the 740000th sentence\n",
      "Cleaning the 750000th sentence\n",
      "Cleaning the 760000th sentence\n",
      "Cleaning the 770000th sentence\n",
      "Cleaning the 780000th sentence\n",
      "Cleaning the 790000th sentence\n",
      "Cleaning the 800000th sentence\n",
      "Cleaning the 810000th sentence\n",
      "Cleaning the 820000th sentence\n",
      "Cleaning the 830000th sentence\n",
      "Cleaning the 840000th sentence\n",
      "Cleaning the 850000th sentence\n",
      "Cleaning the 860000th sentence\n",
      "Cleaning the 870000th sentence\n",
      "Cleaning the 880000th sentence\n"
     ]
    }
   ],
   "source": [
    "outputs = clean_text_loop(manifesto_d2v, 'countryname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_d2v.loc[:,'text_cleaned'] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_d2v.loc[:,'party_election'] = manifesto_d2v.party.astype(str).str.cat(manifesto_d2v[['election']].astype(str).values, sep='_')\n",
    "manifesto_d2v.loc[:,'country_party_election'] = manifesto_d2v.countryname.str.cat(manifesto_d2v[['party','election']].astype(str).values, sep='_')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec scaling - original approach by R&C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: Sweden\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Norway\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Denmark\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Finland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Iceland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Belgium\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Netherlands\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: France\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Italy\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Spain\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Greece\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Portugal\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Germany\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Austria\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Switzerland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: United Kingdom\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Ireland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   party_election  731 non-null    object \n",
      " 1   d2v_d1          731 non-null    float32\n",
      " 2   d2v_d2          731 non-null    float32\n",
      " 3   party           731 non-null    object \n",
      " 4   election        731 non-null    object \n",
      " 5   country         731 non-null    object \n",
      "dtypes: float32(2), object(4)\n",
      "memory usage: 28.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the country-level dataframes\n",
    "country_dfs = []\n",
    "\n",
    "# Get the unique list of countries from your data\n",
    "unique_countries = manifesto_d2v['countryname'].unique()\n",
    "\n",
    "# Loop through each country and process separately\n",
    "for country in unique_countries:\n",
    "    print(f\"Processing country: {country}\")\n",
    "    \n",
    "    # Filter the dataset for the current country\n",
    "    country_data = manifesto_d2v[manifesto_d2v['countryname'] == country]\n",
    "    \n",
    "    # Build the corpus iterator for this country's data\n",
    "    outputs_stream = phraseIterator(country_data, 'text_cleaned')\n",
    "    bigram = Phraser(Phrases(outputs_stream, min_count=1, threshold=5))\n",
    "    trigram = Phrases(bigram[outputs_stream], min_count=1, threshold=5)\n",
    "    \n",
    "    # Create the Doc2Vec model and build vocabulary\n",
    "    model = Doc2Vec(vector_size=500, window=6, min_count=1, workers=16, epochs=20, seed=seed_val)\n",
    "    model.build_vocab(corpusIterator(country_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'))\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(corpusIterator(country_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'),\n",
    "                total_examples=model.corpus_count, epochs=20)\n",
    "    \n",
    "    # Generate embeddings and apply dimensionality reduction\n",
    "    embed_dict = d2v_reduct(model)\n",
    "    df_d2v = pd.DataFrame.from_dict(embed_dict).transpose()\n",
    "    df_d2v.index.name = 'party_election'\n",
    "    df_d2v.reset_index(inplace=True)\n",
    "    pca = PCA(n_components=2, random_state=seed_val)\n",
    "    df_d2v[['d2v_d1', 'd2v_d2']] = pca.fit_transform(df_d2v.iloc[:, 1:])\n",
    "    df_d2v = df_d2v[['party_election', 'd2v_d1', 'd2v_d2']]\n",
    "    \n",
    "    # Split the 'party_election' label into separate columns\n",
    "    df_d2v[['party', 'election']] = df_d2v['party_election'].str.split('_', expand=True)\n",
    "    df_d2v.loc[:, 'election'] = df_d2v['election'].astype(int)\n",
    "    df_d2v['country'] = country  # Add country column for merging later\n",
    "    \n",
    "    # Append the country-level dataframe to the list\n",
    "    country_dfs.append(df_d2v)\n",
    "\n",
    "# Merge all country-level datasets into a single dataframe\n",
    "final_df_d2v = pd.concat(country_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final dataframe to a CSV file\n",
    "final_df_d2v.to_csv('data/py_outputs/r&c_gen_party_election.csv', index=False)\n",
    "\n",
    "# Print a summary\n",
    "print(final_df_d2v.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_election</th>\n",
       "      <th>d2v_d1</th>\n",
       "      <th>d2v_d2</th>\n",
       "      <th>party</th>\n",
       "      <th>election</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11110_2006</td>\n",
       "      <td>1.565795</td>\n",
       "      <td>2.218436</td>\n",
       "      <td>11110</td>\n",
       "      <td>2006</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11220_2006</td>\n",
       "      <td>0.950567</td>\n",
       "      <td>2.639069</td>\n",
       "      <td>11220</td>\n",
       "      <td>2006</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11320_2006</td>\n",
       "      <td>3.613655</td>\n",
       "      <td>2.860114</td>\n",
       "      <td>11320</td>\n",
       "      <td>2006</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11420_2006</td>\n",
       "      <td>-1.436965</td>\n",
       "      <td>-0.344464</td>\n",
       "      <td>11420</td>\n",
       "      <td>2006</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11520_2006</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>1.376408</td>\n",
       "      <td>11520</td>\n",
       "      <td>2006</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  party_election    d2v_d1    d2v_d2  party  election country\n",
       "0     11110_2006  1.565795  2.218436  11110      2006  Sweden\n",
       "1     11220_2006  0.950567  2.639069  11220      2006  Sweden\n",
       "2     11320_2006  3.613655  2.860114  11320      2006  Sweden\n",
       "3     11420_2006 -1.436965 -0.344464  11420      2006  Sweden\n",
       "4     11520_2006  1.335489  1.376408  11520      2006  Sweden"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_d2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_election</th>\n",
       "      <th>d2v_d1</th>\n",
       "      <th>d2v_d2</th>\n",
       "      <th>party</th>\n",
       "      <th>election</th>\n",
       "      <th>country</th>\n",
       "      <th>party_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41113_1998</td>\n",
       "      <td>0.849250</td>\n",
       "      <td>14.821021</td>\n",
       "      <td>41113</td>\n",
       "      <td>1998</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Alliance 90/Greens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41221_1998</td>\n",
       "      <td>0.504377</td>\n",
       "      <td>15.658217</td>\n",
       "      <td>41221</td>\n",
       "      <td>1998</td>\n",
       "      <td>Germany</td>\n",
       "      <td>The Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41320_1998</td>\n",
       "      <td>-7.870218</td>\n",
       "      <td>-11.670078</td>\n",
       "      <td>41320</td>\n",
       "      <td>1998</td>\n",
       "      <td>Germany</td>\n",
       "      <td>SPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41420_1998</td>\n",
       "      <td>18.112190</td>\n",
       "      <td>-4.072463</td>\n",
       "      <td>41420</td>\n",
       "      <td>1998</td>\n",
       "      <td>Germany</td>\n",
       "      <td>FDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41521_1998</td>\n",
       "      <td>2.736619</td>\n",
       "      <td>-14.968621</td>\n",
       "      <td>41521</td>\n",
       "      <td>1998</td>\n",
       "      <td>Germany</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  party_election     d2v_d1     d2v_d2  party  election  country  \\\n",
       "0     41113_1998   0.849250  14.821021  41113      1998  Germany   \n",
       "1     41221_1998   0.504377  15.658217  41221      1998  Germany   \n",
       "2     41320_1998  -7.870218 -11.670078  41320      1998  Germany   \n",
       "3     41420_1998  18.112190  -4.072463  41420      1998  Germany   \n",
       "4     41521_1998   2.736619 -14.968621  41521      1998  Germany   \n",
       "\n",
       "           party_name  \n",
       "0  Alliance 90/Greens  \n",
       "1            The Left  \n",
       "2                 SPD  \n",
       "3                 FDP  \n",
       "4             CDU/CSU  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_germany = final_df_d2v[final_df_d2v.country == 'Germany'].copy()\n",
    "d2v_germany.loc[:,'party_name'] = d2v_germany['party'].astype(str).apply(party_deu)\n",
    "d2v_germany = d2v_germany[d2v_germany.party_name != 'Other'].reset_index(drop=True)\n",
    "d2v_germany.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in d2v_germany.groupby('party_name'):\n",
    "    ax.plot(group.election, group.d2v_d1, marker='o',  ms=4, label=name)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d2v.to_csv('data/py_outputs/r&c_gen.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec scaling - relevant topics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Agriculture - Protectionism',\n",
       " 'Economics',\n",
       " 'Education',\n",
       " 'Environment - Growth',\n",
       " 'European Integration',\n",
       " 'Fabrics of Society',\n",
       " 'Immigration',\n",
       " 'International Relations',\n",
       " 'Labour and Social Welfare',\n",
       " 'Military',\n",
       " 'Other',\n",
       " 'Political System'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(manifesto_d2v['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: Sweden\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Norway\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Denmark\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Finland\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Iceland\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Belgium\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Netherlands\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: France\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Italy\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Spain\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Greece\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Portugal\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Germany\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Austria\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Switzerland\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: United Kingdom\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Ireland\n",
      "Processing topic: Environment - Growth\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Economics\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Labour and Social Welfare\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Political System\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: Immigration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing topic: European Integration\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4138 entries, 0 to 4137\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   party_election  4138 non-null   object \n",
      " 1   d2v_d1          4138 non-null   float32\n",
      " 2   d2v_d2          4138 non-null   float32\n",
      " 3   party           4138 non-null   object \n",
      " 4   election        4138 non-null   object \n",
      " 5   country         4138 non-null   object \n",
      " 6   topic           4138 non-null   object \n",
      "dtypes: float32(2), object(5)\n",
      "memory usage: 194.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the country-topic level dataframes\n",
    "country_topic_dfs = []\n",
    "\n",
    "# Get the unique list of countries from your data\n",
    "unique_countries = manifesto_d2v['countryname'].unique()\n",
    "\n",
    "# Loop through each country\n",
    "for country in unique_countries:\n",
    "    print(f\"Processing country: {country}\")\n",
    "    \n",
    "    # Filter the dataset for the current country\n",
    "    country_data = manifesto_d2v[manifesto_d2v['countryname'] == country]\n",
    "    country_data = country_data[country_data['topic'].isin(['Environment - Growth', 'Political System', 'Economics',\n",
    "                                                            'European Integration','Labour and Social Welfare',\n",
    "                                                            'Immigration'])]\n",
    "    # Get the unique list of topics within this country\n",
    "    unique_topics = country_data['topic'].unique()\n",
    "    \n",
    "    # Loop through each topic in the country\n",
    "    for topic in unique_topics:\n",
    "        print(f\"Processing topic: {topic}\")\n",
    "\n",
    "        # Filter the dataset for the current country and topic\n",
    "        country_topic_data = country_data[country_data['topic'] == topic]\n",
    "\n",
    "        # Build the corpus iterator for this country's topic-specific data\n",
    "        outputs_stream = phraseIterator(country_topic_data, 'text_cleaned')\n",
    "        bigram = Phraser(Phrases(outputs_stream, min_count=1, threshold=5))\n",
    "        trigram = Phrases(bigram[outputs_stream], min_count=1, threshold=5)\n",
    "\n",
    "        # Create the Doc2Vec model and build vocabulary\n",
    "        model = Doc2Vec(vector_size=500, window=6, min_count=1, workers=16, epochs=20, seed=seed_val)\n",
    "        model.build_vocab(corpusIterator(country_topic_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'))\n",
    "\n",
    "        # Train the model\n",
    "        model.train(corpusIterator(country_topic_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'),\n",
    "                    total_examples=model.corpus_count, epochs=20)\n",
    "\n",
    "        # Generate embeddings and apply dimensionality reduction\n",
    "        embed_dict = d2v_reduct(model)\n",
    "        df_d2v = pd.DataFrame.from_dict(embed_dict).transpose()\n",
    "        df_d2v.index.name = 'party_election'\n",
    "        df_d2v.reset_index(inplace=True)\n",
    "        pca = PCA(n_components=2, random_state=seed_val)\n",
    "        df_d2v[['d2v_d1', 'd2v_d2']] = pca.fit_transform(df_d2v.iloc[:, 1:])\n",
    "        df_d2v = df_d2v[['party_election', 'd2v_d1', 'd2v_d2']]\n",
    "\n",
    "        # Split the 'party_election' label into separate columns\n",
    "        df_d2v[['party', 'election']] = df_d2v['party_election'].str.split('_', expand=True)\n",
    "        df_d2v.loc[:, 'election'] = df_d2v['election'].astype(int)\n",
    "        df_d2v['country'] = country  # Add country column\n",
    "        df_d2v['topic'] = topic  # Add topic column\n",
    "\n",
    "        # Append the country-topic-level dataframe to the list\n",
    "        country_topic_dfs.append(df_d2v)\n",
    "\n",
    "# Merge all country-topic-level datasets into a single dataframe\n",
    "final_df_d2v = pd.concat(country_topic_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final dataframe to a CSV file\n",
    "final_df_d2v.to_csv('data/py_outputs/r&c_party_election_topic.csv', index=False)\n",
    "\n",
    "# Print a summary\n",
    "print(final_df_d2v.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec scaling - Environment Protection\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_ep = manifesto_d2v[manifesto_d2v.cmp_code.isin(['501'])].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>eu_code</th>\n",
       "      <th>pos</th>\n",
       "      <th>manifesto_id</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>abbrev</th>\n",
       "      <th>name</th>\n",
       "      <th>edate</th>\n",
       "      <th>parfam</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>election</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>party_election</th>\n",
       "      <th>country_party_election</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>och ett lÃ¥ngsiktigt miljÃ¶ansvar.</td>\n",
       "      <td>and long-term environmental responsibility.</td>\n",
       "      <td>501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Environment - Growth</td>\n",
       "      <td>2006</td>\n",
       "      <td>lÃ¥ngsiktigt miljÃ¶ansvar</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VÃ¥r politik kombinerar en grÃ¶n omstÃ¤llning</td>\n",
       "      <td>Our policies combine a green transition</td>\n",
       "      <td>501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Environment - Growth</td>\n",
       "      <td>2006</td>\n",
       "      <td>politik kombinerar grÃ¶n omstÃ¤llning</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>solidaritet med djur, natur och det ekologiska...</td>\n",
       "      <td>with animals, nature and the ecological system...</td>\n",
       "      <td>501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Environment - Growth</td>\n",
       "      <td>2006</td>\n",
       "      <td>solidaritet djur natur ekologiska systemet</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vi vill att Sveriges beroende av olja och andr...</td>\n",
       "      <td>We want to break Sweden's dependence on oil an...</td>\n",
       "      <td>501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Environment - Growth</td>\n",
       "      <td>2006</td>\n",
       "      <td>vill sveriges beroende olja andra fossila brÃ¤n...</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>och hus byggas energisnÃ¥lt.</td>\n",
       "      <td>and houses are built in an energy-efficient way.</td>\n",
       "      <td>501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Environment - Growth</td>\n",
       "      <td>2006</td>\n",
       "      <td>hus byggas energisnÃ¥lt</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                   och ett lÃ¥ngsiktigt miljÃ¶ansvar.   \n",
       "1         VÃ¥r politik kombinerar en grÃ¶n omstÃ¤llning   \n",
       "2  solidaritet med djur, natur och det ekologiska...   \n",
       "3  Vi vill att Sveriges beroende av olja och andr...   \n",
       "4                        och hus byggas energisnÃ¥lt.   \n",
       "\n",
       "                                             text_en cmp_code  eu_code  pos  \\\n",
       "0        and long-term environmental responsibility.      501      NaN    5   \n",
       "1            Our policies combine a green transition      501      NaN    6   \n",
       "2  with animals, nature and the ecological system...      501      NaN   13   \n",
       "3  We want to break Sweden's dependence on oil an...      501      NaN   19   \n",
       "4   and houses are built in an energy-efficient way.      501      NaN   22   \n",
       "\n",
       "   manifesto_id  party    date language  source  ...  abbrev  \\\n",
       "0  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "1  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "2  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "3  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "4  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "\n",
       "                    name       edate parfam sentiment                 topic  \\\n",
       "0  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left  Environment - Growth   \n",
       "1  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left  Environment - Growth   \n",
       "2  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left  Environment - Growth   \n",
       "3  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left  Environment - Growth   \n",
       "4  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left  Environment - Growth   \n",
       "\n",
       "   election                                       text_cleaned party_election  \\\n",
       "0      2006                            lÃ¥ngsiktigt miljÃ¶ansvar     11110_2006   \n",
       "1      2006                politik kombinerar grÃ¶n omstÃ¤llning     11110_2006   \n",
       "2      2006         solidaritet djur natur ekologiska systemet     11110_2006   \n",
       "3      2006  vill sveriges beroende olja andra fossila brÃ¤n...     11110_2006   \n",
       "4      2006                             hus byggas energisnÃ¥lt     11110_2006   \n",
       "\n",
       "  country_party_election  \n",
       "0      Sweden_11110_2006  \n",
       "1      Sweden_11110_2006  \n",
       "2      Sweden_11110_2006  \n",
       "3      Sweden_11110_2006  \n",
       "4      Sweden_11110_2006  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_ep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: Sweden\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Norway\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Denmark\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Finland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Iceland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Belgium\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Netherlands\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: France\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Italy\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Spain\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Greece\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Portugal\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Germany\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Austria\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Switzerland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: United Kingdom\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Ireland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 684 entries, 0 to 683\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   party_election  684 non-null    object \n",
      " 1   d2v_d1          684 non-null    float32\n",
      " 2   d2v_d2          684 non-null    float32\n",
      " 3   party           684 non-null    object \n",
      " 4   election        684 non-null    object \n",
      " 5   country         684 non-null    object \n",
      "dtypes: float32(2), object(4)\n",
      "memory usage: 26.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the country-level dataframes\n",
    "country_dfs = []\n",
    "\n",
    "# Get the unique list of countries from your data\n",
    "unique_countries = manifesto_ep['countryname'].unique()\n",
    "\n",
    "# Loop through each country and process separately\n",
    "for country in unique_countries:\n",
    "    print(f\"Processing country: {country}\")\n",
    "    \n",
    "    # Filter the dataset for the current country\n",
    "    country_data = manifesto_ep[manifesto_ep['countryname'] == country]\n",
    "    \n",
    "    # Build the corpus iterator for this country's data\n",
    "    outputs_stream = phraseIterator(country_data, 'text_cleaned')\n",
    "    bigram = Phraser(Phrases(outputs_stream, min_count=1, threshold=5))\n",
    "    trigram = Phrases(bigram[outputs_stream], min_count=1, threshold=5)\n",
    "    \n",
    "    # Create the Doc2Vec model and build vocabulary\n",
    "    model = Doc2Vec(vector_size=500, window=6, min_count=1, workers=16, epochs=20, seed=seed_val)\n",
    "    model.build_vocab(corpusIterator(country_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'))\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(corpusIterator(country_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'),\n",
    "                total_examples=model.corpus_count, epochs=20)\n",
    "    \n",
    "    # Generate embeddings and apply dimensionality reduction\n",
    "    embed_dict = d2v_reduct(model)\n",
    "    df_d2v = pd.DataFrame.from_dict(embed_dict).transpose()\n",
    "    df_d2v.index.name = 'party_election'\n",
    "    df_d2v.reset_index(inplace=True)\n",
    "    pca = PCA(n_components=2, random_state=seed_val)\n",
    "    df_d2v[['d2v_d1', 'd2v_d2']] = pca.fit_transform(df_d2v.iloc[:, 1:])\n",
    "    df_d2v = df_d2v[['party_election', 'd2v_d1', 'd2v_d2']]\n",
    "    \n",
    "    # Split the 'party_election' label into separate columns\n",
    "    df_d2v[['party', 'election']] = df_d2v['party_election'].str.split('_', expand=True)\n",
    "    df_d2v.loc[:, 'election'] = df_d2v['election'].astype(int)\n",
    "    df_d2v['country'] = country  # Add country column for merging later\n",
    "    \n",
    "    # Append the country-level dataframe to the list\n",
    "    country_dfs.append(df_d2v)\n",
    "\n",
    "# Merge all country-level datasets into a single dataframe\n",
    "final_df_d2v = pd.concat(country_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final dataframe to a CSV file\n",
    "final_df_d2v.to_csv('data/py_outputs/r&c_ep_party_election.csv', index=False)\n",
    "\n",
    "# Print a summary\n",
    "print(final_df_d2v.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec scaling - Germany, growth vs anti growth\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_welfare = manifesto_d2v[manifesto_d2v.cmp_code.isin(['504', '505'])].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>eu_code</th>\n",
       "      <th>pos</th>\n",
       "      <th>manifesto_id</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>abbrev</th>\n",
       "      <th>name</th>\n",
       "      <th>edate</th>\n",
       "      <th>parfam</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>election</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>party_election</th>\n",
       "      <th>country_party_election</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nya livschanser</td>\n",
       "      <td>new life chances</td>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Labour and Social Welfare</td>\n",
       "      <td>2006</td>\n",
       "      <td>nya livschanser</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Det finns behov av och utrymme fÃ¶r fler anstÃ¤l...</td>\n",
       "      <td>There is a need and room for more employees in...</td>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Labour and Social Welfare</td>\n",
       "      <td>2006</td>\n",
       "      <td>finns behov utrymme fler anstÃ¤llda offentligt ...</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vi vill Ã¶verfÃ¶ra pengar frÃ¥n arbetsmarknadsÃ¥tg...</td>\n",
       "      <td>We want to transfer money from labor market me...</td>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Labour and Social Welfare</td>\n",
       "      <td>2006</td>\n",
       "      <td>vill Ã¶verfÃ¶ra pengar arbetsmarknadsÃ¥tgÃ¤rder an...</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sjukvÃ¥rden,</td>\n",
       "      <td>healthcare,</td>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Labour and Social Welfare</td>\n",
       "      <td>2006</td>\n",
       "      <td>sjukvÃ¥rden</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ã¤ldreomsorgen</td>\n",
       "      <td>elderly care</td>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10</td>\n",
       "      <td>left</td>\n",
       "      <td>Labour and Social Welfare</td>\n",
       "      <td>2006</td>\n",
       "      <td>Ã¤ldreomsorgen</td>\n",
       "      <td>11110_2006</td>\n",
       "      <td>Sweden_11110_2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                    nya livschanser   \n",
       "1  Det finns behov av och utrymme fÃ¶r fler anstÃ¤l...   \n",
       "2  Vi vill Ã¶verfÃ¶ra pengar frÃ¥n arbetsmarknadsÃ¥tg...   \n",
       "3                                        sjukvÃ¥rden,   \n",
       "4                                      Ã¤ldreomsorgen   \n",
       "\n",
       "                                             text_en cmp_code  eu_code  pos  \\\n",
       "0                                   new life chances      504      NaN    4   \n",
       "1  There is a need and room for more employees in...      504      NaN   31   \n",
       "2  We want to transfer money from labor market me...      504      NaN   32   \n",
       "3                                        healthcare,      504      NaN   35   \n",
       "4                                       elderly care      504      NaN   36   \n",
       "\n",
       "   manifesto_id  party    date language  source  ...  abbrev  \\\n",
       "0  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "1  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "2  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "3  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "4  11110_200609  11110  200609  swedish  MARPOR  ...      MP   \n",
       "\n",
       "                    name       edate parfam sentiment  \\\n",
       "0  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left   \n",
       "1  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left   \n",
       "2  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left   \n",
       "3  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left   \n",
       "4  MiljÃ¶partiet de GrÃ¶na  17/09/2006     10      left   \n",
       "\n",
       "                       topic  election  \\\n",
       "0  Labour and Social Welfare      2006   \n",
       "1  Labour and Social Welfare      2006   \n",
       "2  Labour and Social Welfare      2006   \n",
       "3  Labour and Social Welfare      2006   \n",
       "4  Labour and Social Welfare      2006   \n",
       "\n",
       "                                        text_cleaned party_election  \\\n",
       "0                                    nya livschanser     11110_2006   \n",
       "1  finns behov utrymme fler anstÃ¤llda offentligt ...     11110_2006   \n",
       "2  vill Ã¶verfÃ¶ra pengar arbetsmarknadsÃ¥tgÃ¤rder an...     11110_2006   \n",
       "3                                         sjukvÃ¥rden     11110_2006   \n",
       "4                                      Ã¤ldreomsorgen     11110_2006   \n",
       "\n",
       "  country_party_election  \n",
       "0      Sweden_11110_2006  \n",
       "1      Sweden_11110_2006  \n",
       "2      Sweden_11110_2006  \n",
       "3      Sweden_11110_2006  \n",
       "4      Sweden_11110_2006  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_welfare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: Sweden\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Norway\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Denmark\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Finland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Iceland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Belgium\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Netherlands\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: France\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Italy\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Spain\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Greece\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Portugal\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Germany\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Austria\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Switzerland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: United Kingdom\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Processing country: Ireland\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "Starting new epoch\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 722 entries, 0 to 721\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   party_election  722 non-null    object \n",
      " 1   d2v_d1          722 non-null    float32\n",
      " 2   d2v_d2          722 non-null    float32\n",
      " 3   party           722 non-null    object \n",
      " 4   election        722 non-null    object \n",
      " 5   country         722 non-null    object \n",
      "dtypes: float32(2), object(4)\n",
      "memory usage: 28.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the country-level dataframes\n",
    "country_dfs = []\n",
    "\n",
    "# Get the unique list of countries from your data\n",
    "unique_countries = manifesto_welfare['countryname'].unique()\n",
    "\n",
    "# Loop through each country and process separately\n",
    "for country in unique_countries:\n",
    "    print(f\"Processing country: {country}\")\n",
    "    \n",
    "    # Filter the dataset for the current country\n",
    "    country_data = manifesto_welfare[manifesto_welfare['countryname'] == country]\n",
    "    \n",
    "    # Build the corpus iterator for this country's data\n",
    "    outputs_stream = phraseIterator(country_data, 'text_cleaned')\n",
    "    bigram = Phraser(Phrases(outputs_stream, min_count=1, threshold=5))\n",
    "    trigram = Phrases(bigram[outputs_stream], min_count=1, threshold=5)\n",
    "    \n",
    "    # Create the Doc2Vec model and build vocabulary\n",
    "    model = Doc2Vec(vector_size=500, window=6, min_count=1, workers=16, epochs=20, seed=seed_val)\n",
    "    model.build_vocab(corpusIterator(country_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'))\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(corpusIterator(country_data, bigram=bigram, trigram=trigram, text='text_cleaned', labels='party_election'),\n",
    "                total_examples=model.corpus_count, epochs=20)\n",
    "    \n",
    "    # Generate embeddings and apply dimensionality reduction\n",
    "    embed_dict = d2v_reduct(model)\n",
    "    df_d2v = pd.DataFrame.from_dict(embed_dict).transpose()\n",
    "    df_d2v.index.name = 'party_election'\n",
    "    df_d2v.reset_index(inplace=True)\n",
    "    pca = PCA(n_components=2, random_state=seed_val)\n",
    "    df_d2v[['d2v_d1', 'd2v_d2']] = pca.fit_transform(df_d2v.iloc[:, 1:])\n",
    "    df_d2v = df_d2v[['party_election', 'd2v_d1', 'd2v_d2']]\n",
    "    \n",
    "    # Split the 'party_election' label into separate columns\n",
    "    df_d2v[['party', 'election']] = df_d2v['party_election'].str.split('_', expand=True)\n",
    "    df_d2v.loc[:, 'election'] = df_d2v['election'].astype(int)\n",
    "    df_d2v['country'] = country  # Add country column for merging later\n",
    "    \n",
    "    # Append the country-level dataframe to the list\n",
    "    country_dfs.append(df_d2v)\n",
    "\n",
    "# Merge all country-level datasets into a single dataframe\n",
    "final_df_d2v = pd.concat(country_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final dataframe to a CSV file\n",
    "final_df_d2v.to_csv('data/py_outputs/r&c_welfare_party_election.csv', index=False)\n",
    "\n",
    "# Print a summary\n",
    "print(final_df_d2v.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale position scores for all countries (released dataset + model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain for the entire dataset with all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_org = pd.read_csv(os.path.join(\"data\", \"r_outputs\",\"pulled_manifestoes.csv\"), encoding=\"utf-8\", dtype={2:'str',18:'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_other = pd.read_csv(os.path.join(\"data\", \"r_outputs\",\"pulled_manifestoes_test.csv\"), encoding=\"utf-8\", dtype={2:'str',18:'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_org_cleaned = manifesto_org.dropna(axis=1, how='all')\n",
    "manifesto_other_cleaned = manifesto_other.dropna(axis=1, how='all')\n",
    "manifesto_full = pd.concat([manifesto_org_cleaned, manifesto_other_cleaned]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>eu_code</th>\n",
       "      <th>pos</th>\n",
       "      <th>manifesto_id</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>title</th>\n",
       "      <th>translation_en</th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>party_code</th>\n",
       "      <th>countryname</th>\n",
       "      <th>abbrev</th>\n",
       "      <th>name</th>\n",
       "      <th>edate</th>\n",
       "      <th>parfam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na gÃ¥r till val pÃ¥ en polit...</td>\n",
       "      <td>The Green Party is running on a policy for mor...</td>\n",
       "      <td>408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...</td>\n",
       "      <td>True</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11</td>\n",
       "      <td>11110</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fler fÃ¶retag,</td>\n",
       "      <td>more businesses, a</td>\n",
       "      <td>402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...</td>\n",
       "      <td>True</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11</td>\n",
       "      <td>11110</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>levande landsbygd,</td>\n",
       "      <td>vibrant countryside,</td>\n",
       "      <td>301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...</td>\n",
       "      <td>True</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11</td>\n",
       "      <td>11110</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nya livschanser</td>\n",
       "      <td>new life chances</td>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...</td>\n",
       "      <td>True</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11</td>\n",
       "      <td>11110</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>och ett lÃ¥ngsiktigt miljÃ¶ansvar.</td>\n",
       "      <td>and long-term environmental responsibility.</td>\n",
       "      <td>501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11110</td>\n",
       "      <td>200609</td>\n",
       "      <td>swedish</td>\n",
       "      <td>MARPOR</td>\n",
       "      <td>...</td>\n",
       "      <td>Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...</td>\n",
       "      <td>True</td>\n",
       "      <td>11110_200609</td>\n",
       "      <td>11</td>\n",
       "      <td>11110</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>MP</td>\n",
       "      <td>MiljÃ¶partiet de GrÃ¶na</td>\n",
       "      <td>17/09/2006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  MiljÃ¶partiet de GrÃ¶na gÃ¥r till val pÃ¥ en polit...   \n",
       "1                                      fler fÃ¶retag,   \n",
       "2                                 levande landsbygd,   \n",
       "3                                    nya livschanser   \n",
       "4                   och ett lÃ¥ngsiktigt miljÃ¶ansvar.   \n",
       "\n",
       "                                             text_en cmp_code  eu_code  pos  \\\n",
       "0  The Green Party is running on a policy for mor...      408      NaN    1   \n",
       "1                                 more businesses, a      402      NaN    2   \n",
       "2                               vibrant countryside,      301      NaN    3   \n",
       "3                                   new life chances      504      NaN    4   \n",
       "4        and long-term environmental responsibility.      501      NaN    5   \n",
       "\n",
       "   manifesto_id  party    date language  source  ...  \\\n",
       "0  11110_200609  11110  200609  swedish  MARPOR  ...   \n",
       "1  11110_200609  11110  200609  swedish  MARPOR  ...   \n",
       "2  11110_200609  11110  200609  swedish  MARPOR  ...   \n",
       "3  11110_200609  11110  200609  swedish  MARPOR  ...   \n",
       "4  11110_200609  11110  200609  swedish  MARPOR  ...   \n",
       "\n",
       "                                               title translation_en  \\\n",
       "0  Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...           True   \n",
       "1  Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...           True   \n",
       "2  Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...           True   \n",
       "3  Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...           True   \n",
       "4  Valmanifest 2006. GrÃ¶nare Sverige! â€“ fÃ¶r Ã¶kad ...           True   \n",
       "\n",
       "             id country party_code countryname  abbrev                   name  \\\n",
       "0  11110_200609      11      11110      Sweden      MP  MiljÃ¶partiet de GrÃ¶na   \n",
       "1  11110_200609      11      11110      Sweden      MP  MiljÃ¶partiet de GrÃ¶na   \n",
       "2  11110_200609      11      11110      Sweden      MP  MiljÃ¶partiet de GrÃ¶na   \n",
       "3  11110_200609      11      11110      Sweden      MP  MiljÃ¶partiet de GrÃ¶na   \n",
       "4  11110_200609      11      11110      Sweden      MP  MiljÃ¶partiet de GrÃ¶na   \n",
       "\n",
       "        edate parfam  \n",
       "0  17/09/2006   10.0  \n",
       "1  17/09/2006   10.0  \n",
       "2  17/09/2006   10.0  \n",
       "3  17/09/2006   10.0  \n",
       "4  17/09/2006   10.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manifesto_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_full = manifesto_full[(manifesto_full.cmp_code.notna()) & ~(manifesto_full.cmp_code == 'H')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_full['sentiment'] = manifesto_full['cmp_code'].apply(sentiment_code)\n",
    "manifesto_full['topic'] = manifesto_full['cmp_code'].apply(topic_code)\n",
    "manifesto_full['election'] = manifesto_full['date'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>eu_code</th>\n",
       "      <th>pos</th>\n",
       "      <th>manifesto_id</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>party_code</th>\n",
       "      <th>countryname</th>\n",
       "      <th>abbrev</th>\n",
       "      <th>name</th>\n",
       "      <th>edate</th>\n",
       "      <th>parfam</th>\n",
       "      <th>topic</th>\n",
       "      <th>election</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>498722</td>\n",
       "      <td>441871</td>\n",
       "      <td>498722</td>\n",
       "      <td>491</td>\n",
       "      <td>498722</td>\n",
       "      <td>498722</td>\n",
       "      <td>498722</td>\n",
       "      <td>498722</td>\n",
       "      <td>498722</td>\n",
       "      <td>459916</td>\n",
       "      <td>...</td>\n",
       "      <td>459916</td>\n",
       "      <td>498722</td>\n",
       "      <td>498722</td>\n",
       "      <td>495031</td>\n",
       "      <td>467230</td>\n",
       "      <td>495031</td>\n",
       "      <td>495031</td>\n",
       "      <td>495031</td>\n",
       "      <td>498722</td>\n",
       "      <td>498722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>271060</td>\n",
       "      <td>230575</td>\n",
       "      <td>271060</td>\n",
       "      <td>208</td>\n",
       "      <td>271060</td>\n",
       "      <td>271060</td>\n",
       "      <td>271060</td>\n",
       "      <td>271060</td>\n",
       "      <td>271060</td>\n",
       "      <td>240526</td>\n",
       "      <td>...</td>\n",
       "      <td>240526</td>\n",
       "      <td>271060</td>\n",
       "      <td>271060</td>\n",
       "      <td>269746</td>\n",
       "      <td>254848</td>\n",
       "      <td>269746</td>\n",
       "      <td>269746</td>\n",
       "      <td>269746</td>\n",
       "      <td>271060</td>\n",
       "      <td>271060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>205496</td>\n",
       "      <td>175872</td>\n",
       "      <td>205496</td>\n",
       "      <td>462</td>\n",
       "      <td>205496</td>\n",
       "      <td>205496</td>\n",
       "      <td>205496</td>\n",
       "      <td>205496</td>\n",
       "      <td>205496</td>\n",
       "      <td>182881</td>\n",
       "      <td>...</td>\n",
       "      <td>182881</td>\n",
       "      <td>205496</td>\n",
       "      <td>205496</td>\n",
       "      <td>204199</td>\n",
       "      <td>196171</td>\n",
       "      <td>204199</td>\n",
       "      <td>204199</td>\n",
       "      <td>204199</td>\n",
       "      <td>205496</td>\n",
       "      <td>205496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             text  text_en  cmp_code  eu_code     pos  manifesto_id   party  \\\n",
       "sentiment                                                                     \n",
       "left       498722   441871    498722      491  498722        498722  498722   \n",
       "neutral    271060   230575    271060      208  271060        271060  271060   \n",
       "right      205496   175872    205496      462  205496        205496  205496   \n",
       "\n",
       "             date  language  source  ...      id  country  party_code  \\\n",
       "sentiment                            ...                                \n",
       "left       498722    498722  459916  ...  459916   498722      498722   \n",
       "neutral    271060    271060  240526  ...  240526   271060      271060   \n",
       "right      205496    205496  182881  ...  182881   205496      205496   \n",
       "\n",
       "           countryname  abbrev    name   edate  parfam   topic  election  \n",
       "sentiment                                                                 \n",
       "left            495031  467230  495031  495031  495031  498722    498722  \n",
       "neutral         269746  254848  269746  269746  269746  271060    271060  \n",
       "right           204199  196171  204199  204199  204199  205496    205496  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_full.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>left</th>\n",
       "      <th>neutral</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agriculture - Protectionism</th>\n",
       "      <td>22929</td>\n",
       "      <td>0</td>\n",
       "      <td>2506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>58507</td>\n",
       "      <td>86154</td>\n",
       "      <td>44805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>49656</td>\n",
       "      <td>0</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Environment - Growth</th>\n",
       "      <td>67767</td>\n",
       "      <td>21287</td>\n",
       "      <td>14133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>European Integration</th>\n",
       "      <td>14662</td>\n",
       "      <td>0</td>\n",
       "      <td>6240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fabrics of Society</th>\n",
       "      <td>39173</td>\n",
       "      <td>19394</td>\n",
       "      <td>57744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Immigration</th>\n",
       "      <td>12906</td>\n",
       "      <td>0</td>\n",
       "      <td>20398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>International Relations</th>\n",
       "      <td>27241</td>\n",
       "      <td>3776</td>\n",
       "      <td>2441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labour and Social Welfare</th>\n",
       "      <td>198438</td>\n",
       "      <td>4396</td>\n",
       "      <td>6782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Military</th>\n",
       "      <td>5068</td>\n",
       "      <td>0</td>\n",
       "      <td>12104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0</td>\n",
       "      <td>87143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Political System</th>\n",
       "      <td>2375</td>\n",
       "      <td>48910</td>\n",
       "      <td>37681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment                      left  neutral  right\n",
       "topic                                              \n",
       "Agriculture - Protectionism   22929        0   2506\n",
       "Economics                     58507    86154  44805\n",
       "Education                     49656        0    662\n",
       "Environment - Growth          67767    21287  14133\n",
       "European Integration          14662        0   6240\n",
       "Fabrics of Society            39173    19394  57744\n",
       "Immigration                   12906        0  20398\n",
       "International Relations       27241     3776   2441\n",
       "Labour and Social Welfare    198438     4396   6782\n",
       "Military                       5068        0  12104\n",
       "Other                             0    87143      0\n",
       "Political System               2375    48910  37681"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_full.groupby(['topic'])['sentiment'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = group_texts(manifesto_full, \n",
    "                      ['countryname','election','party','cmp_code'], 'text', \n",
    "                      max_group_factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped = pd.DataFrame(results)\n",
    "manifesto_regrouped = manifesto_regrouped.explode('text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = manifesto_regrouped['labels'].str.split(';', expand=True)\n",
    "manifesto_regrouped = pd.concat([manifesto_regrouped, df_cols], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.columns = ['text', 'country_election_party_code', 'country','election', 'party', 'cmp_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>country_election_party_code</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statt dessen soll ein Freiwilligen-Milizheer g...</td>\n",
       "      <td>Austria;1999;42110;104</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weg von der Sicherheit durch RÃ¼stung, Unsere S...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Militarisierung der EU bringt mehr RÃ¼stung...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Statt dessen soll ein Freiwilligen-Milizheer g...   \n",
       "1  Weg von der Sicherheit durch RÃ¼stung, Unsere S...   \n",
       "2  Die Militarisierung der EU bringt mehr RÃ¼stung...   \n",
       "3  Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...   \n",
       "4  Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...   \n",
       "\n",
       "  country_election_party_code  country election  party cmp_code  \n",
       "0      Austria;1999;42110;104  Austria     1999  42110      104  \n",
       "1      Austria;1999;42110;105  Austria     1999  42110      105  \n",
       "2      Austria;1999;42110;105  Austria     1999  42110      105  \n",
       "3      Austria;1999;42110;105  Austria     1999  42110      105  \n",
       "4      Austria;1999;42110;105  Austria     1999  42110      105  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.loc[:,'sentiment'] = manifesto_regrouped['cmp_code'].apply(sentiment_code)\n",
    "manifesto_regrouped.loc[:,'topic'] = manifesto_regrouped['cmp_code'].apply(topic_code)\n",
    "manifesto_regrouped = manifesto_regrouped.drop_duplicates().reset_index(drop=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>country_election_party_code</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statt dessen soll ein Freiwilligen-Milizheer g...</td>\n",
       "      <td>Austria;1999;42110;104</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>104</td>\n",
       "      <td>right</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weg von der Sicherheit durch RÃ¼stung, Unsere S...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Militarisierung der EU bringt mehr RÃ¼stung...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Statt dessen soll ein Freiwilligen-Milizheer g...   \n",
       "1  Weg von der Sicherheit durch RÃ¼stung, Unsere S...   \n",
       "2  Die Militarisierung der EU bringt mehr RÃ¼stung...   \n",
       "3  Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...   \n",
       "4  Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...   \n",
       "\n",
       "  country_election_party_code  country election  party cmp_code sentiment  \\\n",
       "0      Austria;1999;42110;104  Austria     1999  42110      104     right   \n",
       "1      Austria;1999;42110;105  Austria     1999  42110      105      left   \n",
       "2      Austria;1999;42110;105  Austria     1999  42110      105      left   \n",
       "3      Austria;1999;42110;105  Austria     1999  42110      105      left   \n",
       "4      Austria;1999;42110;105  Austria     1999  42110      105      left   \n",
       "\n",
       "      topic  \n",
       "0  Military  \n",
       "1  Military  \n",
       "2  Military  \n",
       "3  Military  \n",
       "4  Military  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.to_csv('data/temps/manifesto_regrouped_full_processed.csv', encoding='utf-8', index=False)\n",
    "manifesto_full.to_csv('data/temps/manifesto_full_processed.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped = pd.read_csv('data/temps/coalitionagree_regrouped_processed.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.loc[:, 'source'] = 'manifestos'\n",
    "coalition_regrouped.loc[:, 'source'] = 'coalition_contracts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39287 entries, 0 to 39286\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       39287 non-null  object\n",
      " 1   labels     39287 non-null  object\n",
      " 2   country    39287 non-null  object\n",
      " 3   year       39287 non-null  int64 \n",
      " 4   cmp_short  39287 non-null  int64 \n",
      " 5   cmp_long   39287 non-null  int64 \n",
      " 6   sentiment  39287 non-null  object\n",
      " 7   topic      39287 non-null  object\n",
      " 8   source     39287 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "coalition_regrouped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>left</th>\n",
       "      <th>neutral</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agriculture - Protectionism</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>2025</td>\n",
       "      <td>3564</td>\n",
       "      <td>2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>1883</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Environment - Growth</th>\n",
       "      <td>2045</td>\n",
       "      <td>337</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>European Integration</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fabrics of Society</th>\n",
       "      <td>831</td>\n",
       "      <td>769</td>\n",
       "      <td>2115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Immigration</th>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>International Relations</th>\n",
       "      <td>1236</td>\n",
       "      <td>202</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labour and Social Welfare</th>\n",
       "      <td>6767</td>\n",
       "      <td>30</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Military</th>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0</td>\n",
       "      <td>5633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Political System</th>\n",
       "      <td>121</td>\n",
       "      <td>2541</td>\n",
       "      <td>1542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment                    left  neutral  right\n",
       "topic                                            \n",
       "Agriculture - Protectionism   877        0     86\n",
       "Economics                    2025     3564   2592\n",
       "Education                    1883        0     37\n",
       "Environment - Growth         2045      337    754\n",
       "European Integration          862        0     90\n",
       "Fabrics of Society            831      769   2115\n",
       "Immigration                   485        0    429\n",
       "International Relations      1236      202     36\n",
       "Labour and Social Welfare    6767       30    502\n",
       "Military                      211        0    685\n",
       "Other                           0     5633      0\n",
       "Political System              121     2541   1542"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalition_regrouped.groupby(['topic'])['sentiment'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([manifesto_regrouped[['text','sentiment','topic', 'source']], coalition_regrouped[['text','sentiment','topic','source']]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[:,'topic_sentiment'] = final_df['topic'] + '_' + final_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d2bac5a9d14fd4b65e6babd8850b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/409042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551c792d2f864b8cbc2029eb838e6140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/409042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9484a3b47f754471bb68734a95ac85df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/409042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bafa86045d5419fb7f08eba29bdee78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/409042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset = Dataset.from_pandas(final_df)\n",
    "final_dataset = final_dataset.class_encode_column('sentiment')\n",
    "final_dataset = final_dataset.class_encode_column('topic')\n",
    "final_dataset = final_dataset.class_encode_column('topic_sentiment')\n",
    "final_dataset = final_dataset.class_encode_column('source')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = final_dataset.train_test_split(test_size=0.1, stratify_by_column='topic_sentiment', seed=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'sentiment', 'topic', 'source', 'topic_sentiment'],\n",
       "        num_rows: 368137\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'sentiment', 'topic', 'source', 'topic_sentiment'],\n",
       "        num_rows: 40905\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "final_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dea383599749019135e4f5c146b5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/368137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30dc428add748b6960190bed50fe9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': ['sentiment', 'topic', 'source', 'input_ids', 'attention_mask'],\n",
       " 'test': ['sentiment', 'topic', 'source', 'input_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = final_datasets.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text','topic_sentiment'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=16, shuffle=True, collate_fn = data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=16, shuffle=False, collate_fn = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(final_df['topic']))\n",
    "num_sentiments = len(set(final_df['sentiment']))\n",
    "model = ContextScalePrediction(roberta_model=model_name, num_topics=12, num_sentiments=3,lora=False, use_shared_attention=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "total_steps = len(train_dataloader)*n_epochs\n",
    "warmup = total_steps*0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) ## Recommended for LoRA. Without LoRA, can use 2e-5 instead.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.759544, elapsed=12.99s, remaining=2850.86s.\n",
      "Batch 200: loss=3.699653, elapsed=24.18s, remaining=2654.37s.\n",
      "Batch 300: loss=3.643533, elapsed=35.06s, remaining=2557.49s.\n",
      "Batch 400: loss=3.581381, elapsed=45.60s, remaining=2486.90s.\n",
      "Batch 500: loss=3.525105, elapsed=56.21s, remaining=2443.31s.\n",
      "Batch 600: loss=3.484221, elapsed=66.85s, remaining=2411.37s.\n",
      "Batch 700: loss=3.436211, elapsed=77.69s, remaining=2391.76s.\n",
      "Batch 800: loss=3.375388, elapsed=88.79s, remaining=2380.93s.\n",
      "Batch 900: loss=3.299624, elapsed=99.91s, remaining=2369.78s.\n",
      "Batch 1000: loss=3.218033, elapsed=110.86s, remaining=2355.73s.\n",
      "Batch 1100: loss=3.138289, elapsed=121.73s, remaining=2341.01s.\n",
      "Batch 1200: loss=3.064908, elapsed=132.28s, remaining=2320.46s.\n",
      "Batch 1300: loss=2.989242, elapsed=143.56s, remaining=2313.28s.\n",
      "Batch 1400: loss=2.923838, elapsed=154.63s, remaining=2302.43s.\n",
      "Batch 1500: loss=2.862984, elapsed=165.20s, remaining=2285.21s.\n",
      "Batch 1600: loss=2.808982, elapsed=175.79s, remaining=2268.94s.\n",
      "Batch 1700: loss=2.754743, elapsed=186.46s, remaining=2254.38s.\n",
      "Batch 1800: loss=2.704411, elapsed=197.03s, remaining=2239.09s.\n",
      "Batch 1900: loss=2.659259, elapsed=207.87s, remaining=2227.35s.\n",
      "Batch 2000: loss=2.615942, elapsed=218.87s, remaining=2217.01s.\n",
      "Batch 2100: loss=2.575106, elapsed=229.77s, remaining=2205.64s.\n",
      "Batch 2200: loss=2.535151, elapsed=240.39s, remaining=2191.89s.\n",
      "Batch 2300: loss=2.498281, elapsed=251.15s, remaining=2179.61s.\n",
      "Batch 2400: loss=2.467289, elapsed=262.06s, remaining=2168.67s.\n",
      "Batch 2500: loss=2.434969, elapsed=272.78s, remaining=2156.34s.\n",
      "Batch 2600: loss=2.404484, elapsed=283.59s, remaining=2144.80s.\n",
      "Batch 2700: loss=2.374126, elapsed=294.32s, remaining=2132.86s.\n",
      "Batch 2800: loss=2.346342, elapsed=304.77s, remaining=2119.02s.\n",
      "Batch 2900: loss=2.320825, elapsed=315.31s, remaining=2105.83s.\n",
      "Batch 3000: loss=2.296473, elapsed=325.93s, remaining=2093.49s.\n",
      "Batch 3100: loss=2.273851, elapsed=336.78s, remaining=2082.65s.\n",
      "Batch 3200: loss=2.250867, elapsed=347.25s, remaining=2069.35s.\n",
      "Batch 3300: loss=2.227443, elapsed=358.04s, remaining=2058.03s.\n",
      "Batch 3400: loss=2.207991, elapsed=368.81s, remaining=2046.74s.\n",
      "Batch 3500: loss=2.188302, elapsed=379.53s, remaining=2035.24s.\n",
      "Batch 3600: loss=2.168514, elapsed=390.12s, remaining=2023.03s.\n",
      "Batch 3700: loss=2.151863, elapsed=400.93s, remaining=2012.02s.\n",
      "Batch 3800: loss=2.135927, elapsed=411.63s, remaining=2000.45s.\n",
      "Batch 3900: loss=2.119284, elapsed=422.51s, remaining=1989.88s.\n",
      "Batch 4000: loss=2.102526, elapsed=433.07s, remaining=1977.75s.\n",
      "Batch 4100: loss=2.087435, elapsed=443.91s, remaining=1966.96s.\n",
      "Batch 4200: loss=2.072422, elapsed=454.59s, remaining=1955.39s.\n",
      "Batch 4300: loss=2.056559, elapsed=465.46s, remaining=1944.75s.\n",
      "Batch 4400: loss=2.040474, elapsed=476.37s, remaining=1934.33s.\n",
      "Batch 4500: loss=2.028842, elapsed=486.90s, remaining=1922.32s.\n",
      "Batch 4600: loss=2.015771, elapsed=497.89s, remaining=1912.06s.\n",
      "Batch 4700: loss=2.002369, elapsed=508.54s, remaining=1900.58s.\n",
      "Batch 4800: loss=1.989904, elapsed=519.00s, remaining=1888.52s.\n",
      "Batch 4900: loss=1.975805, elapsed=529.80s, remaining=1877.72s.\n",
      "Batch 5000: loss=1.964545, elapsed=540.56s, remaining=1866.79s.\n",
      "Batch 5100: loss=1.954178, elapsed=551.34s, remaining=1855.89s.\n",
      "Batch 5200: loss=1.944265, elapsed=562.29s, remaining=1845.58s.\n",
      "Batch 5300: loss=1.934898, elapsed=573.08s, remaining=1834.72s.\n",
      "Batch 5400: loss=1.924869, elapsed=583.56s, remaining=1822.82s.\n",
      "Batch 5500: loss=1.916900, elapsed=594.51s, remaining=1812.43s.\n",
      "Batch 5600: loss=1.908859, elapsed=605.32s, remaining=1801.59s.\n",
      "Batch 5700: loss=1.900587, elapsed=616.19s, remaining=1791.02s.\n",
      "Batch 5800: loss=1.892035, elapsed=626.97s, remaining=1780.19s.\n",
      "Batch 5900: loss=1.881681, elapsed=637.75s, remaining=1769.31s.\n",
      "Batch 6000: loss=1.873523, elapsed=648.50s, remaining=1758.32s.\n",
      "Batch 6100: loss=1.865906, elapsed=659.17s, remaining=1747.15s.\n",
      "Batch 6200: loss=1.858046, elapsed=670.21s, remaining=1736.99s.\n",
      "Batch 6300: loss=1.850167, elapsed=681.11s, remaining=1726.39s.\n",
      "Batch 6400: loss=1.843545, elapsed=691.87s, remaining=1715.45s.\n",
      "Batch 6500: loss=1.835131, elapsed=702.65s, remaining=1704.58s.\n",
      "Batch 6600: loss=1.828109, elapsed=713.16s, remaining=1693.11s.\n",
      "Batch 6700: loss=1.820445, elapsed=723.87s, remaining=1682.08s.\n",
      "Batch 6800: loss=1.814348, elapsed=734.63s, remaining=1671.22s.\n",
      "Batch 6900: loss=1.807883, elapsed=745.46s, remaining=1660.49s.\n",
      "Batch 7000: loss=1.800961, elapsed=756.25s, remaining=1649.69s.\n",
      "Batch 7100: loss=1.795161, elapsed=766.72s, remaining=1638.18s.\n",
      "Batch 7200: loss=1.788999, elapsed=777.44s, remaining=1627.23s.\n",
      "Batch 7300: loss=1.781841, elapsed=788.08s, remaining=1616.14s.\n",
      "Batch 7400: loss=1.776375, elapsed=798.86s, remaining=1605.34s.\n",
      "Batch 7500: loss=1.769920, elapsed=809.43s, remaining=1594.08s.\n",
      "Batch 7600: loss=1.763628, elapsed=820.11s, remaining=1582.99s.\n",
      "Batch 7700: loss=1.756519, elapsed=830.53s, remaining=1571.50s.\n",
      "Batch 7800: loss=1.750843, elapsed=841.24s, remaining=1560.58s.\n",
      "Batch 7900: loss=1.746195, elapsed=851.89s, remaining=1549.53s.\n",
      "Batch 8000: loss=1.740803, elapsed=862.56s, remaining=1538.56s.\n",
      "Batch 8100: loss=1.736208, elapsed=872.96s, remaining=1527.06s.\n",
      "Batch 8200: loss=1.731250, elapsed=884.14s, remaining=1516.99s.\n",
      "Batch 8300: loss=1.726560, elapsed=894.82s, remaining=1505.98s.\n",
      "Batch 8400: loss=1.721270, elapsed=905.47s, remaining=1494.89s.\n",
      "Batch 8500: loss=1.716508, elapsed=916.17s, remaining=1483.94s.\n",
      "Batch 8600: loss=1.711439, elapsed=927.03s, remaining=1473.26s.\n",
      "Batch 8700: loss=1.706397, elapsed=937.86s, remaining=1462.52s.\n",
      "Batch 8800: loss=1.702626, elapsed=948.36s, remaining=1451.33s.\n",
      "Batch 8900: loss=1.697798, elapsed=959.21s, remaining=1440.67s.\n",
      "Batch 9000: loss=1.693297, elapsed=970.10s, remaining=1430.09s.\n",
      "Batch 9100: loss=1.688740, elapsed=980.75s, remaining=1419.14s.\n",
      "Batch 9200: loss=1.685074, elapsed=991.33s, remaining=1408.06s.\n",
      "Batch 9300: loss=1.681610, elapsed=1002.54s, remaining=1397.92s.\n",
      "Batch 9400: loss=1.677700, elapsed=1012.94s, remaining=1386.62s.\n",
      "Batch 9500: loss=1.673559, elapsed=1023.67s, remaining=1375.79s.\n",
      "Batch 9600: loss=1.669854, elapsed=1034.40s, remaining=1364.96s.\n",
      "Batch 9700: loss=1.665833, elapsed=1045.10s, remaining=1354.11s.\n",
      "Batch 9800: loss=1.662418, elapsed=1056.05s, remaining=1343.58s.\n",
      "Batch 9900: loss=1.657865, elapsed=1067.21s, remaining=1333.32s.\n",
      "Batch 10000: loss=1.654001, elapsed=1077.94s, remaining=1322.51s.\n",
      "Batch 10100: loss=1.649981, elapsed=1088.42s, remaining=1311.39s.\n",
      "Batch 10200: loss=1.645855, elapsed=1099.22s, remaining=1300.67s.\n",
      "Batch 10300: loss=1.642340, elapsed=1110.12s, remaining=1290.02s.\n",
      "Batch 10400: loss=1.638550, elapsed=1121.10s, remaining=1279.52s.\n",
      "Batch 10500: loss=1.635140, elapsed=1132.28s, remaining=1269.19s.\n",
      "Batch 10600: loss=1.632079, elapsed=1142.93s, remaining=1258.28s.\n",
      "Batch 10700: loss=1.628375, elapsed=1153.72s, remaining=1247.52s.\n",
      "Batch 10800: loss=1.624671, elapsed=1164.26s, remaining=1236.51s.\n",
      "Batch 10900: loss=1.621026, elapsed=1174.79s, remaining=1225.49s.\n",
      "Batch 11000: loss=1.618098, elapsed=1185.33s, remaining=1214.52s.\n",
      "Batch 11100: loss=1.615116, elapsed=1195.82s, remaining=1203.48s.\n",
      "Batch 11200: loss=1.612133, elapsed=1206.25s, remaining=1192.40s.\n",
      "Batch 11300: loss=1.608536, elapsed=1217.04s, remaining=1181.69s.\n",
      "Batch 11400: loss=1.604696, elapsed=1227.53s, remaining=1170.65s.\n",
      "Batch 11500: loss=1.600799, elapsed=1238.05s, remaining=1159.68s.\n",
      "Batch 11600: loss=1.598177, elapsed=1248.69s, remaining=1148.82s.\n",
      "Batch 11700: loss=1.595810, elapsed=1259.23s, remaining=1137.87s.\n",
      "Batch 11800: loss=1.592899, elapsed=1269.69s, remaining=1126.86s.\n",
      "Batch 11900: loss=1.590115, elapsed=1280.37s, remaining=1116.07s.\n",
      "Batch 12000: loss=1.587233, elapsed=1291.24s, remaining=1105.44s.\n",
      "Batch 12100: loss=1.584256, elapsed=1301.97s, remaining=1094.69s.\n",
      "Batch 12200: loss=1.580881, elapsed=1312.38s, remaining=1083.66s.\n",
      "Batch 12300: loss=1.577793, elapsed=1322.92s, remaining=1072.75s.\n",
      "Batch 12400: loss=1.575259, elapsed=1333.30s, remaining=1061.73s.\n",
      "Batch 12500: loss=1.572330, elapsed=1344.04s, remaining=1050.99s.\n",
      "Batch 12600: loss=1.570005, elapsed=1354.49s, remaining=1040.03s.\n",
      "Batch 12700: loss=1.567424, elapsed=1365.05s, remaining=1029.16s.\n",
      "Batch 12800: loss=1.565024, elapsed=1375.50s, remaining=1018.23s.\n",
      "Batch 12900: loss=1.562236, elapsed=1386.05s, remaining=1007.36s.\n",
      "Batch 13000: loss=1.559033, elapsed=1396.48s, remaining=996.40s.\n",
      "Batch 13100: loss=1.556377, elapsed=1407.04s, remaining=985.55s.\n",
      "Batch 13200: loss=1.553629, elapsed=1417.43s, remaining=974.59s.\n",
      "Batch 13300: loss=1.551062, elapsed=1428.03s, remaining=963.77s.\n",
      "Batch 13400: loss=1.548246, elapsed=1438.80s, remaining=953.08s.\n",
      "Batch 13500: loss=1.545609, elapsed=1449.15s, remaining=942.09s.\n",
      "Batch 13600: loss=1.543639, elapsed=1459.56s, remaining=931.17s.\n",
      "Batch 13700: loss=1.540860, elapsed=1470.31s, remaining=920.47s.\n",
      "Batch 13800: loss=1.538109, elapsed=1480.60s, remaining=909.48s.\n",
      "Batch 13900: loss=1.535668, elapsed=1491.14s, remaining=898.63s.\n",
      "Batch 14000: loss=1.533435, elapsed=1501.70s, remaining=887.84s.\n",
      "Batch 14100: loss=1.531316, elapsed=1512.12s, remaining=876.95s.\n",
      "Batch 14200: loss=1.528857, elapsed=1522.82s, remaining=866.24s.\n",
      "Batch 14300: loss=1.526114, elapsed=1533.38s, remaining=855.43s.\n",
      "Batch 14400: loss=1.523824, elapsed=1543.68s, remaining=844.49s.\n",
      "Batch 14500: loss=1.521696, elapsed=1554.00s, remaining=833.57s.\n",
      "Batch 14600: loss=1.519041, elapsed=1564.55s, remaining=822.78s.\n",
      "Batch 14700: loss=1.516748, elapsed=1574.97s, remaining=811.93s.\n",
      "Batch 14800: loss=1.514625, elapsed=1585.53s, remaining=801.15s.\n",
      "Batch 14900: loss=1.512861, elapsed=1595.81s, remaining=790.23s.\n",
      "Batch 15000: loss=1.510778, elapsed=1606.43s, remaining=779.49s.\n",
      "Batch 15100: loss=1.508382, elapsed=1617.06s, remaining=768.77s.\n",
      "Batch 15200: loss=1.506858, elapsed=1627.62s, remaining=757.98s.\n",
      "Batch 15300: loss=1.504731, elapsed=1637.99s, remaining=747.13s.\n",
      "Batch 15400: loss=1.502832, elapsed=1648.34s, remaining=736.28s.\n",
      "Batch 15500: loss=1.500629, elapsed=1658.92s, remaining=725.54s.\n",
      "Batch 15600: loss=1.498176, elapsed=1669.44s, remaining=714.79s.\n",
      "Batch 15700: loss=1.496560, elapsed=1679.93s, remaining=704.01s.\n",
      "Batch 15800: loss=1.494636, elapsed=1690.59s, remaining=693.33s.\n",
      "Batch 15900: loss=1.492554, elapsed=1701.09s, remaining=682.56s.\n",
      "Batch 16000: loss=1.490601, elapsed=1711.54s, remaining=671.77s.\n",
      "Batch 16100: loss=1.488516, elapsed=1722.18s, remaining=661.07s.\n",
      "Batch 16200: loss=1.486560, elapsed=1732.91s, remaining=650.40s.\n",
      "Batch 16300: loss=1.484074, elapsed=1743.65s, remaining=639.73s.\n",
      "Batch 16400: loss=1.482546, elapsed=1753.99s, remaining=628.92s.\n",
      "Batch 16500: loss=1.480579, elapsed=1764.70s, remaining=618.25s.\n",
      "Batch 16600: loss=1.478533, elapsed=1775.29s, remaining=607.53s.\n",
      "Batch 16700: loss=1.476364, elapsed=1785.74s, remaining=596.75s.\n",
      "Batch 16800: loss=1.474669, elapsed=1796.14s, remaining=585.97s.\n",
      "Batch 16900: loss=1.472542, elapsed=1806.68s, remaining=575.24s.\n",
      "Batch 17000: loss=1.470674, elapsed=1817.04s, remaining=564.46s.\n",
      "Batch 17100: loss=1.468855, elapsed=1827.75s, remaining=553.78s.\n",
      "Batch 17200: loss=1.466748, elapsed=1838.67s, remaining=543.17s.\n",
      "Batch 17300: loss=1.464718, elapsed=1849.08s, remaining=532.40s.\n",
      "Batch 17400: loss=1.462930, elapsed=1859.90s, remaining=521.78s.\n",
      "Batch 17500: loss=1.460649, elapsed=1870.25s, remaining=511.00s.\n",
      "Batch 17600: loss=1.458328, elapsed=1880.99s, remaining=500.35s.\n",
      "Batch 17700: loss=1.456684, elapsed=1891.43s, remaining=489.60s.\n",
      "Batch 17800: loss=1.455081, elapsed=1901.95s, remaining=478.88s.\n",
      "Batch 17900: loss=1.453303, elapsed=1912.73s, remaining=468.23s.\n",
      "Batch 18000: loss=1.451454, elapsed=1923.52s, remaining=457.59s.\n",
      "Batch 18100: loss=1.449836, elapsed=1933.99s, remaining=446.85s.\n",
      "Batch 18200: loss=1.448095, elapsed=1944.76s, remaining=436.20s.\n",
      "Batch 18300: loss=1.445971, elapsed=1955.42s, remaining=425.52s.\n",
      "Batch 18400: loss=1.444392, elapsed=1965.89s, remaining=414.80s.\n",
      "Batch 18500: loss=1.442590, elapsed=1976.39s, remaining=404.08s.\n",
      "Batch 18600: loss=1.440949, elapsed=1986.96s, remaining=393.39s.\n",
      "Batch 18700: loss=1.438935, elapsed=1997.30s, remaining=382.65s.\n",
      "Batch 18800: loss=1.437083, elapsed=2007.95s, remaining=371.97s.\n",
      "Batch 18900: loss=1.435271, elapsed=2018.29s, remaining=361.22s.\n",
      "Batch 19000: loss=1.433460, elapsed=2028.59s, remaining=350.49s.\n",
      "Batch 19100: loss=1.431627, elapsed=2039.20s, remaining=339.81s.\n",
      "Batch 19200: loss=1.429876, elapsed=2049.53s, remaining=329.08s.\n",
      "Batch 19300: loss=1.427982, elapsed=2059.85s, remaining=318.35s.\n",
      "Batch 19400: loss=1.426500, elapsed=2070.59s, remaining=307.69s.\n",
      "Batch 19500: loss=1.424714, elapsed=2081.09s, remaining=297.00s.\n",
      "Batch 19600: loss=1.423287, elapsed=2091.59s, remaining=286.31s.\n",
      "Batch 19700: loss=1.421572, elapsed=2102.16s, remaining=275.64s.\n",
      "Batch 19800: loss=1.420270, elapsed=2112.62s, remaining=264.95s.\n",
      "Batch 19900: loss=1.418569, elapsed=2122.94s, remaining=254.24s.\n",
      "Batch 20000: loss=1.417065, elapsed=2133.30s, remaining=243.55s.\n",
      "Batch 20100: loss=1.415797, elapsed=2143.68s, remaining=232.86s.\n",
      "Batch 20200: loss=1.414260, elapsed=2153.98s, remaining=222.16s.\n",
      "Batch 20300: loss=1.412638, elapsed=2164.22s, remaining=211.45s.\n",
      "Batch 20400: loss=1.411449, elapsed=2174.63s, remaining=200.77s.\n",
      "Batch 20500: loss=1.409955, elapsed=2185.31s, remaining=190.11s.\n",
      "Batch 20600: loss=1.408395, elapsed=2195.79s, remaining=179.44s.\n",
      "Batch 20700: loss=1.406568, elapsed=2206.21s, remaining=168.77s.\n",
      "Batch 20800: loss=1.405224, elapsed=2217.09s, remaining=158.15s.\n",
      "Batch 20900: loss=1.403692, elapsed=2227.79s, remaining=147.50s.\n",
      "Batch 21000: loss=1.402060, elapsed=2238.46s, remaining=136.85s.\n",
      "Batch 21100: loss=1.400532, elapsed=2248.99s, remaining=126.20s.\n",
      "Batch 21200: loss=1.399181, elapsed=2259.48s, remaining=115.54s.\n",
      "Batch 21300: loss=1.397513, elapsed=2269.97s, remaining=104.88s.\n",
      "Batch 21400: loss=1.396411, elapsed=2280.64s, remaining=94.23s.\n",
      "Batch 21500: loss=1.395169, elapsed=2291.16s, remaining=83.57s.\n",
      "Batch 21600: loss=1.393663, elapsed=2301.72s, remaining=72.91s.\n",
      "Batch 21700: loss=1.391867, elapsed=2312.36s, remaining=62.27s.\n",
      "Batch 21800: loss=1.390602, elapsed=2322.84s, remaining=51.62s.\n",
      "Batch 21900: loss=1.388907, elapsed=2333.37s, remaining=40.97s.\n",
      "Batch 22000: loss=1.387333, elapsed=2343.89s, remaining=30.32s.\n",
      "Batch 22100: loss=1.385822, elapsed=2354.42s, remaining=19.67s.\n",
      "Batch 22200: loss=1.384308, elapsed=2364.93s, remaining=9.03s.\n",
      "Batch 22300: loss=1.383084, elapsed=2375.55s, remaining=-1.61s.\n",
      "Batch 22400: loss=1.381321, elapsed=2386.01s, remaining=-12.27s.\n",
      "Batch 22500: loss=1.379988, elapsed=2396.57s, remaining=-22.90s.\n",
      "Batch 22600: loss=1.378656, elapsed=2407.32s, remaining=-33.54s.\n",
      "Batch 22700: loss=1.377212, elapsed=2417.84s, remaining=-44.19s.\n",
      "Batch 22800: loss=1.375996, elapsed=2428.46s, remaining=-54.83s.\n",
      "Batch 22900: loss=1.374393, elapsed=2438.95s, remaining=-65.47s.\n",
      "Batch 23000: loss=1.373088, elapsed=2449.61s, remaining=-76.11s.\n",
      "\n",
      "Training epoch took: 2450.43s\n",
      "Epoch: 2\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.994417, elapsed=10.47s, remaining=2298.19s.\n",
      "Batch 200: loss=0.933383, elapsed=20.99s, remaining=2305.71s.\n",
      "Batch 300: loss=0.943394, elapsed=31.49s, remaining=2300.02s.\n",
      "Batch 400: loss=0.940459, elapsed=41.96s, remaining=2289.86s.\n",
      "Batch 500: loss=0.937274, elapsed=52.21s, remaining=2269.73s.\n",
      "Batch 600: loss=0.945408, elapsed=62.85s, remaining=2267.53s.\n",
      "Batch 700: loss=0.952134, elapsed=73.35s, remaining=2258.91s.\n",
      "Batch 800: loss=0.952764, elapsed=83.89s, remaining=2250.76s.\n",
      "Batch 900: loss=0.957014, elapsed=94.36s, remaining=2240.32s.\n",
      "Batch 1000: loss=0.957414, elapsed=104.85s, remaining=2230.15s.\n",
      "Batch 1100: loss=0.958582, elapsed=115.43s, remaining=2221.85s.\n",
      "Batch 1200: loss=0.958376, elapsed=126.01s, remaining=2213.16s.\n",
      "Batch 1300: loss=0.958982, elapsed=136.53s, remaining=2203.19s.\n",
      "Batch 1400: loss=0.958910, elapsed=147.18s, remaining=2195.14s.\n",
      "Batch 1500: loss=0.956583, elapsed=157.92s, remaining=2187.96s.\n",
      "Batch 1600: loss=0.954889, elapsed=168.38s, remaining=2176.64s.\n",
      "Batch 1700: loss=0.955112, elapsed=178.99s, remaining=2167.32s.\n",
      "Batch 1800: loss=0.953734, elapsed=189.63s, remaining=2158.16s.\n",
      "Batch 1900: loss=0.954643, elapsed=200.13s, remaining=2147.37s.\n",
      "Batch 2000: loss=0.950956, elapsed=210.52s, remaining=2135.39s.\n",
      "Batch 2100: loss=0.950555, elapsed=221.01s, remaining=2124.42s.\n",
      "Batch 2200: loss=0.948308, elapsed=231.32s, remaining=2111.96s.\n",
      "Batch 2300: loss=0.947580, elapsed=241.77s, remaining=2100.98s.\n",
      "Batch 2400: loss=0.946406, elapsed=252.46s, remaining=2092.13s.\n",
      "Batch 2500: loss=0.946736, elapsed=263.01s, remaining=2081.89s.\n",
      "Batch 2600: loss=0.947881, elapsed=273.59s, remaining=2071.85s.\n",
      "Batch 2700: loss=0.947757, elapsed=284.05s, remaining=2060.91s.\n",
      "Batch 2800: loss=0.948557, elapsed=294.53s, remaining=2050.09s.\n",
      "Batch 2900: loss=0.946552, elapsed=304.97s, remaining=2039.09s.\n",
      "Batch 3000: loss=0.945818, elapsed=315.40s, remaining=2028.04s.\n",
      "Batch 3100: loss=0.946137, elapsed=325.89s, remaining=2017.43s.\n",
      "Batch 3200: loss=0.944590, elapsed=336.19s, remaining=2005.60s.\n",
      "Batch 3300: loss=0.943013, elapsed=346.72s, remaining=1995.26s.\n",
      "Batch 3400: loss=0.942340, elapsed=357.37s, remaining=1985.58s.\n",
      "Batch 3500: loss=0.942595, elapsed=368.00s, remaining=1975.74s.\n",
      "Batch 3600: loss=0.944326, elapsed=378.30s, remaining=1964.08s.\n",
      "Batch 3700: loss=0.941728, elapsed=388.95s, remaining=1954.36s.\n",
      "Batch 3800: loss=0.940954, elapsed=399.49s, remaining=1944.05s.\n",
      "Batch 3900: loss=0.941772, elapsed=410.02s, remaining=1933.70s.\n",
      "Batch 4000: loss=0.942945, elapsed=420.71s, remaining=1923.99s.\n",
      "Batch 4100: loss=0.942631, elapsed=431.51s, remaining=1914.78s.\n",
      "Batch 4200: loss=0.942373, elapsed=441.98s, remaining=1904.03s.\n",
      "Batch 4300: loss=0.941766, elapsed=452.55s, remaining=1893.75s.\n",
      "Batch 4400: loss=0.941972, elapsed=463.31s, remaining=1884.18s.\n",
      "Batch 4500: loss=0.941976, elapsed=473.96s, remaining=1874.14s.\n",
      "Batch 4600: loss=0.941694, elapsed=484.44s, remaining=1863.40s.\n",
      "Batch 4700: loss=0.942397, elapsed=494.93s, remaining=1852.73s.\n",
      "Batch 4800: loss=0.941237, elapsed=505.39s, remaining=1841.97s.\n",
      "Batch 4900: loss=0.941963, elapsed=515.88s, remaining=1831.26s.\n",
      "Batch 5000: loss=0.942113, elapsed=526.16s, remaining=1819.88s.\n",
      "Batch 5100: loss=0.941827, elapsed=536.70s, remaining=1809.44s.\n",
      "Batch 5200: loss=0.942841, elapsed=547.15s, remaining=1798.65s.\n",
      "Batch 5300: loss=0.941696, elapsed=557.35s, remaining=1787.08s.\n",
      "Batch 5400: loss=0.942225, elapsed=567.91s, remaining=1776.74s.\n",
      "Batch 5500: loss=0.942188, elapsed=578.41s, remaining=1766.16s.\n",
      "Batch 5600: loss=0.942075, elapsed=589.12s, remaining=1756.23s.\n",
      "Batch 5700: loss=0.942061, elapsed=599.85s, remaining=1746.36s.\n",
      "Batch 5800: loss=0.941439, elapsed=610.43s, remaining=1736.03s.\n",
      "Batch 5900: loss=0.940780, elapsed=621.04s, remaining=1725.76s.\n",
      "Batch 6000: loss=0.940378, elapsed=631.49s, remaining=1714.99s.\n",
      "Batch 6100: loss=0.940362, elapsed=641.96s, remaining=1704.36s.\n",
      "Batch 6200: loss=0.939597, elapsed=652.37s, remaining=1693.55s.\n",
      "Batch 6300: loss=0.939709, elapsed=662.82s, remaining=1682.88s.\n",
      "Batch 6400: loss=0.939031, elapsed=673.36s, remaining=1672.40s.\n",
      "Batch 6500: loss=0.939147, elapsed=683.91s, remaining=1661.96s.\n",
      "Batch 6600: loss=0.939209, elapsed=694.42s, remaining=1651.39s.\n",
      "Batch 6700: loss=0.938561, elapsed=704.94s, remaining=1640.90s.\n",
      "Batch 6800: loss=0.937850, elapsed=715.51s, remaining=1630.47s.\n",
      "Batch 6900: loss=0.937384, elapsed=725.99s, remaining=1619.89s.\n",
      "Batch 7000: loss=0.937544, elapsed=736.65s, remaining=1609.66s.\n",
      "Batch 7100: loss=0.937003, elapsed=747.28s, remaining=1599.38s.\n",
      "Batch 7200: loss=0.937547, elapsed=757.62s, remaining=1588.46s.\n",
      "Batch 7300: loss=0.937216, elapsed=767.97s, remaining=1577.55s.\n",
      "Batch 7400: loss=0.937742, elapsed=778.67s, remaining=1567.38s.\n",
      "Batch 7500: loss=0.937988, elapsed=789.12s, remaining=1556.69s.\n",
      "Batch 7600: loss=0.937746, elapsed=799.84s, remaining=1546.55s.\n",
      "Batch 7700: loss=0.937791, elapsed=810.45s, remaining=1536.16s.\n",
      "Batch 7800: loss=0.937332, elapsed=820.75s, remaining=1525.19s.\n",
      "Batch 7900: loss=0.936957, elapsed=831.38s, remaining=1514.88s.\n",
      "Batch 8000: loss=0.936828, elapsed=842.01s, remaining=1504.55s.\n",
      "Batch 8100: loss=0.936189, elapsed=852.80s, remaining=1494.48s.\n",
      "Batch 8200: loss=0.935627, elapsed=863.30s, remaining=1483.90s.\n",
      "Batch 8300: loss=0.935104, elapsed=873.84s, remaining=1473.40s.\n",
      "Batch 8400: loss=0.935138, elapsed=884.30s, remaining=1462.76s.\n",
      "Batch 8500: loss=0.935036, elapsed=894.74s, remaining=1452.10s.\n",
      "Batch 8600: loss=0.935429, elapsed=905.31s, remaining=1441.63s.\n",
      "Batch 8700: loss=0.935092, elapsed=915.88s, remaining=1431.16s.\n",
      "Batch 8800: loss=0.934468, elapsed=926.37s, remaining=1420.57s.\n",
      "Batch 8900: loss=0.934070, elapsed=936.97s, remaining=1410.18s.\n",
      "Batch 9000: loss=0.933371, elapsed=947.62s, remaining=1399.83s.\n",
      "Batch 9100: loss=0.933316, elapsed=958.30s, remaining=1389.53s.\n",
      "Batch 9200: loss=0.933332, elapsed=968.86s, remaining=1379.05s.\n",
      "Batch 9300: loss=0.933693, elapsed=979.50s, remaining=1368.67s.\n",
      "Batch 9400: loss=0.933689, elapsed=990.10s, remaining=1358.25s.\n",
      "Batch 9500: loss=0.933341, elapsed=1000.73s, remaining=1347.85s.\n",
      "Batch 9600: loss=0.932821, elapsed=1011.27s, remaining=1337.34s.\n",
      "Batch 9700: loss=0.932013, elapsed=1021.90s, remaining=1326.93s.\n",
      "Batch 9800: loss=0.931356, elapsed=1032.41s, remaining=1316.34s.\n",
      "Batch 9900: loss=0.931126, elapsed=1042.90s, remaining=1305.75s.\n",
      "Batch 10000: loss=0.931104, elapsed=1053.62s, remaining=1295.45s.\n",
      "Batch 10100: loss=0.930522, elapsed=1064.12s, remaining=1284.84s.\n",
      "Batch 10200: loss=0.929761, elapsed=1074.67s, remaining=1274.33s.\n",
      "Batch 10300: loss=0.928954, elapsed=1085.23s, remaining=1263.83s.\n",
      "Batch 10400: loss=0.928935, elapsed=1095.78s, remaining=1253.33s.\n",
      "Batch 10500: loss=0.929019, elapsed=1106.48s, remaining=1242.97s.\n",
      "Batch 10600: loss=0.928158, elapsed=1117.10s, remaining=1232.53s.\n",
      "Batch 10700: loss=0.927787, elapsed=1127.69s, remaining=1222.04s.\n",
      "Batch 10800: loss=0.927278, elapsed=1138.32s, remaining=1211.59s.\n",
      "Batch 10900: loss=0.926828, elapsed=1148.70s, remaining=1200.88s.\n",
      "Batch 11000: loss=0.926909, elapsed=1159.13s, remaining=1190.21s.\n",
      "Batch 11100: loss=0.926209, elapsed=1169.58s, remaining=1179.59s.\n",
      "Batch 11200: loss=0.925815, elapsed=1180.17s, remaining=1169.10s.\n",
      "Batch 11300: loss=0.925969, elapsed=1190.59s, remaining=1158.45s.\n",
      "Batch 11400: loss=0.925669, elapsed=1201.23s, remaining=1148.02s.\n",
      "Batch 11500: loss=0.925346, elapsed=1212.03s, remaining=1137.76s.\n",
      "Batch 11600: loss=0.925134, elapsed=1222.51s, remaining=1127.16s.\n",
      "Batch 11700: loss=0.925209, elapsed=1233.21s, remaining=1116.77s.\n",
      "Batch 11800: loss=0.924992, elapsed=1243.80s, remaining=1106.28s.\n",
      "Batch 11900: loss=0.924546, elapsed=1254.26s, remaining=1095.67s.\n",
      "Batch 12000: loss=0.923974, elapsed=1264.88s, remaining=1085.22s.\n",
      "Batch 12100: loss=0.923262, elapsed=1275.47s, remaining=1074.74s.\n",
      "Batch 12200: loss=0.922671, elapsed=1285.97s, remaining=1064.16s.\n",
      "Batch 12300: loss=0.922141, elapsed=1296.50s, remaining=1053.60s.\n",
      "Batch 12400: loss=0.921952, elapsed=1306.91s, remaining=1042.96s.\n",
      "Batch 12500: loss=0.921663, elapsed=1317.26s, remaining=1032.28s.\n",
      "Batch 12600: loss=0.921660, elapsed=1327.51s, remaining=1021.51s.\n",
      "Batch 12700: loss=0.921314, elapsed=1337.98s, remaining=1010.93s.\n",
      "Batch 12800: loss=0.920438, elapsed=1348.62s, remaining=1000.47s.\n",
      "Batch 12900: loss=0.919758, elapsed=1359.00s, remaining=989.82s.\n",
      "Batch 13000: loss=0.919531, elapsed=1369.43s, remaining=979.21s.\n",
      "Batch 13100: loss=0.919275, elapsed=1379.77s, remaining=968.55s.\n",
      "Batch 13200: loss=0.918832, elapsed=1390.32s, remaining=958.02s.\n",
      "Batch 13300: loss=0.918368, elapsed=1401.40s, remaining=947.87s.\n",
      "Batch 13400: loss=0.918121, elapsed=1411.97s, remaining=937.36s.\n",
      "Batch 13500: loss=0.917498, elapsed=1422.48s, remaining=926.81s.\n",
      "Batch 13600: loss=0.917038, elapsed=1433.06s, remaining=916.32s.\n",
      "Batch 13700: loss=0.916562, elapsed=1443.82s, remaining=905.93s.\n",
      "Batch 13800: loss=0.916323, elapsed=1454.36s, remaining=895.40s.\n",
      "Batch 13900: loss=0.916216, elapsed=1464.69s, remaining=884.75s.\n",
      "Batch 14000: loss=0.915992, elapsed=1475.14s, remaining=874.16s.\n",
      "Batch 14100: loss=0.915694, elapsed=1485.86s, remaining=863.73s.\n",
      "Batch 14200: loss=0.915567, elapsed=1496.27s, remaining=853.12s.\n",
      "Batch 14300: loss=0.915112, elapsed=1506.94s, remaining=842.68s.\n",
      "Batch 14400: loss=0.914839, elapsed=1517.55s, remaining=832.19s.\n",
      "Batch 14500: loss=0.915013, elapsed=1528.07s, remaining=821.63s.\n",
      "Batch 14600: loss=0.914576, elapsed=1538.72s, remaining=811.15s.\n",
      "Batch 14700: loss=0.914773, elapsed=1549.14s, remaining=800.54s.\n",
      "Batch 14800: loss=0.914125, elapsed=1559.51s, remaining=789.92s.\n",
      "Batch 14900: loss=0.913486, elapsed=1570.29s, remaining=779.50s.\n",
      "Batch 15000: loss=0.913136, elapsed=1581.40s, remaining=769.22s.\n",
      "Batch 15100: loss=0.913158, elapsed=1591.94s, remaining=758.66s.\n",
      "Batch 15200: loss=0.913264, elapsed=1602.52s, remaining=748.12s.\n",
      "Batch 15300: loss=0.912809, elapsed=1613.17s, remaining=737.59s.\n",
      "Batch 15400: loss=0.912899, elapsed=1623.79s, remaining=727.05s.\n",
      "Batch 15500: loss=0.912780, elapsed=1634.27s, remaining=716.47s.\n",
      "Batch 15600: loss=0.912438, elapsed=1644.84s, remaining=705.92s.\n",
      "Batch 15700: loss=0.911973, elapsed=1655.32s, remaining=695.32s.\n",
      "Batch 15800: loss=0.911882, elapsed=1665.97s, remaining=684.81s.\n",
      "Batch 15900: loss=0.911688, elapsed=1676.51s, remaining=674.26s.\n",
      "Batch 16000: loss=0.911303, elapsed=1687.08s, remaining=663.71s.\n",
      "Batch 16100: loss=0.911009, elapsed=1697.58s, remaining=653.10s.\n",
      "Batch 16200: loss=0.910711, elapsed=1708.11s, remaining=642.52s.\n",
      "Batch 16300: loss=0.910324, elapsed=1718.84s, remaining=632.03s.\n",
      "Batch 16400: loss=0.910431, elapsed=1729.48s, remaining=621.52s.\n",
      "Batch 16500: loss=0.910532, elapsed=1740.20s, remaining=611.02s.\n",
      "Batch 16600: loss=0.910195, elapsed=1750.79s, remaining=600.47s.\n",
      "Batch 16700: loss=0.909837, elapsed=1761.49s, remaining=589.95s.\n",
      "Batch 16800: loss=0.909856, elapsed=1772.11s, remaining=579.40s.\n",
      "Batch 16900: loss=0.909720, elapsed=1782.83s, remaining=568.89s.\n",
      "Batch 17000: loss=0.909224, elapsed=1793.64s, remaining=558.39s.\n",
      "Batch 17100: loss=0.908822, elapsed=1804.18s, remaining=547.83s.\n",
      "Batch 17200: loss=0.908718, elapsed=1814.64s, remaining=537.25s.\n",
      "Batch 17300: loss=0.908699, elapsed=1825.58s, remaining=526.81s.\n",
      "Batch 17400: loss=0.907824, elapsed=1836.11s, remaining=516.24s.\n",
      "Batch 17500: loss=0.907711, elapsed=1846.93s, remaining=505.75s.\n",
      "Batch 17600: loss=0.907361, elapsed=1857.58s, remaining=495.21s.\n",
      "Batch 17700: loss=0.907105, elapsed=1868.40s, remaining=484.71s.\n",
      "Batch 17800: loss=0.906814, elapsed=1879.02s, remaining=474.17s.\n",
      "Batch 17900: loss=0.906291, elapsed=1890.10s, remaining=463.74s.\n",
      "Batch 18000: loss=0.905563, elapsed=1900.84s, remaining=453.20s.\n",
      "Batch 18100: loss=0.905439, elapsed=1911.71s, remaining=442.68s.\n",
      "Batch 18200: loss=0.904893, elapsed=1922.86s, remaining=432.23s.\n",
      "Batch 18300: loss=0.904935, elapsed=1933.56s, remaining=421.68s.\n",
      "Batch 18400: loss=0.904488, elapsed=1944.51s, remaining=411.19s.\n",
      "Batch 18500: loss=0.904293, elapsed=1955.26s, remaining=400.66s.\n",
      "Batch 18600: loss=0.904297, elapsed=1966.18s, remaining=390.15s.\n",
      "Batch 18700: loss=0.903947, elapsed=1976.75s, remaining=379.56s.\n",
      "Batch 18800: loss=0.903524, elapsed=1987.92s, remaining=369.07s.\n",
      "Batch 18900: loss=0.903381, elapsed=1998.88s, remaining=358.54s.\n",
      "Batch 19000: loss=0.903279, elapsed=2009.68s, remaining=347.99s.\n",
      "Batch 19100: loss=0.902901, elapsed=2020.74s, remaining=337.48s.\n",
      "Batch 19200: loss=0.902603, elapsed=2031.69s, remaining=326.93s.\n",
      "Batch 19300: loss=0.902558, elapsed=2042.59s, remaining=316.39s.\n",
      "Batch 19400: loss=0.901966, elapsed=2053.18s, remaining=305.79s.\n",
      "Batch 19500: loss=0.901730, elapsed=2063.86s, remaining=295.22s.\n",
      "Batch 19600: loss=0.901718, elapsed=2074.37s, remaining=284.62s.\n",
      "Batch 19700: loss=0.901325, elapsed=2085.14s, remaining=274.00s.\n",
      "Batch 19800: loss=0.901381, elapsed=2095.57s, remaining=263.38s.\n",
      "Batch 19900: loss=0.901198, elapsed=2106.04s, remaining=252.75s.\n",
      "Batch 20000: loss=0.900847, elapsed=2116.75s, remaining=242.16s.\n",
      "Batch 20100: loss=0.900329, elapsed=2127.52s, remaining=231.58s.\n",
      "Batch 20200: loss=0.900044, elapsed=2138.22s, remaining=221.00s.\n",
      "Batch 20300: loss=0.899906, elapsed=2148.83s, remaining=210.40s.\n",
      "Batch 20400: loss=0.899706, elapsed=2159.84s, remaining=199.84s.\n",
      "Batch 20500: loss=0.899372, elapsed=2170.63s, remaining=189.25s.\n",
      "Batch 20600: loss=0.899411, elapsed=2181.44s, remaining=178.66s.\n",
      "Batch 20700: loss=0.899220, elapsed=2192.11s, remaining=168.05s.\n",
      "Batch 20800: loss=0.898938, elapsed=2203.11s, remaining=157.46s.\n",
      "Batch 20900: loss=0.898997, elapsed=2213.69s, remaining=146.85s.\n",
      "Batch 21000: loss=0.898313, elapsed=2224.45s, remaining=136.25s.\n",
      "Batch 21100: loss=0.897771, elapsed=2235.42s, remaining=125.65s.\n",
      "Batch 21200: loss=0.897439, elapsed=2246.44s, remaining=115.04s.\n",
      "Batch 21300: loss=0.897047, elapsed=2257.13s, remaining=104.44s.\n",
      "Batch 21400: loss=0.896890, elapsed=2267.81s, remaining=93.81s.\n",
      "Batch 21500: loss=0.896714, elapsed=2278.44s, remaining=83.17s.\n",
      "Batch 21600: loss=0.896628, elapsed=2289.55s, remaining=72.55s.\n",
      "Batch 21700: loss=0.896407, elapsed=2300.21s, remaining=61.95s.\n",
      "Batch 21800: loss=0.896184, elapsed=2311.12s, remaining=51.34s.\n",
      "Batch 21900: loss=0.895634, elapsed=2322.08s, remaining=40.76s.\n",
      "Batch 22000: loss=0.895301, elapsed=2332.70s, remaining=30.13s.\n",
      "Batch 22100: loss=0.894863, elapsed=2343.57s, remaining=19.50s.\n",
      "Batch 22200: loss=0.894548, elapsed=2354.03s, remaining=8.89s.\n",
      "Batch 22300: loss=0.894190, elapsed=2364.96s, remaining=-1.72s.\n",
      "Batch 22400: loss=0.894006, elapsed=2376.02s, remaining=-12.34s.\n",
      "Batch 22500: loss=0.893867, elapsed=2387.13s, remaining=-22.97s.\n",
      "Batch 22600: loss=0.893528, elapsed=2397.75s, remaining=-33.59s.\n",
      "Batch 22700: loss=0.893115, elapsed=2408.83s, remaining=-44.21s.\n",
      "Batch 22800: loss=0.892766, elapsed=2419.95s, remaining=-54.85s.\n",
      "Batch 22900: loss=0.892577, elapsed=2430.87s, remaining=-65.46s.\n",
      "Batch 23000: loss=0.892328, elapsed=2441.22s, remaining=-76.07s.\n",
      "\n",
      "Training epoch took: 2442.06s\n",
      "Epoch: 3\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.683817, elapsed=10.73s, remaining=2357.59s.\n",
      "Batch 200: loss=0.664538, elapsed=21.04s, remaining=2312.85s.\n",
      "Batch 300: loss=0.665251, elapsed=31.59s, remaining=2308.12s.\n",
      "Batch 400: loss=0.656819, elapsed=42.11s, remaining=2299.12s.\n",
      "Batch 500: loss=0.657663, elapsed=52.47s, remaining=2282.60s.\n",
      "Batch 600: loss=0.653931, elapsed=62.96s, remaining=2272.52s.\n",
      "Batch 700: loss=0.663971, elapsed=73.38s, remaining=2260.49s.\n",
      "Batch 800: loss=0.673734, elapsed=83.91s, remaining=2251.90s.\n",
      "Batch 900: loss=0.675963, elapsed=94.35s, remaining=2240.48s.\n",
      "Batch 1000: loss=0.682242, elapsed=105.04s, remaining=2234.70s.\n",
      "Batch 1100: loss=0.678457, elapsed=115.62s, remaining=2225.70s.\n",
      "Batch 1200: loss=0.682489, elapsed=126.00s, remaining=2213.12s.\n",
      "Batch 1300: loss=0.684827, elapsed=136.66s, remaining=2205.41s.\n",
      "Batch 1400: loss=0.683348, elapsed=147.38s, remaining=2198.29s.\n",
      "Batch 1500: loss=0.684424, elapsed=157.81s, remaining=2186.49s.\n",
      "Batch 1600: loss=0.679867, elapsed=168.31s, remaining=2175.66s.\n",
      "Batch 1700: loss=0.679460, elapsed=178.97s, remaining=2166.92s.\n",
      "Batch 1800: loss=0.675755, elapsed=189.43s, remaining=2155.67s.\n",
      "Batch 1900: loss=0.677752, elapsed=200.25s, remaining=2148.25s.\n",
      "Batch 2000: loss=0.678814, elapsed=211.26s, remaining=2142.20s.\n",
      "Batch 2100: loss=0.681331, elapsed=221.90s, remaining=2132.07s.\n",
      "Batch 2200: loss=0.680705, elapsed=232.63s, remaining=2122.89s.\n",
      "Batch 2300: loss=0.683281, elapsed=243.22s, remaining=2112.18s.\n",
      "Batch 2400: loss=0.682312, elapsed=253.73s, remaining=2101.02s.\n",
      "Batch 2500: loss=0.682101, elapsed=264.21s, remaining=2089.56s.\n",
      "Batch 2600: loss=0.683126, elapsed=274.64s, remaining=2077.87s.\n",
      "Batch 2700: loss=0.683738, elapsed=285.03s, remaining=2066.03s.\n",
      "Batch 2800: loss=0.682179, elapsed=295.57s, remaining=2055.24s.\n",
      "Batch 2900: loss=0.681740, elapsed=306.16s, remaining=2044.93s.\n",
      "Batch 3000: loss=0.683038, elapsed=316.58s, remaining=2033.48s.\n",
      "Batch 3100: loss=0.683849, elapsed=327.44s, remaining=2024.74s.\n",
      "Batch 3200: loss=0.683166, elapsed=337.92s, remaining=2013.75s.\n",
      "Batch 3300: loss=0.681405, elapsed=348.47s, remaining=2003.16s.\n",
      "Batch 3400: loss=0.680705, elapsed=359.15s, remaining=1993.24s.\n",
      "Batch 3500: loss=0.680886, elapsed=369.41s, remaining=1981.10s.\n",
      "Batch 3600: loss=0.680460, elapsed=379.78s, remaining=1969.58s.\n",
      "Batch 3700: loss=0.679119, elapsed=390.44s, remaining=1959.63s.\n",
      "Batch 3800: loss=0.678671, elapsed=401.01s, remaining=1949.19s.\n",
      "Batch 3900: loss=0.677191, elapsed=411.57s, remaining=1938.63s.\n",
      "Batch 4000: loss=0.677626, elapsed=422.10s, remaining=1927.98s.\n",
      "Batch 4100: loss=0.679192, elapsed=432.91s, remaining=1918.43s.\n",
      "Batch 4200: loss=0.678794, elapsed=443.36s, remaining=1907.42s.\n",
      "Batch 4300: loss=0.677653, elapsed=453.93s, remaining=1896.90s.\n",
      "Batch 4400: loss=0.677960, elapsed=464.38s, remaining=1885.90s.\n",
      "Batch 4500: loss=0.677581, elapsed=475.05s, remaining=1875.78s.\n",
      "Batch 4600: loss=0.676402, elapsed=485.62s, remaining=1865.28s.\n",
      "Batch 4700: loss=0.677065, elapsed=496.08s, remaining=1854.39s.\n",
      "Batch 4800: loss=0.677029, elapsed=506.71s, remaining=1844.08s.\n",
      "Batch 4900: loss=0.677073, elapsed=517.07s, remaining=1832.89s.\n",
      "Batch 5000: loss=0.676333, elapsed=527.62s, remaining=1822.30s.\n",
      "Batch 5100: loss=0.677012, elapsed=538.40s, remaining=1812.44s.\n",
      "Batch 5200: loss=0.676773, elapsed=548.93s, remaining=1801.77s.\n",
      "Batch 5300: loss=0.677410, elapsed=559.44s, remaining=1791.09s.\n",
      "Batch 5400: loss=0.676629, elapsed=569.94s, remaining=1780.36s.\n",
      "Batch 5500: loss=0.675839, elapsed=580.33s, remaining=1769.25s.\n",
      "Batch 5600: loss=0.675127, elapsed=590.91s, remaining=1758.78s.\n",
      "Batch 5700: loss=0.674986, elapsed=601.36s, remaining=1747.91s.\n",
      "Batch 5800: loss=0.674336, elapsed=611.64s, remaining=1736.56s.\n",
      "Batch 5900: loss=0.673959, elapsed=622.51s, remaining=1726.91s.\n",
      "Batch 6000: loss=0.673509, elapsed=632.96s, remaining=1716.12s.\n",
      "Batch 6100: loss=0.674277, elapsed=643.46s, remaining=1705.41s.\n",
      "Batch 6200: loss=0.674374, elapsed=654.03s, remaining=1694.94s.\n",
      "Batch 6300: loss=0.674609, elapsed=664.41s, remaining=1683.96s.\n",
      "Batch 6400: loss=0.674449, elapsed=675.61s, remaining=1675.13s.\n",
      "Batch 6500: loss=0.674224, elapsed=685.97s, remaining=1664.01s.\n",
      "Batch 6600: loss=0.673830, elapsed=696.50s, remaining=1653.38s.\n",
      "Batch 6700: loss=0.673026, elapsed=707.09s, remaining=1642.88s.\n",
      "Batch 6800: loss=0.673455, elapsed=717.64s, remaining=1632.26s.\n",
      "Batch 6900: loss=0.672943, elapsed=728.24s, remaining=1621.70s.\n",
      "Batch 7000: loss=0.672222, elapsed=738.91s, remaining=1611.40s.\n",
      "Batch 7100: loss=0.671345, elapsed=749.43s, remaining=1600.75s.\n",
      "Batch 7200: loss=0.671153, elapsed=759.75s, remaining=1589.68s.\n",
      "Batch 7300: loss=0.670565, elapsed=770.02s, remaining=1578.57s.\n",
      "Batch 7400: loss=0.670869, elapsed=780.59s, remaining=1568.01s.\n",
      "Batch 7500: loss=0.670431, elapsed=790.98s, remaining=1557.18s.\n",
      "Batch 7600: loss=0.670061, elapsed=801.51s, remaining=1546.63s.\n",
      "Batch 7700: loss=0.670109, elapsed=812.12s, remaining=1536.21s.\n",
      "Batch 7800: loss=0.670893, elapsed=822.77s, remaining=1525.89s.\n",
      "Batch 7900: loss=0.670285, elapsed=833.09s, remaining=1514.96s.\n",
      "Batch 8000: loss=0.669760, elapsed=844.16s, remaining=1505.28s.\n",
      "Batch 8100: loss=0.669513, elapsed=854.64s, remaining=1494.61s.\n",
      "Batch 8200: loss=0.669736, elapsed=865.30s, remaining=1484.26s.\n",
      "Batch 8300: loss=0.669493, elapsed=875.61s, remaining=1473.29s.\n",
      "Batch 8400: loss=0.670120, elapsed=886.19s, remaining=1462.81s.\n",
      "Batch 8500: loss=0.669041, elapsed=897.12s, remaining=1452.87s.\n",
      "Batch 8600: loss=0.669096, elapsed=907.79s, remaining=1442.51s.\n",
      "Batch 8700: loss=0.668756, elapsed=918.15s, remaining=1431.66s.\n",
      "Batch 8800: loss=0.669257, elapsed=928.59s, remaining=1420.94s.\n",
      "Batch 8900: loss=0.668884, elapsed=938.76s, remaining=1409.78s.\n",
      "Batch 9000: loss=0.668559, elapsed=949.34s, remaining=1399.26s.\n",
      "Batch 9100: loss=0.668416, elapsed=959.76s, remaining=1388.54s.\n",
      "Batch 9200: loss=0.668699, elapsed=970.44s, remaining=1378.19s.\n",
      "Batch 9300: loss=0.668623, elapsed=981.23s, remaining=1367.98s.\n",
      "Batch 9400: loss=0.668162, elapsed=991.77s, remaining=1357.43s.\n",
      "Batch 9500: loss=0.668656, elapsed=1002.25s, remaining=1346.78s.\n",
      "Batch 9600: loss=0.668826, elapsed=1013.01s, remaining=1336.46s.\n",
      "Batch 9700: loss=0.668584, elapsed=1023.81s, remaining=1326.20s.\n",
      "Batch 9800: loss=0.668206, elapsed=1034.54s, remaining=1315.89s.\n",
      "Batch 9900: loss=0.668715, elapsed=1045.14s, remaining=1305.41s.\n",
      "Batch 10000: loss=0.668856, elapsed=1055.86s, remaining=1295.07s.\n",
      "Batch 10100: loss=0.668733, elapsed=1066.56s, remaining=1284.69s.\n",
      "Batch 10200: loss=0.668632, elapsed=1077.34s, remaining=1274.39s.\n",
      "Batch 10300: loss=0.668641, elapsed=1087.97s, remaining=1263.95s.\n",
      "Batch 10400: loss=0.668140, elapsed=1098.41s, remaining=1253.23s.\n",
      "Batch 10500: loss=0.668930, elapsed=1109.23s, remaining=1242.98s.\n",
      "Batch 10600: loss=0.668859, elapsed=1119.64s, remaining=1232.27s.\n",
      "Batch 10700: loss=0.668554, elapsed=1130.24s, remaining=1221.75s.\n",
      "Batch 10800: loss=0.668341, elapsed=1141.22s, remaining=1211.63s.\n",
      "Batch 10900: loss=0.667774, elapsed=1151.72s, remaining=1200.97s.\n",
      "Batch 11000: loss=0.667657, elapsed=1162.04s, remaining=1190.14s.\n",
      "Batch 11100: loss=0.667460, elapsed=1172.41s, remaining=1179.38s.\n",
      "Batch 11200: loss=0.667528, elapsed=1182.65s, remaining=1168.51s.\n",
      "Batch 11300: loss=0.667694, elapsed=1193.24s, remaining=1157.99s.\n",
      "Batch 11400: loss=0.667353, elapsed=1203.65s, remaining=1147.26s.\n",
      "Batch 11500: loss=0.667122, elapsed=1214.03s, remaining=1136.53s.\n",
      "Batch 11600: loss=0.666884, elapsed=1224.69s, remaining=1126.09s.\n",
      "Batch 11700: loss=0.667294, elapsed=1235.10s, remaining=1115.39s.\n",
      "Batch 11800: loss=0.667235, elapsed=1245.41s, remaining=1104.61s.\n",
      "Batch 11900: loss=0.666988, elapsed=1255.82s, remaining=1093.90s.\n",
      "Batch 12000: loss=0.666787, elapsed=1266.60s, remaining=1083.56s.\n",
      "Batch 12100: loss=0.666700, elapsed=1276.94s, remaining=1072.83s.\n",
      "Batch 12200: loss=0.666728, elapsed=1287.39s, remaining=1062.20s.\n",
      "Batch 12300: loss=0.666600, elapsed=1297.98s, remaining=1051.69s.\n",
      "Batch 12400: loss=0.666609, elapsed=1308.42s, remaining=1041.02s.\n",
      "Batch 12500: loss=0.666532, elapsed=1319.25s, remaining=1030.64s.\n",
      "Batch 12600: loss=0.666448, elapsed=1329.89s, remaining=1020.15s.\n",
      "Batch 12700: loss=0.666769, elapsed=1340.52s, remaining=1009.61s.\n",
      "Batch 12800: loss=0.666604, elapsed=1351.01s, remaining=998.96s.\n",
      "Batch 12900: loss=0.666508, elapsed=1361.51s, remaining=988.35s.\n",
      "Batch 13000: loss=0.666363, elapsed=1372.36s, remaining=977.96s.\n",
      "Batch 13100: loss=0.665921, elapsed=1383.35s, remaining=967.71s.\n",
      "Batch 13200: loss=0.666116, elapsed=1394.06s, remaining=957.26s.\n",
      "Batch 13300: loss=0.665668, elapsed=1405.03s, remaining=946.96s.\n",
      "Batch 13400: loss=0.665346, elapsed=1415.87s, remaining=936.60s.\n",
      "Batch 13500: loss=0.665190, elapsed=1426.69s, remaining=926.20s.\n",
      "Batch 13600: loss=0.665170, elapsed=1437.49s, remaining=915.77s.\n",
      "Batch 13700: loss=0.664786, elapsed=1448.29s, remaining=905.28s.\n",
      "Batch 13800: loss=0.664657, elapsed=1459.37s, remaining=895.02s.\n",
      "Batch 13900: loss=0.664394, elapsed=1470.05s, remaining=884.53s.\n",
      "Batch 14000: loss=0.664525, elapsed=1480.80s, remaining=874.07s.\n",
      "Batch 14100: loss=0.664160, elapsed=1491.91s, remaining=863.80s.\n",
      "Batch 14200: loss=0.664297, elapsed=1502.80s, remaining=853.40s.\n",
      "Batch 14300: loss=0.664088, elapsed=1513.60s, remaining=842.90s.\n",
      "Batch 14400: loss=0.664064, elapsed=1524.28s, remaining=832.36s.\n",
      "Batch 14500: loss=0.663497, elapsed=1535.02s, remaining=821.84s.\n",
      "Batch 14600: loss=0.663613, elapsed=1545.85s, remaining=811.39s.\n",
      "Batch 14700: loss=0.663442, elapsed=1556.81s, remaining=800.99s.\n",
      "Batch 14800: loss=0.663084, elapsed=1567.97s, remaining=790.66s.\n",
      "Batch 14900: loss=0.663091, elapsed=1578.94s, remaining=780.28s.\n",
      "Batch 15000: loss=0.662817, elapsed=1589.53s, remaining=769.67s.\n",
      "Batch 15100: loss=0.662834, elapsed=1600.34s, remaining=759.16s.\n",
      "Batch 15200: loss=0.662690, elapsed=1611.38s, remaining=748.73s.\n",
      "Batch 15300: loss=0.662727, elapsed=1621.95s, remaining=738.04s.\n",
      "Batch 15400: loss=0.662425, elapsed=1632.52s, remaining=727.40s.\n",
      "Batch 15500: loss=0.662780, elapsed=1642.78s, remaining=716.65s.\n",
      "Batch 15600: loss=0.662218, elapsed=1653.46s, remaining=706.09s.\n",
      "Batch 15700: loss=0.662128, elapsed=1663.96s, remaining=695.43s.\n",
      "Batch 15800: loss=0.662207, elapsed=1674.91s, remaining=684.93s.\n",
      "Batch 15900: loss=0.662318, elapsed=1685.76s, remaining=674.42s.\n",
      "Batch 16000: loss=0.662377, elapsed=1696.50s, remaining=663.88s.\n",
      "Batch 16100: loss=0.662467, elapsed=1707.30s, remaining=653.34s.\n",
      "Batch 16200: loss=0.662378, elapsed=1718.27s, remaining=642.84s.\n",
      "Batch 16300: loss=0.662073, elapsed=1729.23s, remaining=632.32s.\n",
      "Batch 16400: loss=0.661827, elapsed=1740.22s, remaining=621.81s.\n",
      "Batch 16500: loss=0.661740, elapsed=1751.00s, remaining=611.22s.\n",
      "Batch 16600: loss=0.661616, elapsed=1761.63s, remaining=600.60s.\n",
      "Batch 16700: loss=0.661559, elapsed=1772.65s, remaining=590.11s.\n",
      "Batch 16800: loss=0.661732, elapsed=1783.64s, remaining=579.60s.\n",
      "Batch 16900: loss=0.661435, elapsed=1794.62s, remaining=569.08s.\n",
      "Batch 17000: loss=0.661232, elapsed=1805.42s, remaining=558.54s.\n",
      "Batch 17100: loss=0.660680, elapsed=1816.21s, remaining=547.99s.\n",
      "Batch 17200: loss=0.660410, elapsed=1826.97s, remaining=537.39s.\n",
      "Batch 17300: loss=0.660254, elapsed=1837.96s, remaining=526.88s.\n",
      "Batch 17400: loss=0.659937, elapsed=1848.55s, remaining=516.23s.\n",
      "Batch 17500: loss=0.659305, elapsed=1859.62s, remaining=505.69s.\n",
      "Batch 17600: loss=0.659477, elapsed=1870.48s, remaining=495.10s.\n",
      "Batch 17700: loss=0.659219, elapsed=1881.45s, remaining=484.53s.\n",
      "Batch 17800: loss=0.658909, elapsed=1892.45s, remaining=474.01s.\n",
      "Batch 17900: loss=0.658935, elapsed=1903.41s, remaining=463.45s.\n",
      "Batch 18000: loss=0.658730, elapsed=1914.22s, remaining=452.86s.\n",
      "Batch 18100: loss=0.658504, elapsed=1925.06s, remaining=442.29s.\n",
      "Batch 18200: loss=0.658334, elapsed=1936.13s, remaining=431.74s.\n",
      "Batch 18300: loss=0.658498, elapsed=1947.01s, remaining=421.15s.\n",
      "Batch 18400: loss=0.658442, elapsed=1957.79s, remaining=410.53s.\n",
      "Batch 18500: loss=0.658397, elapsed=1968.84s, remaining=399.95s.\n",
      "Batch 18600: loss=0.658311, elapsed=1979.44s, remaining=389.27s.\n",
      "Batch 18700: loss=0.658285, elapsed=1990.36s, remaining=378.66s.\n",
      "Batch 18800: loss=0.657867, elapsed=2000.80s, remaining=367.99s.\n",
      "Batch 18900: loss=0.657664, elapsed=2011.27s, remaining=357.32s.\n",
      "Batch 19000: loss=0.657371, elapsed=2022.13s, remaining=346.75s.\n",
      "Batch 19100: loss=0.656999, elapsed=2032.77s, remaining=336.10s.\n",
      "Batch 19200: loss=0.657089, elapsed=2043.41s, remaining=325.45s.\n",
      "Batch 19300: loss=0.656809, elapsed=2054.52s, remaining=314.87s.\n",
      "Batch 19400: loss=0.656424, elapsed=2064.96s, remaining=304.19s.\n",
      "Batch 19500: loss=0.656146, elapsed=2075.31s, remaining=293.50s.\n",
      "Batch 19600: loss=0.656245, elapsed=2085.92s, remaining=282.85s.\n",
      "Batch 19700: loss=0.656039, elapsed=2096.88s, remaining=272.21s.\n",
      "Batch 19800: loss=0.655788, elapsed=2107.55s, remaining=261.56s.\n",
      "Batch 19900: loss=0.655901, elapsed=2118.03s, remaining=250.90s.\n",
      "Batch 20000: loss=0.655751, elapsed=2128.66s, remaining=240.25s.\n",
      "Batch 20100: loss=0.655561, elapsed=2139.36s, remaining=229.60s.\n",
      "Batch 20200: loss=0.655304, elapsed=2150.01s, remaining=218.97s.\n",
      "Batch 20300: loss=0.655302, elapsed=2160.55s, remaining=208.31s.\n",
      "Batch 20400: loss=0.654967, elapsed=2171.16s, remaining=197.65s.\n",
      "Batch 20500: loss=0.655078, elapsed=2181.80s, remaining=187.01s.\n",
      "Batch 20600: loss=0.654827, elapsed=2192.53s, remaining=176.39s.\n",
      "Batch 20700: loss=0.654618, elapsed=2203.14s, remaining=165.72s.\n",
      "Batch 20800: loss=0.654607, elapsed=2213.61s, remaining=155.07s.\n",
      "Batch 20900: loss=0.654398, elapsed=2223.94s, remaining=144.42s.\n",
      "Batch 21000: loss=0.654573, elapsed=2234.51s, remaining=133.77s.\n",
      "Batch 21100: loss=0.654702, elapsed=2244.95s, remaining=123.13s.\n",
      "Batch 21200: loss=0.654593, elapsed=2255.64s, remaining=112.49s.\n",
      "Batch 21300: loss=0.654340, elapsed=2266.13s, remaining=101.86s.\n",
      "Batch 21400: loss=0.654262, elapsed=2276.57s, remaining=91.21s.\n",
      "Batch 21500: loss=0.653877, elapsed=2286.95s, remaining=80.56s.\n",
      "Batch 21600: loss=0.653611, elapsed=2297.44s, remaining=69.93s.\n",
      "Batch 21700: loss=0.653667, elapsed=2307.91s, remaining=59.30s.\n",
      "Batch 21800: loss=0.653552, elapsed=2318.55s, remaining=48.67s.\n",
      "Batch 21900: loss=0.653102, elapsed=2328.94s, remaining=38.04s.\n",
      "Batch 22000: loss=0.653007, elapsed=2339.66s, remaining=27.40s.\n",
      "Batch 22100: loss=0.652741, elapsed=2350.26s, remaining=16.76s.\n",
      "Batch 22200: loss=0.652795, elapsed=2360.53s, remaining=6.13s.\n",
      "Batch 22300: loss=0.652850, elapsed=2371.20s, remaining=-4.49s.\n",
      "Batch 22400: loss=0.652676, elapsed=2381.21s, remaining=-15.11s.\n",
      "Batch 22500: loss=0.652150, elapsed=2391.71s, remaining=-25.74s.\n",
      "Batch 22600: loss=0.652189, elapsed=2402.31s, remaining=-36.37s.\n",
      "Batch 22700: loss=0.652152, elapsed=2412.94s, remaining=-46.99s.\n",
      "Batch 22800: loss=0.651992, elapsed=2423.44s, remaining=-57.61s.\n",
      "Batch 22900: loss=0.651651, elapsed=2434.12s, remaining=-68.24s.\n",
      "Batch 23000: loss=0.651574, elapsed=2445.15s, remaining=-78.89s.\n",
      "\n",
      "Training epoch took: 2445.97s\n",
      "Epoch: 4\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.481766, elapsed=10.91s, remaining=2382.03s.\n",
      "Batch 200: loss=0.474671, elapsed=21.55s, remaining=2359.92s.\n",
      "Batch 300: loss=0.477799, elapsed=32.31s, remaining=2353.97s.\n",
      "Batch 400: loss=0.477746, elapsed=42.86s, remaining=2332.51s.\n",
      "Batch 500: loss=0.477475, elapsed=53.74s, remaining=2329.07s.\n",
      "Batch 600: loss=0.479047, elapsed=64.55s, remaining=2322.46s.\n",
      "Batch 700: loss=0.481210, elapsed=75.09s, remaining=2306.03s.\n",
      "Batch 800: loss=0.480439, elapsed=85.57s, remaining=2289.61s.\n",
      "Batch 900: loss=0.480444, elapsed=96.17s, remaining=2276.95s.\n",
      "Batch 1000: loss=0.478151, elapsed=107.22s, remaining=2274.51s.\n",
      "Batch 1100: loss=0.477476, elapsed=118.26s, remaining=2270.74s.\n",
      "Batch 1200: loss=0.477460, elapsed=128.97s, remaining=2259.60s.\n",
      "Batch 1300: loss=0.473594, elapsed=139.91s, remaining=2252.56s.\n",
      "Batch 1400: loss=0.475440, elapsed=150.50s, remaining=2239.55s.\n",
      "Batch 1500: loss=0.476657, elapsed=161.40s, remaining=2231.27s.\n",
      "Batch 1600: loss=0.477171, elapsed=172.20s, remaining=2221.06s.\n",
      "Batch 1700: loss=0.476372, elapsed=182.87s, remaining=2209.42s.\n",
      "Batch 1800: loss=0.477249, elapsed=193.39s, remaining=2196.19s.\n",
      "Batch 1900: loss=0.476324, elapsed=203.79s, remaining=2181.96s.\n",
      "Batch 2000: loss=0.475598, elapsed=214.38s, remaining=2169.93s.\n",
      "Batch 2100: loss=0.477172, elapsed=224.70s, remaining=2155.55s.\n",
      "Batch 2200: loss=0.475321, elapsed=235.19s, remaining=2143.28s.\n",
      "Batch 2300: loss=0.475164, elapsed=245.89s, remaining=2132.95s.\n",
      "Batch 2400: loss=0.476200, elapsed=256.79s, remaining=2124.08s.\n",
      "Batch 2500: loss=0.475192, elapsed=267.48s, remaining=2113.39s.\n",
      "Batch 2600: loss=0.476622, elapsed=278.26s, remaining=2103.58s.\n",
      "Batch 2700: loss=0.476038, elapsed=289.21s, remaining=2094.72s.\n",
      "Batch 2800: loss=0.478487, elapsed=300.12s, remaining=2085.54s.\n",
      "Batch 2900: loss=0.476688, elapsed=310.50s, remaining=2072.63s.\n",
      "Batch 3000: loss=0.476705, elapsed=321.48s, remaining=2063.79s.\n",
      "Batch 3100: loss=0.475781, elapsed=332.12s, remaining=2052.70s.\n",
      "Batch 3200: loss=0.475545, elapsed=342.76s, remaining=2041.49s.\n",
      "Batch 3300: loss=0.475529, elapsed=353.37s, remaining=2030.11s.\n",
      "Batch 3400: loss=0.476499, elapsed=363.84s, remaining=2018.09s.\n",
      "Batch 3500: loss=0.475923, elapsed=374.32s, remaining=2006.26s.\n",
      "Batch 3600: loss=0.476041, elapsed=385.29s, remaining=1997.06s.\n",
      "Batch 3700: loss=0.476344, elapsed=396.11s, remaining=1987.02s.\n",
      "Batch 3800: loss=0.476065, elapsed=406.69s, remaining=1975.73s.\n",
      "Batch 3900: loss=0.477609, elapsed=417.52s, remaining=1965.71s.\n",
      "Batch 4000: loss=0.476328, elapsed=428.15s, remaining=1954.66s.\n",
      "Batch 4100: loss=0.476264, elapsed=439.16s, remaining=1945.41s.\n",
      "Batch 4200: loss=0.476667, elapsed=449.67s, remaining=1933.89s.\n",
      "Batch 4300: loss=0.476799, elapsed=460.28s, remaining=1922.87s.\n",
      "Batch 4400: loss=0.475997, elapsed=470.88s, remaining=1911.87s.\n",
      "Batch 4500: loss=0.476237, elapsed=481.35s, remaining=1900.33s.\n",
      "Batch 4600: loss=0.476130, elapsed=491.94s, remaining=1889.29s.\n",
      "Batch 4700: loss=0.475319, elapsed=502.54s, remaining=1878.31s.\n",
      "Batch 4800: loss=0.475591, elapsed=513.15s, remaining=1867.38s.\n",
      "Batch 4900: loss=0.476093, elapsed=523.84s, remaining=1856.82s.\n",
      "Batch 5000: loss=0.476995, elapsed=534.39s, remaining=1845.72s.\n",
      "Batch 5100: loss=0.476748, elapsed=544.73s, remaining=1833.90s.\n",
      "Batch 5200: loss=0.475966, elapsed=555.09s, remaining=1822.24s.\n",
      "Batch 5300: loss=0.476291, elapsed=565.87s, remaining=1811.98s.\n",
      "Batch 5400: loss=0.477148, elapsed=576.41s, remaining=1800.93s.\n",
      "Batch 5500: loss=0.477466, elapsed=586.99s, remaining=1790.06s.\n",
      "Batch 5600: loss=0.477843, elapsed=597.52s, remaining=1779.01s.\n",
      "Batch 5700: loss=0.477633, elapsed=607.97s, remaining=1767.69s.\n",
      "Batch 5800: loss=0.477054, elapsed=618.53s, remaining=1756.75s.\n",
      "Batch 5900: loss=0.476943, elapsed=629.13s, remaining=1745.95s.\n",
      "Batch 6000: loss=0.476317, elapsed=639.92s, remaining=1735.68s.\n",
      "Batch 6100: loss=0.476925, elapsed=650.22s, remaining=1724.09s.\n",
      "Batch 6200: loss=0.476899, elapsed=660.73s, remaining=1713.14s.\n",
      "Batch 6300: loss=0.476980, elapsed=671.32s, remaining=1702.38s.\n",
      "Batch 6400: loss=0.475898, elapsed=681.95s, remaining=1691.70s.\n",
      "Batch 6500: loss=0.476090, elapsed=692.41s, remaining=1680.62s.\n",
      "Batch 6600: loss=0.475892, elapsed=702.73s, remaining=1669.17s.\n",
      "Batch 6700: loss=0.475319, elapsed=713.32s, remaining=1658.45s.\n",
      "Batch 6800: loss=0.475102, elapsed=723.82s, remaining=1647.50s.\n",
      "Batch 6900: loss=0.474783, elapsed=734.26s, remaining=1636.39s.\n",
      "Batch 7000: loss=0.474492, elapsed=745.09s, remaining=1626.24s.\n",
      "Batch 7100: loss=0.475061, elapsed=755.79s, remaining=1615.77s.\n",
      "Batch 7200: loss=0.474647, elapsed=766.19s, remaining=1604.63s.\n",
      "Batch 7300: loss=0.475163, elapsed=776.74s, remaining=1593.84s.\n",
      "Batch 7400: loss=0.475019, elapsed=787.24s, remaining=1582.97s.\n",
      "Batch 7500: loss=0.474756, elapsed=797.81s, remaining=1572.26s.\n",
      "Batch 7600: loss=0.473861, elapsed=808.29s, remaining=1561.35s.\n",
      "Batch 7700: loss=0.473139, elapsed=818.83s, remaining=1550.55s.\n",
      "Batch 7800: loss=0.473163, elapsed=829.57s, remaining=1540.18s.\n",
      "Batch 7900: loss=0.473168, elapsed=839.85s, remaining=1528.90s.\n",
      "Batch 8000: loss=0.472976, elapsed=850.42s, remaining=1518.20s.\n",
      "Batch 8100: loss=0.473296, elapsed=860.85s, remaining=1507.26s.\n",
      "Batch 8200: loss=0.473754, elapsed=871.42s, remaining=1496.57s.\n",
      "Batch 8300: loss=0.473711, elapsed=882.01s, remaining=1485.90s.\n",
      "Batch 8400: loss=0.473260, elapsed=892.68s, remaining=1475.39s.\n",
      "Batch 8500: loss=0.473294, elapsed=903.09s, remaining=1464.45s.\n",
      "Batch 8600: loss=0.473758, elapsed=913.73s, remaining=1453.89s.\n",
      "Batch 8700: loss=0.473873, elapsed=924.27s, remaining=1443.17s.\n",
      "Batch 8800: loss=0.474562, elapsed=934.97s, remaining=1432.68s.\n",
      "Batch 8900: loss=0.474350, elapsed=945.46s, remaining=1421.86s.\n",
      "Batch 9000: loss=0.473610, elapsed=956.07s, remaining=1411.25s.\n",
      "Batch 9100: loss=0.474203, elapsed=966.83s, remaining=1400.81s.\n",
      "Batch 9200: loss=0.474833, elapsed=977.41s, remaining=1390.15s.\n",
      "Batch 9300: loss=0.474384, elapsed=987.84s, remaining=1379.27s.\n",
      "Batch 9400: loss=0.474549, elapsed=998.51s, remaining=1368.71s.\n",
      "Batch 9500: loss=0.474286, elapsed=1009.12s, remaining=1358.09s.\n",
      "Batch 9600: loss=0.474792, elapsed=1019.74s, remaining=1347.47s.\n",
      "Batch 9700: loss=0.474861, elapsed=1030.26s, remaining=1336.70s.\n",
      "Batch 9800: loss=0.475511, elapsed=1041.06s, remaining=1326.34s.\n",
      "Batch 9900: loss=0.475445, elapsed=1051.25s, remaining=1315.18s.\n",
      "Batch 10000: loss=0.475546, elapsed=1062.00s, remaining=1304.75s.\n",
      "Batch 10100: loss=0.475390, elapsed=1072.53s, remaining=1294.03s.\n",
      "Batch 10200: loss=0.475464, elapsed=1082.84s, remaining=1283.04s.\n",
      "Batch 10300: loss=0.475487, elapsed=1093.22s, remaining=1272.15s.\n",
      "Batch 10400: loss=0.475338, elapsed=1103.55s, remaining=1261.23s.\n",
      "Batch 10500: loss=0.475656, elapsed=1113.85s, remaining=1250.28s.\n",
      "Batch 10600: loss=0.474933, elapsed=1124.25s, remaining=1239.44s.\n",
      "Batch 10700: loss=0.474667, elapsed=1134.69s, remaining=1228.68s.\n",
      "Batch 10800: loss=0.474479, elapsed=1145.15s, remaining=1217.94s.\n",
      "Batch 10900: loss=0.473920, elapsed=1156.01s, remaining=1207.63s.\n",
      "Batch 11000: loss=0.474204, elapsed=1166.56s, remaining=1196.99s.\n",
      "Batch 11100: loss=0.474505, elapsed=1177.42s, remaining=1186.66s.\n",
      "Batch 11200: loss=0.474772, elapsed=1187.86s, remaining=1175.91s.\n",
      "Batch 11300: loss=0.474430, elapsed=1198.52s, remaining=1165.38s.\n",
      "Batch 11400: loss=0.474371, elapsed=1209.28s, remaining=1154.92s.\n",
      "Batch 11500: loss=0.474450, elapsed=1220.02s, remaining=1144.41s.\n",
      "Batch 11600: loss=0.474576, elapsed=1230.86s, remaining=1134.01s.\n",
      "Batch 11700: loss=0.474489, elapsed=1241.47s, remaining=1123.43s.\n",
      "Batch 11800: loss=0.474063, elapsed=1251.74s, remaining=1112.52s.\n",
      "Batch 11900: loss=0.474461, elapsed=1262.19s, remaining=1101.77s.\n",
      "Batch 12000: loss=0.474238, elapsed=1272.68s, remaining=1091.06s.\n",
      "Batch 12100: loss=0.474160, elapsed=1282.94s, remaining=1080.16s.\n",
      "Batch 12200: loss=0.474203, elapsed=1293.57s, remaining=1069.58s.\n",
      "Batch 12300: loss=0.474096, elapsed=1304.51s, remaining=1059.24s.\n",
      "Batch 12400: loss=0.474172, elapsed=1315.56s, remaining=1048.95s.\n",
      "Batch 12500: loss=0.474347, elapsed=1326.25s, remaining=1038.39s.\n",
      "Batch 12600: loss=0.474151, elapsed=1336.87s, remaining=1027.81s.\n",
      "Batch 12700: loss=0.474331, elapsed=1347.59s, remaining=1017.29s.\n",
      "Batch 12800: loss=0.474672, elapsed=1358.18s, remaining=1006.66s.\n",
      "Batch 12900: loss=0.474353, elapsed=1368.76s, remaining=996.03s.\n",
      "Batch 13000: loss=0.474198, elapsed=1379.41s, remaining=985.48s.\n",
      "Batch 13100: loss=0.473872, elapsed=1390.25s, remaining=975.06s.\n",
      "Batch 13200: loss=0.474235, elapsed=1400.94s, remaining=964.50s.\n",
      "Batch 13300: loss=0.473941, elapsed=1411.76s, remaining=954.06s.\n",
      "Batch 13400: loss=0.473823, elapsed=1422.36s, remaining=943.45s.\n",
      "Batch 13500: loss=0.473782, elapsed=1432.87s, remaining=932.77s.\n",
      "Batch 13600: loss=0.473458, elapsed=1443.55s, remaining=922.21s.\n",
      "Batch 13700: loss=0.473697, elapsed=1454.10s, remaining=911.58s.\n",
      "Batch 13800: loss=0.473608, elapsed=1464.73s, remaining=900.98s.\n",
      "Batch 13900: loss=0.473379, elapsed=1475.15s, remaining=890.26s.\n",
      "Batch 14000: loss=0.473049, elapsed=1485.52s, remaining=879.49s.\n",
      "Batch 14100: loss=0.473386, elapsed=1496.15s, remaining=868.91s.\n",
      "Batch 14200: loss=0.473228, elapsed=1506.78s, remaining=858.32s.\n",
      "Batch 14300: loss=0.473220, elapsed=1517.58s, remaining=847.81s.\n",
      "Batch 14400: loss=0.473431, elapsed=1528.18s, remaining=837.22s.\n",
      "Batch 14500: loss=0.473215, elapsed=1538.82s, remaining=826.64s.\n",
      "Batch 14600: loss=0.472972, elapsed=1549.48s, remaining=816.07s.\n",
      "Batch 14700: loss=0.472988, elapsed=1560.09s, remaining=805.48s.\n",
      "Batch 14800: loss=0.472833, elapsed=1570.83s, remaining=794.94s.\n",
      "Batch 14900: loss=0.472868, elapsed=1581.25s, remaining=784.23s.\n",
      "Batch 15000: loss=0.472749, elapsed=1591.75s, remaining=773.58s.\n",
      "Batch 15100: loss=0.472769, elapsed=1602.37s, remaining=762.99s.\n",
      "Batch 15200: loss=0.473007, elapsed=1613.03s, remaining=752.41s.\n",
      "Batch 15300: loss=0.472862, elapsed=1623.65s, remaining=741.82s.\n",
      "Batch 15400: loss=0.473008, elapsed=1634.02s, remaining=731.10s.\n",
      "Batch 15500: loss=0.473001, elapsed=1644.44s, remaining=720.41s.\n",
      "Batch 15600: loss=0.472610, elapsed=1655.04s, remaining=709.81s.\n",
      "Batch 15700: loss=0.472572, elapsed=1665.81s, remaining=699.29s.\n",
      "Batch 15800: loss=0.472425, elapsed=1676.33s, remaining=688.64s.\n",
      "Batch 15900: loss=0.472280, elapsed=1687.00s, remaining=678.05s.\n",
      "Batch 16000: loss=0.472390, elapsed=1697.44s, remaining=667.38s.\n",
      "Batch 16100: loss=0.472283, elapsed=1708.07s, remaining=656.78s.\n",
      "Batch 16200: loss=0.472115, elapsed=1718.55s, remaining=646.14s.\n",
      "Batch 16300: loss=0.471962, elapsed=1729.14s, remaining=635.52s.\n",
      "Batch 16400: loss=0.471871, elapsed=1739.73s, remaining=624.90s.\n",
      "Batch 16500: loss=0.471838, elapsed=1750.16s, remaining=614.24s.\n",
      "Batch 16600: loss=0.471791, elapsed=1760.71s, remaining=603.61s.\n",
      "Batch 16700: loss=0.471986, elapsed=1771.37s, remaining=593.03s.\n",
      "Batch 16800: loss=0.471730, elapsed=1781.84s, remaining=582.38s.\n",
      "Batch 16900: loss=0.471771, elapsed=1792.50s, remaining=571.80s.\n",
      "Batch 17000: loss=0.471488, elapsed=1803.09s, remaining=561.20s.\n",
      "Batch 17100: loss=0.471448, elapsed=1813.76s, remaining=550.62s.\n",
      "Batch 17200: loss=0.471151, elapsed=1824.27s, remaining=540.00s.\n",
      "Batch 17300: loss=0.470939, elapsed=1835.03s, remaining=529.45s.\n",
      "Batch 17400: loss=0.470798, elapsed=1845.44s, remaining=518.79s.\n",
      "Batch 17500: loss=0.470801, elapsed=1856.04s, remaining=508.18s.\n",
      "Batch 17600: loss=0.470687, elapsed=1866.74s, remaining=497.60s.\n",
      "Batch 17700: loss=0.470694, elapsed=1877.31s, remaining=486.99s.\n",
      "Batch 17800: loss=0.470562, elapsed=1887.70s, remaining=476.33s.\n",
      "Batch 17900: loss=0.470641, elapsed=1898.24s, remaining=465.71s.\n",
      "Batch 18000: loss=0.470655, elapsed=1908.61s, remaining=455.06s.\n",
      "Batch 18100: loss=0.470398, elapsed=1918.99s, remaining=444.41s.\n",
      "Batch 18200: loss=0.470108, elapsed=1929.39s, remaining=433.76s.\n",
      "Batch 18300: loss=0.469878, elapsed=1940.12s, remaining=423.20s.\n",
      "Batch 18400: loss=0.469574, elapsed=1950.56s, remaining=412.57s.\n",
      "Batch 18500: loss=0.469489, elapsed=1961.18s, remaining=401.98s.\n",
      "Batch 18600: loss=0.469393, elapsed=1971.63s, remaining=391.35s.\n",
      "Batch 18700: loss=0.469460, elapsed=1982.21s, remaining=380.75s.\n",
      "Batch 18800: loss=0.469322, elapsed=1992.84s, remaining=370.16s.\n",
      "Batch 18900: loss=0.469110, elapsed=2003.53s, remaining=359.58s.\n",
      "Batch 19000: loss=0.468710, elapsed=2014.15s, remaining=348.98s.\n",
      "Batch 19100: loss=0.468694, elapsed=2024.52s, remaining=338.35s.\n",
      "Batch 19200: loss=0.468456, elapsed=2035.10s, remaining=327.75s.\n",
      "Batch 19300: loss=0.468556, elapsed=2045.59s, remaining=317.14s.\n",
      "Batch 19400: loss=0.468615, elapsed=2056.00s, remaining=306.51s.\n",
      "Batch 19500: loss=0.468567, elapsed=2066.49s, remaining=295.90s.\n",
      "Batch 19600: loss=0.468524, elapsed=2076.80s, remaining=285.26s.\n",
      "Batch 19700: loss=0.468447, elapsed=2087.24s, remaining=274.65s.\n",
      "Batch 19800: loss=0.468274, elapsed=2097.63s, remaining=264.03s.\n",
      "Batch 19900: loss=0.468125, elapsed=2108.07s, remaining=253.40s.\n",
      "Batch 20000: loss=0.467786, elapsed=2118.53s, remaining=242.79s.\n",
      "Batch 20100: loss=0.467779, elapsed=2128.93s, remaining=232.18s.\n",
      "Batch 20200: loss=0.467669, elapsed=2139.47s, remaining=221.58s.\n",
      "Batch 20300: loss=0.467412, elapsed=2149.87s, remaining=210.97s.\n",
      "Batch 20400: loss=0.467434, elapsed=2160.75s, remaining=200.43s.\n",
      "Batch 20500: loss=0.467548, elapsed=2171.46s, remaining=189.85s.\n",
      "Batch 20600: loss=0.467483, elapsed=2182.08s, remaining=179.25s.\n",
      "Batch 20700: loss=0.467228, elapsed=2192.63s, remaining=168.65s.\n",
      "Batch 20800: loss=0.467298, elapsed=2203.24s, remaining=158.06s.\n",
      "Batch 20900: loss=0.467203, elapsed=2213.69s, remaining=147.46s.\n",
      "Batch 21000: loss=0.467321, elapsed=2224.24s, remaining=136.87s.\n",
      "Batch 21100: loss=0.466949, elapsed=2234.83s, remaining=126.28s.\n",
      "Batch 21200: loss=0.467301, elapsed=2245.41s, remaining=115.69s.\n",
      "Batch 21300: loss=0.467191, elapsed=2255.89s, remaining=105.10s.\n",
      "Batch 21400: loss=0.467122, elapsed=2266.58s, remaining=94.51s.\n",
      "Batch 21500: loss=0.467095, elapsed=2277.22s, remaining=83.93s.\n",
      "Batch 21600: loss=0.467033, elapsed=2287.84s, remaining=73.33s.\n",
      "Batch 21700: loss=0.466810, elapsed=2298.89s, remaining=62.78s.\n",
      "Batch 21800: loss=0.466878, elapsed=2309.42s, remaining=52.19s.\n",
      "Batch 21900: loss=0.466900, elapsed=2319.94s, remaining=41.60s.\n",
      "Batch 22000: loss=0.466953, elapsed=2330.55s, remaining=31.01s.\n",
      "Batch 22100: loss=0.466763, elapsed=2341.10s, remaining=20.42s.\n",
      "Batch 22200: loss=0.466599, elapsed=2351.85s, remaining=9.83s.\n",
      "Batch 22300: loss=0.466571, elapsed=2362.42s, remaining=-0.76s.\n",
      "Batch 22400: loss=0.466431, elapsed=2372.93s, remaining=-11.35s.\n",
      "Batch 22500: loss=0.466017, elapsed=2383.67s, remaining=-21.95s.\n",
      "Batch 22600: loss=0.465767, elapsed=2394.27s, remaining=-32.54s.\n",
      "Batch 22700: loss=0.465774, elapsed=2404.79s, remaining=-43.14s.\n",
      "Batch 22800: loss=0.465567, elapsed=2415.48s, remaining=-53.74s.\n",
      "Batch 22900: loss=0.465323, elapsed=2426.02s, remaining=-64.34s.\n",
      "Batch 23000: loss=0.465192, elapsed=2436.62s, remaining=-74.94s.\n",
      "\n",
      "Training epoch took: 2437.44s\n",
      "Epoch: 5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.323790, elapsed=10.64s, remaining=2335.62s.\n",
      "Batch 200: loss=0.327947, elapsed=21.06s, remaining=2313.74s.\n",
      "Batch 300: loss=0.335957, elapsed=31.52s, remaining=2301.21s.\n",
      "Batch 400: loss=0.337555, elapsed=41.97s, remaining=2289.86s.\n",
      "Batch 500: loss=0.331108, elapsed=52.49s, remaining=2282.03s.\n",
      "Batch 600: loss=0.338545, elapsed=62.93s, remaining=2270.40s.\n",
      "Batch 700: loss=0.334849, elapsed=73.53s, remaining=2263.77s.\n",
      "Batch 800: loss=0.332472, elapsed=84.21s, remaining=2258.89s.\n",
      "Batch 900: loss=0.336691, elapsed=94.51s, remaining=2243.15s.\n",
      "Batch 1000: loss=0.342196, elapsed=105.16s, remaining=2236.52s.\n",
      "Batch 1100: loss=0.343225, elapsed=115.75s, remaining=2227.41s.\n",
      "Batch 1200: loss=0.346699, elapsed=126.38s, remaining=2219.11s.\n",
      "Batch 1300: loss=0.346725, elapsed=136.92s, remaining=2208.97s.\n",
      "Batch 1400: loss=0.350869, elapsed=147.56s, remaining=2200.31s.\n",
      "Batch 1500: loss=0.347600, elapsed=158.22s, remaining=2191.71s.\n",
      "Batch 1600: loss=0.348615, elapsed=168.82s, remaining=2181.79s.\n",
      "Batch 1700: loss=0.346863, elapsed=179.25s, remaining=2169.94s.\n",
      "Batch 1800: loss=0.344627, elapsed=190.23s, remaining=2164.36s.\n",
      "Batch 1900: loss=0.346338, elapsed=200.73s, remaining=2153.16s.\n",
      "Batch 2000: loss=0.345445, elapsed=211.27s, remaining=2142.37s.\n",
      "Batch 2100: loss=0.344057, elapsed=222.00s, remaining=2133.50s.\n",
      "Batch 2200: loss=0.343445, elapsed=232.47s, remaining=2122.10s.\n",
      "Batch 2300: loss=0.343551, elapsed=243.01s, remaining=2111.28s.\n",
      "Batch 2400: loss=0.344559, elapsed=253.70s, remaining=2101.87s.\n",
      "Batch 2500: loss=0.344403, elapsed=264.37s, remaining=2092.02s.\n",
      "Batch 2600: loss=0.343432, elapsed=275.09s, remaining=2082.62s.\n",
      "Batch 2700: loss=0.341568, elapsed=285.49s, remaining=2070.75s.\n",
      "Batch 2800: loss=0.340725, elapsed=296.30s, remaining=2062.02s.\n",
      "Batch 2900: loss=0.341568, elapsed=306.90s, remaining=2051.57s.\n",
      "Batch 3000: loss=0.342608, elapsed=317.41s, remaining=2040.61s.\n",
      "Batch 3100: loss=0.342860, elapsed=328.07s, remaining=2030.53s.\n",
      "Batch 3200: loss=0.341742, elapsed=338.85s, remaining=2021.13s.\n",
      "Batch 3300: loss=0.341584, elapsed=349.34s, remaining=2010.03s.\n",
      "Batch 3400: loss=0.341815, elapsed=360.04s, remaining=2000.06s.\n",
      "Batch 3500: loss=0.342690, elapsed=370.53s, remaining=1989.01s.\n",
      "Batch 3600: loss=0.342749, elapsed=381.09s, remaining=1978.34s.\n",
      "Batch 3700: loss=0.341608, elapsed=391.62s, remaining=1967.55s.\n",
      "Batch 3800: loss=0.341223, elapsed=402.06s, remaining=1956.25s.\n",
      "Batch 3900: loss=0.340392, elapsed=412.67s, remaining=1945.84s.\n",
      "Batch 4000: loss=0.340170, elapsed=423.34s, remaining=1935.75s.\n",
      "Batch 4100: loss=0.339929, elapsed=433.76s, remaining=1924.44s.\n",
      "Batch 4200: loss=0.339079, elapsed=444.30s, remaining=1913.74s.\n",
      "Batch 4300: loss=0.339111, elapsed=454.96s, remaining=1903.56s.\n",
      "Batch 4400: loss=0.339419, elapsed=465.61s, remaining=1893.28s.\n",
      "Batch 4500: loss=0.339097, elapsed=476.08s, remaining=1882.30s.\n",
      "Batch 4600: loss=0.339086, elapsed=486.60s, remaining=1871.48s.\n",
      "Batch 4700: loss=0.339650, elapsed=497.34s, remaining=1861.49s.\n",
      "Batch 4800: loss=0.338571, elapsed=507.76s, remaining=1850.34s.\n",
      "Batch 4900: loss=0.338761, elapsed=518.46s, remaining=1840.23s.\n",
      "Batch 5000: loss=0.338812, elapsed=528.78s, remaining=1828.76s.\n",
      "Batch 5100: loss=0.339686, elapsed=539.42s, remaining=1818.40s.\n",
      "Batch 5200: loss=0.339242, elapsed=550.00s, remaining=1807.79s.\n",
      "Batch 5300: loss=0.339248, elapsed=560.39s, remaining=1796.62s.\n",
      "Batch 5400: loss=0.338855, elapsed=571.01s, remaining=1786.21s.\n",
      "Batch 5500: loss=0.339789, elapsed=581.39s, remaining=1775.06s.\n",
      "Batch 5600: loss=0.339354, elapsed=591.82s, remaining=1764.10s.\n",
      "Batch 5700: loss=0.340029, elapsed=602.04s, remaining=1752.49s.\n",
      "Batch 5800: loss=0.340276, elapsed=612.41s, remaining=1741.35s.\n",
      "Batch 5900: loss=0.339681, elapsed=622.93s, remaining=1730.67s.\n",
      "Batch 6000: loss=0.340021, elapsed=633.30s, remaining=1719.60s.\n",
      "Batch 6100: loss=0.339704, elapsed=644.02s, remaining=1709.51s.\n",
      "Batch 6200: loss=0.340050, elapsed=654.59s, remaining=1699.02s.\n",
      "Batch 6300: loss=0.340540, elapsed=665.27s, remaining=1688.81s.\n",
      "Batch 6400: loss=0.340249, elapsed=676.03s, remaining=1678.75s.\n",
      "Batch 6500: loss=0.340115, elapsed=686.62s, remaining=1668.28s.\n",
      "Batch 6600: loss=0.339236, elapsed=697.05s, remaining=1657.37s.\n",
      "Batch 6700: loss=0.339474, elapsed=707.67s, remaining=1646.97s.\n",
      "Batch 6800: loss=0.341066, elapsed=718.26s, remaining=1636.47s.\n",
      "Batch 6900: loss=0.340864, elapsed=729.07s, remaining=1626.45s.\n",
      "Batch 7000: loss=0.340221, elapsed=739.57s, remaining=1615.76s.\n",
      "Batch 7100: loss=0.340183, elapsed=750.15s, remaining=1605.24s.\n",
      "Batch 7200: loss=0.340454, elapsed=760.72s, remaining=1594.69s.\n",
      "Batch 7300: loss=0.340074, elapsed=771.47s, remaining=1584.51s.\n",
      "Batch 7400: loss=0.340338, elapsed=781.95s, remaining=1573.78s.\n",
      "Batch 7500: loss=0.340865, elapsed=792.34s, remaining=1562.85s.\n",
      "Batch 7600: loss=0.340932, elapsed=802.85s, remaining=1552.20s.\n",
      "Batch 7700: loss=0.340768, elapsed=813.50s, remaining=1541.79s.\n",
      "Batch 7800: loss=0.341100, elapsed=824.19s, remaining=1531.48s.\n",
      "Batch 7900: loss=0.340363, elapsed=834.65s, remaining=1520.73s.\n",
      "Batch 8000: loss=0.340409, elapsed=845.31s, remaining=1510.34s.\n",
      "Batch 8100: loss=0.339881, elapsed=855.85s, remaining=1499.73s.\n",
      "Batch 8200: loss=0.340379, elapsed=866.39s, remaining=1489.15s.\n",
      "Batch 8300: loss=0.340101, elapsed=876.92s, remaining=1478.55s.\n",
      "Batch 8400: loss=0.340472, elapsed=887.31s, remaining=1467.70s.\n",
      "Batch 8500: loss=0.340269, elapsed=897.89s, remaining=1457.19s.\n",
      "Batch 8600: loss=0.340847, elapsed=908.76s, remaining=1447.14s.\n",
      "Batch 8700: loss=0.340440, elapsed=919.30s, remaining=1436.51s.\n",
      "Batch 8800: loss=0.340223, elapsed=929.54s, remaining=1425.45s.\n",
      "Batch 8900: loss=0.340171, elapsed=940.02s, remaining=1414.79s.\n",
      "Batch 9000: loss=0.340629, elapsed=950.55s, remaining=1404.18s.\n",
      "Batch 9100: loss=0.340498, elapsed=961.08s, remaining=1393.58s.\n",
      "Batch 9200: loss=0.340466, elapsed=971.87s, remaining=1383.38s.\n",
      "Batch 9300: loss=0.340914, elapsed=982.38s, remaining=1372.76s.\n",
      "Batch 9400: loss=0.340879, elapsed=993.26s, remaining=1362.65s.\n",
      "Batch 9500: loss=0.340857, elapsed=1003.88s, remaining=1352.18s.\n",
      "Batch 9600: loss=0.340719, elapsed=1014.49s, remaining=1341.68s.\n",
      "Batch 9700: loss=0.340857, elapsed=1024.97s, remaining=1331.00s.\n",
      "Batch 9800: loss=0.340922, elapsed=1035.45s, remaining=1320.32s.\n",
      "Batch 9900: loss=0.341122, elapsed=1045.98s, remaining=1309.72s.\n",
      "Batch 10000: loss=0.340990, elapsed=1056.56s, remaining=1299.17s.\n",
      "Batch 10100: loss=0.340945, elapsed=1067.21s, remaining=1288.71s.\n",
      "Batch 10200: loss=0.340918, elapsed=1078.07s, remaining=1278.52s.\n",
      "Batch 10300: loss=0.340736, elapsed=1088.62s, remaining=1267.91s.\n",
      "Batch 10400: loss=0.340879, elapsed=1099.09s, remaining=1257.23s.\n",
      "Batch 10500: loss=0.340973, elapsed=1109.55s, remaining=1246.53s.\n",
      "Batch 10600: loss=0.340596, elapsed=1120.02s, remaining=1235.86s.\n",
      "Batch 10700: loss=0.340092, elapsed=1130.47s, remaining=1225.16s.\n",
      "Batch 10800: loss=0.340120, elapsed=1141.17s, remaining=1214.73s.\n",
      "Batch 10900: loss=0.339958, elapsed=1151.70s, remaining=1204.12s.\n",
      "Batch 11000: loss=0.340518, elapsed=1162.45s, remaining=1193.75s.\n",
      "Batch 11100: loss=0.340344, elapsed=1173.02s, remaining=1183.20s.\n",
      "Batch 11200: loss=0.340007, elapsed=1183.32s, remaining=1172.38s.\n",
      "Batch 11300: loss=0.340260, elapsed=1193.93s, remaining=1161.86s.\n",
      "Batch 11400: loss=0.340270, elapsed=1204.56s, remaining=1151.37s.\n",
      "Batch 11500: loss=0.340424, elapsed=1214.93s, remaining=1140.61s.\n",
      "Batch 11600: loss=0.340735, elapsed=1225.60s, remaining=1130.15s.\n",
      "Batch 11700: loss=0.340770, elapsed=1236.05s, remaining=1119.48s.\n",
      "Batch 11800: loss=0.340401, elapsed=1246.49s, remaining=1108.81s.\n",
      "Batch 11900: loss=0.340031, elapsed=1257.03s, remaining=1098.24s.\n",
      "Batch 12000: loss=0.339884, elapsed=1267.78s, remaining=1087.84s.\n",
      "Batch 12100: loss=0.339582, elapsed=1278.16s, remaining=1077.11s.\n",
      "Batch 12200: loss=0.339433, elapsed=1288.56s, remaining=1066.39s.\n",
      "Batch 12300: loss=0.338534, elapsed=1299.12s, remaining=1055.84s.\n",
      "Batch 12400: loss=0.338339, elapsed=1309.85s, remaining=1045.43s.\n",
      "Batch 12500: loss=0.337935, elapsed=1320.37s, remaining=1034.83s.\n",
      "Batch 12600: loss=0.338229, elapsed=1330.72s, remaining=1024.12s.\n",
      "Batch 12700: loss=0.337992, elapsed=1341.14s, remaining=1013.44s.\n",
      "Batch 12800: loss=0.337870, elapsed=1351.85s, remaining=1003.00s.\n",
      "Batch 12900: loss=0.338074, elapsed=1362.23s, remaining=992.31s.\n",
      "Batch 13000: loss=0.338577, elapsed=1372.71s, remaining=981.71s.\n",
      "Batch 13100: loss=0.338467, elapsed=1383.04s, remaining=970.99s.\n",
      "Batch 13200: loss=0.338380, elapsed=1393.72s, remaining=960.52s.\n",
      "Batch 13300: loss=0.338179, elapsed=1404.16s, remaining=949.89s.\n",
      "Batch 13400: loss=0.338019, elapsed=1414.62s, remaining=939.26s.\n",
      "Batch 13500: loss=0.338445, elapsed=1425.10s, remaining=928.65s.\n",
      "Batch 13600: loss=0.338269, elapsed=1435.47s, remaining=917.96s.\n",
      "Batch 13700: loss=0.338177, elapsed=1446.04s, remaining=907.43s.\n",
      "Batch 13800: loss=0.338143, elapsed=1456.95s, remaining=897.12s.\n",
      "Batch 13900: loss=0.338140, elapsed=1467.37s, remaining=886.50s.\n",
      "Batch 14000: loss=0.337692, elapsed=1477.80s, remaining=875.88s.\n",
      "Batch 14100: loss=0.337714, elapsed=1488.22s, remaining=865.24s.\n",
      "Batch 14200: loss=0.337667, elapsed=1498.72s, remaining=854.65s.\n",
      "Batch 14300: loss=0.337704, elapsed=1509.19s, remaining=844.06s.\n",
      "Batch 14400: loss=0.337402, elapsed=1519.67s, remaining=833.46s.\n",
      "Batch 14500: loss=0.337250, elapsed=1530.17s, remaining=822.87s.\n",
      "Batch 14600: loss=0.336996, elapsed=1540.66s, remaining=812.28s.\n",
      "Batch 14700: loss=0.336807, elapsed=1551.26s, remaining=801.76s.\n",
      "Batch 14800: loss=0.337172, elapsed=1561.76s, remaining=791.18s.\n",
      "Batch 14900: loss=0.337236, elapsed=1572.33s, remaining=780.62s.\n",
      "Batch 15000: loss=0.337252, elapsed=1582.84s, remaining=770.06s.\n",
      "Batch 15100: loss=0.336995, elapsed=1593.23s, remaining=759.43s.\n",
      "Batch 15200: loss=0.336918, elapsed=1603.81s, remaining=748.89s.\n",
      "Batch 15300: loss=0.336497, elapsed=1614.27s, remaining=738.29s.\n",
      "Batch 15400: loss=0.336566, elapsed=1624.86s, remaining=727.76s.\n",
      "Batch 15500: loss=0.336250, elapsed=1635.42s, remaining=717.20s.\n",
      "Batch 15600: loss=0.336369, elapsed=1646.10s, remaining=706.70s.\n",
      "Batch 15700: loss=0.336028, elapsed=1656.66s, remaining=696.15s.\n",
      "Batch 15800: loss=0.336230, elapsed=1667.29s, remaining=685.63s.\n",
      "Batch 15900: loss=0.336035, elapsed=1677.78s, remaining=675.05s.\n",
      "Batch 16000: loss=0.336201, elapsed=1688.46s, remaining=664.54s.\n",
      "Batch 16100: loss=0.335853, elapsed=1698.90s, remaining=653.95s.\n",
      "Batch 16200: loss=0.335577, elapsed=1709.51s, remaining=643.41s.\n",
      "Batch 16300: loss=0.335492, elapsed=1720.04s, remaining=632.85s.\n",
      "Batch 16400: loss=0.335376, elapsed=1730.57s, remaining=622.28s.\n",
      "Batch 16500: loss=0.334933, elapsed=1741.08s, remaining=611.70s.\n",
      "Batch 16600: loss=0.334879, elapsed=1751.57s, remaining=601.13s.\n",
      "Batch 16700: loss=0.334923, elapsed=1762.11s, remaining=590.57s.\n",
      "Batch 16800: loss=0.335022, elapsed=1772.60s, remaining=579.99s.\n",
      "Batch 16900: loss=0.335253, elapsed=1783.07s, remaining=569.41s.\n",
      "Batch 17000: loss=0.335139, elapsed=1793.62s, remaining=558.85s.\n",
      "Batch 17100: loss=0.335094, elapsed=1804.25s, remaining=548.33s.\n",
      "Batch 17200: loss=0.335212, elapsed=1814.74s, remaining=537.76s.\n",
      "Batch 17300: loss=0.335036, elapsed=1825.21s, remaining=527.18s.\n",
      "Batch 17400: loss=0.335002, elapsed=1835.81s, remaining=516.64s.\n",
      "Batch 17500: loss=0.335109, elapsed=1846.44s, remaining=506.13s.\n",
      "Batch 17600: loss=0.334785, elapsed=1856.88s, remaining=495.54s.\n",
      "Batch 17700: loss=0.334549, elapsed=1867.55s, remaining=485.03s.\n",
      "Batch 17800: loss=0.334703, elapsed=1878.25s, remaining=474.52s.\n",
      "Batch 17900: loss=0.334876, elapsed=1888.86s, remaining=463.98s.\n",
      "Batch 18000: loss=0.334606, elapsed=1899.36s, remaining=453.40s.\n",
      "Batch 18100: loss=0.334397, elapsed=1910.02s, remaining=442.86s.\n",
      "Batch 18200: loss=0.334212, elapsed=1920.95s, remaining=432.40s.\n",
      "Batch 18300: loss=0.334401, elapsed=1931.42s, remaining=421.83s.\n",
      "Batch 18400: loss=0.334300, elapsed=1942.21s, remaining=411.33s.\n",
      "Batch 18500: loss=0.334284, elapsed=1952.78s, remaining=400.77s.\n",
      "Batch 18600: loss=0.334068, elapsed=1963.58s, remaining=390.26s.\n",
      "Batch 18700: loss=0.334179, elapsed=1974.17s, remaining=379.72s.\n",
      "Batch 18800: loss=0.333996, elapsed=1984.77s, remaining=369.18s.\n",
      "Batch 18900: loss=0.333764, elapsed=1995.52s, remaining=358.66s.\n",
      "Batch 19000: loss=0.333769, elapsed=2006.16s, remaining=348.12s.\n",
      "Batch 19100: loss=0.333921, elapsed=2016.61s, remaining=337.54s.\n",
      "Batch 19200: loss=0.333906, elapsed=2027.26s, remaining=326.99s.\n",
      "Batch 19300: loss=0.334015, elapsed=2037.77s, remaining=316.42s.\n",
      "Batch 19400: loss=0.333793, elapsed=2048.43s, remaining=305.88s.\n",
      "Batch 19500: loss=0.333633, elapsed=2059.03s, remaining=295.33s.\n",
      "Batch 19600: loss=0.333510, elapsed=2069.55s, remaining=284.78s.\n",
      "Batch 19700: loss=0.333573, elapsed=2080.16s, remaining=274.22s.\n",
      "Batch 19800: loss=0.333618, elapsed=2090.75s, remaining=263.67s.\n",
      "Batch 19900: loss=0.333549, elapsed=2101.44s, remaining=253.12s.\n",
      "Batch 20000: loss=0.333491, elapsed=2111.92s, remaining=242.55s.\n",
      "Batch 20100: loss=0.333324, elapsed=2122.64s, remaining=232.01s.\n",
      "Batch 20200: loss=0.333659, elapsed=2133.42s, remaining=221.47s.\n",
      "Batch 20300: loss=0.333534, elapsed=2143.65s, remaining=210.86s.\n",
      "Batch 20400: loss=0.333334, elapsed=2154.24s, remaining=200.30s.\n",
      "Batch 20500: loss=0.333071, elapsed=2164.97s, remaining=189.75s.\n",
      "Batch 20600: loss=0.332731, elapsed=2175.47s, remaining=179.19s.\n",
      "Batch 20700: loss=0.332483, elapsed=2185.91s, remaining=168.61s.\n",
      "Batch 20800: loss=0.332363, elapsed=2196.63s, remaining=158.07s.\n",
      "Batch 20900: loss=0.332138, elapsed=2207.06s, remaining=147.50s.\n",
      "Batch 21000: loss=0.331993, elapsed=2217.54s, remaining=136.92s.\n",
      "Batch 21100: loss=0.331919, elapsed=2228.08s, remaining=126.36s.\n",
      "Batch 21200: loss=0.331736, elapsed=2238.60s, remaining=115.80s.\n",
      "Batch 21300: loss=0.331784, elapsed=2249.15s, remaining=105.22s.\n",
      "Batch 21400: loss=0.331866, elapsed=2259.53s, remaining=94.65s.\n",
      "Batch 21500: loss=0.331726, elapsed=2270.33s, remaining=84.10s.\n",
      "Batch 21600: loss=0.331627, elapsed=2280.85s, remaining=73.54s.\n",
      "Batch 21700: loss=0.331444, elapsed=2291.60s, remaining=62.99s.\n",
      "Batch 21800: loss=0.331617, elapsed=2302.15s, remaining=52.42s.\n",
      "Batch 21900: loss=0.331478, elapsed=2312.69s, remaining=41.86s.\n",
      "Batch 22000: loss=0.331263, elapsed=2323.31s, remaining=31.30s.\n",
      "Batch 22100: loss=0.331034, elapsed=2333.85s, remaining=20.73s.\n",
      "Batch 22200: loss=0.331031, elapsed=2344.39s, remaining=10.18s.\n",
      "Batch 22300: loss=0.330827, elapsed=2354.60s, remaining=-0.40s.\n",
      "Batch 22400: loss=0.330614, elapsed=2365.28s, remaining=-10.95s.\n",
      "Batch 22500: loss=0.330428, elapsed=2375.70s, remaining=-21.51s.\n",
      "Batch 22600: loss=0.330299, elapsed=2386.04s, remaining=-32.07s.\n",
      "Batch 22700: loss=0.330111, elapsed=2396.28s, remaining=-42.62s.\n",
      "Batch 22800: loss=0.329918, elapsed=2406.69s, remaining=-53.18s.\n",
      "Batch 22900: loss=0.329995, elapsed=2417.26s, remaining=-63.74s.\n",
      "Batch 23000: loss=0.329914, elapsed=2427.76s, remaining=-74.29s.\n",
      "\n",
      "Training epoch took: 2428.65s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    train_loop(train_dataloader, model,optimizer, scheduler, device, criterion, criterion, sentiment_var='sentiment',\n",
    "               topic_var='topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, 'results/models/contextscale_full_released/model.safetensors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale manifestos and coalition contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped = pd.read_csv('data/temps/coalitionagree_regrouped_processed.csv', encoding='utf-8')\n",
    "manifesto_regrouped = pd.read_csv('data/temps/manifesto_regrouped_full_processed.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 12\n",
    "num_sentiments = 3\n",
    "scaling_model = ContextScalePrediction(roberta_model=model_name, num_topics=12, num_sentiments=3,lora=False, use_shared_attention=True).to(device)\n",
    "\n",
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tensors = load_file('results/models/contextscale_full_released/model.safetensors')\n",
    "scaling_model.load_state_dict(loaded_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee7312ce39143b383b8d0be0c963122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/369755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e616123964bc449bba9180074a30754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d8e48e89fe4b58ae6260aa4e81fcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/369755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7207c14015054454bb185f93ad9ccb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manifesto_dataset = Dataset.from_pandas(manifesto_regrouped[['text','topic','sentiment']].copy())\n",
    "coalition_dataset = Dataset.from_pandas(coalition_regrouped[['text','topic','sentiment']].copy())\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('topic') \n",
    "coalition_dataset = coalition_dataset.class_encode_column('topic')\n",
    "manifesto_dataset = manifesto_dataset.class_encode_column('sentiment') \n",
    "coalition_dataset = coalition_dataset.class_encode_column('sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb573aa51a70407c87e73f931e61919a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/369755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a4ef7d8ba7418991b201639a94fff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_manifesto_dataset = manifesto_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text'])\n",
    "tokenized_manifesto_dataset.set_format(\"torch\")\n",
    "tokenized_coalition_dataset = coalition_dataset.map(tokenize_function, \n",
    "                                            fn_kwargs={'tokenizer': tokenizer, 'text_var': 'text', 'max_length': 512}, \n",
    "                                            remove_columns=['text'])\n",
    "tokenized_coalition_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_dataloader = DataLoader(tokenized_manifesto_dataset, batch_size=16, shuffle=False, collate_fn= data_collator)\n",
    "coalition_dataloader = DataLoader(tokenized_coalition_dataset, batch_size=16, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 33.80s, Estimated remaining time: 747.32s\n",
      "Elapsed time: 76.53s, Estimated remaining time: 807.79s\n",
      "Elapsed time: 117.62s, Estimated remaining time: 788.42s\n",
      "Elapsed time: 158.99s, Estimated remaining time: 759.55s\n",
      "Elapsed time: 200.20s, Estimated remaining time: 725.12s\n",
      "Elapsed time: 231.22s, Estimated remaining time: 659.36s\n",
      "Elapsed time: 265.45s, Estimated remaining time: 610.91s\n",
      "Elapsed time: 298.23s, Estimated remaining time: 563.28s\n",
      "Elapsed time: 342.40s, Estimated remaining time: 536.81s\n",
      "Elapsed time: 379.39s, Estimated remaining time: 497.38s\n",
      "Elapsed time: 423.42s, Estimated remaining time: 466.14s\n",
      "Elapsed time: 457.57s, Estimated remaining time: 423.64s\n",
      "Elapsed time: 488.36s, Estimated remaining time: 379.80s\n",
      "Elapsed time: 519.08s, Estimated remaining time: 337.77s\n",
      "Elapsed time: 549.56s, Estimated remaining time: 297.13s\n",
      "Elapsed time: 582.32s, Estimated remaining time: 258.77s\n",
      "Elapsed time: 627.27s, Estimated remaining time: 225.45s\n",
      "Elapsed time: 670.58s, Estimated remaining time: 190.37s\n",
      "Elapsed time: 721.73s, Estimated remaining time: 156.12s\n",
      "Elapsed time: 773.45s, Estimated remaining time: 120.27s\n",
      "Elapsed time: 806.39s, Estimated remaining time: 81.02s\n",
      "Elapsed time: 841.47s, Estimated remaining time: 42.46s\n",
      "Elapsed time: 875.30s, Estimated remaining time: 4.19s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "## Compute position scores\n",
    "output_manifesto_final = scale_func(manifesto_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/temps/topic_labels'\n",
    "with open(file_path, 'rb') as fp:\n",
    "    topic_labels = pickle.load(fp)\n",
    "name_topic_dict = dict([(x,y) for x,y in enumerate(topic_labels)])\n",
    "\n",
    "\n",
    "file_path = 'data/temps/sentiment_labels'\n",
    "with open(file_path, 'rb') as fp:\n",
    "    sentiment_labels = pickle.load(fp)\n",
    "name_sentiment_dict = dict([(x,y) for x,y in enumerate(sentiment_labels)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['res_table_topic', 'res_table_sentiment', 'position_scores', 'pred_topics', 'pred_sentiment', 'total_time', 'avg_batch_time'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_manifesto_final.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.loc[:,'position_scores'] = output_manifesto_final['position_scores'].flatten()\n",
    "manifesto_regrouped.loc[:,'pred_sentiment'] = output_manifesto_final['pred_sentiment']\n",
    "manifesto_regrouped.loc[:,'pred_sentiment_name'] = manifesto_regrouped.pred_sentiment.map(name_sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>country_election_party_code</th>\n",
       "      <th>country</th>\n",
       "      <th>election</th>\n",
       "      <th>party</th>\n",
       "      <th>cmp_code</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>position_scores</th>\n",
       "      <th>pred_sentiment</th>\n",
       "      <th>pred_sentiment_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statt dessen soll ein Freiwilligen-Milizheer g...</td>\n",
       "      <td>Austria;1999;42110;104</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>104.0</td>\n",
       "      <td>right</td>\n",
       "      <td>Military</td>\n",
       "      <td>0.858892</td>\n",
       "      <td>2</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weg von der Sicherheit durch RÃ¼stung, Unsere S...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.854351</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Militarisierung der EU bringt mehr RÃ¼stung...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.671753</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.819007</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...</td>\n",
       "      <td>Austria;1999;42110;105</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1999</td>\n",
       "      <td>42110</td>\n",
       "      <td>105.0</td>\n",
       "      <td>left</td>\n",
       "      <td>Military</td>\n",
       "      <td>-0.767466</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Statt dessen soll ein Freiwilligen-Milizheer g...   \n",
       "1  Weg von der Sicherheit durch RÃ¼stung, Unsere S...   \n",
       "2  Die Militarisierung der EU bringt mehr RÃ¼stung...   \n",
       "3  Bei einem Beitritt Ã–sterreichs kÃ¶nnten auch Ã¶s...   \n",
       "4  Seit 2 Jahren werden 500 Panzer fÃ¼r das Ã¶sterr...   \n",
       "\n",
       "  country_election_party_code  country  election  party  cmp_code sentiment  \\\n",
       "0      Austria;1999;42110;104  Austria      1999  42110     104.0     right   \n",
       "1      Austria;1999;42110;105  Austria      1999  42110     105.0      left   \n",
       "2      Austria;1999;42110;105  Austria      1999  42110     105.0      left   \n",
       "3      Austria;1999;42110;105  Austria      1999  42110     105.0      left   \n",
       "4      Austria;1999;42110;105  Austria      1999  42110     105.0      left   \n",
       "\n",
       "      topic  position_scores  pred_sentiment pred_sentiment_name  \n",
       "0  Military         0.858892               2               right  \n",
       "1  Military        -0.854351               0                left  \n",
       "2  Military        -0.671753               0                left  \n",
       "3  Military        -0.819007               0                left  \n",
       "4  Military        -0.767466               0                left  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifesto_regrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifesto_regrouped.to_csv('results/datasets/manifesto_full_scaled.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting labels...\n",
      "Labels for topic are provided. They will be used for position scaling!\n",
      "Elapsed time: 43.16s, Estimated remaining time: 62.85s\n",
      "Elapsed time: 79.96s, Estimated remaining time: 18.23s\n",
      "Start computing position scores\n"
     ]
    }
   ],
   "source": [
    "## Compute position scores\n",
    "output_coalition_final = scale_func(coalition_dataloader, \n",
    "               scaling_model, \n",
    "               device, \n",
    "               topic_label='topic', \n",
    "               sentiment_label='sentiment', \n",
    "               timing_log=True,\n",
    "               use_ground_truth_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped.loc[:,'position_scores'] = output_coalition_final['position_scores'].flatten()\n",
    "coalition_regrouped.loc[:,'pred_sentiment'] = output_coalition_final['pred_sentiment']\n",
    "coalition_regrouped.loc[:,'pred_sentiment_name'] = coalition_regrouped.pred_sentiment.map(name_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition_regrouped.to_csv('results/datasets/coalition_full_scaled.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create released dataset (position scores by country-party-election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  =['country','party', 'election','topic','cs_mean_score', 'cs_se_score']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for name, group in manifesto_regrouped.groupby(['country','party','election','topic']):\n",
    "    mean_score = group['position_scores'].mean()\n",
    "    se_score = group['position_scores'].std()/np.sqrt(len(group))\n",
    "    df_temp = pd.DataFrame([[str(group.iloc[0,group.columns.get_loc('country')]),\n",
    "                             str(group.iloc[0,group.columns.get_loc('party')]), \n",
    "                    str(group.iloc[0,group.columns.get_loc('election')]), \n",
    "                    str(group.iloc[0,group.columns.get_loc('topic')]),\n",
    "               mean_score, se_score]], columns = columns)\n",
    "    df = (df_temp if df.empty else pd.concat([df, df_temp], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/datasets/contextscale_manifesto_dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>party</th>\n",
       "      <th>election</th>\n",
       "      <th>topic</th>\n",
       "      <th>cs_mean_score</th>\n",
       "      <th>cs_se_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>42110</td>\n",
       "      <td>1999</td>\n",
       "      <td>Agriculture - Protectionism</td>\n",
       "      <td>-0.884188</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria</td>\n",
       "      <td>42110</td>\n",
       "      <td>1999</td>\n",
       "      <td>Economics</td>\n",
       "      <td>0.025536</td>\n",
       "      <td>0.105402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>42110</td>\n",
       "      <td>1999</td>\n",
       "      <td>Education</td>\n",
       "      <td>-0.877355</td>\n",
       "      <td>0.017827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria</td>\n",
       "      <td>42110</td>\n",
       "      <td>1999</td>\n",
       "      <td>Environment - Growth</td>\n",
       "      <td>-0.535783</td>\n",
       "      <td>0.056544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria</td>\n",
       "      <td>42110</td>\n",
       "      <td>1999</td>\n",
       "      <td>European Integration</td>\n",
       "      <td>-0.531903</td>\n",
       "      <td>0.111941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  party election                        topic  cs_mean_score  \\\n",
       "0  Austria  42110     1999  Agriculture - Protectionism      -0.884188   \n",
       "1  Austria  42110     1999                    Economics       0.025536   \n",
       "2  Austria  42110     1999                    Education      -0.877355   \n",
       "3  Austria  42110     1999         Environment - Growth      -0.535783   \n",
       "4  Austria  42110     1999         European Integration      -0.531903   \n",
       "\n",
       "   cs_se_score  \n",
       "0          NaN  \n",
       "1     0.105402  \n",
       "2     0.017827  \n",
       "3     0.056544  \n",
       "4     0.111941  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  =['country', 'year','topic','cs_mean_score', 'cs_se_score']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for name, group in coalition_regrouped.groupby(['country','year','topic']):\n",
    "    mean_score = group['position_scores'].mean()\n",
    "    se_score = group['position_scores'].std()/np.sqrt(len(group))\n",
    "    df_temp = pd.DataFrame([[str(group.iloc[0,group.columns.get_loc('country')]),\n",
    "                    str(group.iloc[0,group.columns.get_loc('year')]), \n",
    "                    str(group.iloc[0,group.columns.get_loc('topic')]),\n",
    "               mean_score, se_score]], columns = columns)\n",
    "    df = (df_temp if df.empty else pd.concat([df, df_temp], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>topic</th>\n",
       "      <th>cs_mean_score</th>\n",
       "      <th>cs_se_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>Economics</td>\n",
       "      <td>-0.687361</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>Education</td>\n",
       "      <td>-0.923366</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>Other</td>\n",
       "      <td>-0.025228</td>\n",
       "      <td>0.017780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria</td>\n",
       "      <td>1945</td>\n",
       "      <td>Political System</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.095684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria</td>\n",
       "      <td>1949</td>\n",
       "      <td>Agriculture - Protectionism</td>\n",
       "      <td>-0.909265</td>\n",
       "      <td>0.009542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  year                        topic  cs_mean_score  cs_se_score\n",
       "0  Austria  1945                    Economics      -0.687361          NaN\n",
       "1  Austria  1945                    Education      -0.923366          NaN\n",
       "2  Austria  1945                        Other      -0.025228     0.017780\n",
       "3  Austria  1945             Political System       0.051062     0.095684\n",
       "4  Austria  1949  Agriculture - Protectionism      -0.909265     0.009542"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/datasets/contextscale_coalition_dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Ensemble Training and Uncertainty Estimation\n",
    "\n",
    "This section implements deep ensemble training with uncertainty estimation using the redesigned exponential position score computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new uncertainty module\n",
    "import sys\n",
    "sys.path.append('utils/')\n",
    "from uncertainty import (\n",
    "    compute_position_score_exponential, \n",
    "    train_deep_ensemble, \n",
    "    ensemble_inference,\n",
    "    load_ensemble_models,\n",
    "    save_ensemble_results,\n",
    "    create_ensemble_summary_dataframe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Configuration:\n",
      "  num_models: 5\n",
      "  beta: 1.0\n",
      "  n_epochs: 5\n",
      "  lr: 2e-05\n",
      "  save_dir: results/models/ensemble\n",
      "  model_prefix: model_ensemble\n"
     ]
    }
   ],
   "source": [
    "# Configuration for ensemble training and uncertainty estimation\n",
    "ENSEMBLE_CONFIG = {\n",
    "    'num_models': 5,  # Number of ensemble members\n",
    "    'beta': 1.0,      # Beta parameter for exponential position score\n",
    "    'n_epochs': 5,    # Epochs per model\n",
    "    'lr': 2e-5,       # Learning rate\n",
    "    'save_dir': 'results/models/ensemble',\n",
    "    'model_prefix': 'model_ensemble'\n",
    "}\n",
    "\n",
    "print(\"Ensemble Configuration:\")\n",
    "for key, value in ENSEMBLE_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model factory function defined\n"
     ]
    }
   ],
   "source": [
    "# Define model factory function for ensemble training\n",
    "\n",
    "num_topics = len(set(manifesto_dataset['topic']))\n",
    "num_sentiments = len(set(manifesto_dataset['sentiment']))\n",
    "def create_model():\n",
    "    \"\"\"Factory function to create a new model instance for ensemble training.\"\"\"\n",
    "    return ContextScalePrediction(\n",
    "        roberta_model=model_name, \n",
    "        num_topics=num_topics, \n",
    "        num_sentiments=num_sentiments,\n",
    "        lora=True,\n",
    "        use_shared_attention=True  # Using shared attention architecture\n",
    "    )\n",
    "\n",
    "print(\"Model factory function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Multiple Data Splits for Ensemble Training\n",
    "\n",
    "Instead of using the same train/eval/test split with different shuffles, we'll create 5 completely different splits from the original dataset. This provides much better diversity for ensemble training and more robust uncertainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 5 different data splits from the original dataset...\n",
      "Each split will have different train/eval/test samples for better ensemble diversity\n",
      "Creating data split 1/5 with seed 1234\n",
      "Split 1: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 2/5 with seed 2234\n",
      "Split 1: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 2/5 with seed 2234\n",
      "Split 2: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 3/5 with seed 3234\n",
      "Split 2: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 3/5 with seed 3234\n",
      "Split 3: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 4/5 with seed 4234\n",
      "Split 3: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 4/5 with seed 4234\n",
      "Split 4: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 5/5 with seed 5234\n",
      "Split 4: Train=212569, Eval=91101, Test=33742\n",
      "Creating data split 5/5 with seed 5234\n",
      "Split 5: Train=212569, Eval=91101, Test=33742\n",
      "\n",
      "Successfully created 5 different data splits!\n",
      "Summary of splits:\n",
      "Split 1: Train=212569, Eval=91101, Test=33742, Seed=1234\n",
      "Split 2: Train=212569, Eval=91101, Test=33742, Seed=2234\n",
      "Split 3: Train=212569, Eval=91101, Test=33742, Seed=3234\n",
      "Split 4: Train=212569, Eval=91101, Test=33742, Seed=4234\n",
      "Split 5: Train=212569, Eval=91101, Test=33742, Seed=5234\n",
      "Split 5: Train=212569, Eval=91101, Test=33742\n",
      "\n",
      "Successfully created 5 different data splits!\n",
      "Summary of splits:\n",
      "Split 1: Train=212569, Eval=91101, Test=33742, Seed=1234\n",
      "Split 2: Train=212569, Eval=91101, Test=33742, Seed=2234\n",
      "Split 3: Train=212569, Eval=91101, Test=33742, Seed=3234\n",
      "Split 4: Train=212569, Eval=91101, Test=33742, Seed=4234\n",
      "Split 5: Train=212569, Eval=91101, Test=33742, Seed=5234\n"
     ]
    }
   ],
   "source": [
    "# Import the updated uncertainty module with multiple splits functionality\n",
    "from uncertainty import create_multiple_data_splits, train_deep_ensemble\n",
    "\n",
    "# Create 5 different train/eval/test splits from the original dataset\n",
    "print(\"Creating 5 different data splits from the original dataset...\")\n",
    "print(\"Each split will have different train/eval/test samples for better ensemble diversity\")\n",
    "\n",
    "data_splits = create_multiple_data_splits(\n",
    "    original_dataset=manifesto_dataset,\n",
    "    num_splits=5,\n",
    "    test_size=0.1,  # 10% for test\n",
    "    eval_size=0.3,  # 30% of remaining data for eval\n",
    "    stratify_column='topic_sentiment',\n",
    "    base_seed=seed_val\n",
    ")\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(data_splits)} different data splits!\")\n",
    "print(\"Summary of splits:\")\n",
    "for i, split in enumerate(data_splits):\n",
    "    print(f\"Split {i+1}: Train={len(split['train'])}, Eval={len(split['eval'])}, Test={len(split['test'])}, Seed={split['seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Ensemble Training with Different Data Splits\n",
    "\n",
    "Now let's train the ensemble using the different data splits instead of just shuffling the same split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Deep Ensemble Training with Different Data Splits\n",
      "This will train 5 models using completely different train/eval/test splits\n",
      "Each model will be trained for 5 epochs\n",
      "This approach provides much better diversity than shuffling the same split\n",
      "\n",
      "==================================================\n",
      "Training ensemble member 1/5\n",
      "Using data split 0 with seed 1234\n",
      "Train size: 212569, Eval size: 91101\n",
      "==================================================\n",
      "Tokenizing datasets for split 1...\n",
      "Tokenizing datasets for split 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a5925ec83c42f39f41ce0874dbaec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/212569 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee414f4b9c5c4de1b563b2bfa2ea961a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble member 0\n",
      "Epoch: 1/5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=3.773974, elapsed=4.37s, remaining=550.27s.\n",
      "Batch 100: loss=3.773974, elapsed=4.37s, remaining=550.27s.\n",
      "Batch 200: loss=3.686030, elapsed=8.30s, remaining=520.80s.\n",
      "Batch 200: loss=3.686030, elapsed=8.30s, remaining=520.80s.\n",
      "Batch 300: loss=3.594719, elapsed=12.30s, remaining=510.79s.\n",
      "Batch 300: loss=3.594719, elapsed=12.30s, remaining=510.79s.\n",
      "Batch 400: loss=3.529894, elapsed=16.14s, remaining=499.13s.\n",
      "Batch 400: loss=3.529894, elapsed=16.14s, remaining=499.13s.\n",
      "Batch 500: loss=3.472163, elapsed=20.09s, remaining=493.13s.\n",
      "Batch 500: loss=3.472163, elapsed=20.09s, remaining=493.13s.\n",
      "Batch 600: loss=3.438552, elapsed=24.02s, remaining=487.45s.\n",
      "Batch 600: loss=3.438552, elapsed=24.02s, remaining=487.45s.\n",
      "Batch 700: loss=3.405638, elapsed=27.94s, remaining=482.19s.\n",
      "Batch 700: loss=3.405638, elapsed=27.94s, remaining=482.19s.\n",
      "Batch 800: loss=3.389901, elapsed=31.89s, remaining=477.59s.\n",
      "Batch 800: loss=3.389901, elapsed=31.89s, remaining=477.59s.\n",
      "Batch 900: loss=3.373032, elapsed=35.85s, remaining=473.38s.\n",
      "Batch 900: loss=3.373032, elapsed=35.85s, remaining=473.38s.\n",
      "Batch 1000: loss=3.358007, elapsed=39.79s, remaining=468.91s.\n",
      "Batch 1000: loss=3.358007, elapsed=39.79s, remaining=468.91s.\n",
      "Batch 1100: loss=3.347760, elapsed=43.74s, remaining=464.66s.\n",
      "Batch 1100: loss=3.347760, elapsed=43.74s, remaining=464.66s.\n",
      "Batch 1200: loss=3.334723, elapsed=47.74s, remaining=460.96s.\n",
      "Batch 1200: loss=3.334723, elapsed=47.74s, remaining=460.96s.\n",
      "Batch 1300: loss=3.314681, elapsed=51.87s, remaining=458.28s.\n",
      "Batch 1300: loss=3.314681, elapsed=51.87s, remaining=458.28s.\n",
      "Batch 1400: loss=3.266160, elapsed=55.95s, remaining=454.97s.\n",
      "Batch 1400: loss=3.266160, elapsed=55.95s, remaining=454.97s.\n",
      "Batch 1500: loss=3.199977, elapsed=59.92s, remaining=450.74s.\n",
      "Batch 1500: loss=3.199977, elapsed=59.92s, remaining=450.74s.\n",
      "Batch 1600: loss=3.133136, elapsed=63.95s, remaining=447.03s.\n",
      "Batch 1600: loss=3.133136, elapsed=63.95s, remaining=447.03s.\n",
      "Batch 1700: loss=3.068869, elapsed=67.82s, remaining=442.25s.\n",
      "Batch 1700: loss=3.068869, elapsed=67.82s, remaining=442.25s.\n",
      "Batch 1800: loss=3.001791, elapsed=71.87s, remaining=438.60s.\n",
      "Batch 1800: loss=3.001791, elapsed=71.87s, remaining=438.60s.\n",
      "Batch 1900: loss=2.939233, elapsed=75.84s, remaining=434.48s.\n",
      "Batch 1900: loss=2.939233, elapsed=75.84s, remaining=434.48s.\n",
      "Batch 2000: loss=2.883199, elapsed=79.89s, remaining=430.80s.\n",
      "Batch 2000: loss=2.883199, elapsed=79.89s, remaining=430.80s.\n",
      "Batch 2100: loss=2.826198, elapsed=83.91s, remaining=426.97s.\n",
      "Batch 2100: loss=2.826198, elapsed=83.91s, remaining=426.97s.\n",
      "Batch 2200: loss=2.771415, elapsed=87.89s, remaining=422.93s.\n",
      "Batch 2200: loss=2.771415, elapsed=87.89s, remaining=422.93s.\n",
      "Batch 2300: loss=2.720222, elapsed=92.06s, remaining=419.74s.\n",
      "Batch 2300: loss=2.720222, elapsed=92.06s, remaining=419.74s.\n",
      "Batch 2400: loss=2.672681, elapsed=96.00s, remaining=415.51s.\n",
      "Batch 2400: loss=2.672681, elapsed=96.00s, remaining=415.51s.\n",
      "Batch 2500: loss=2.629165, elapsed=100.06s, remaining=411.77s.\n",
      "Batch 2500: loss=2.629165, elapsed=100.06s, remaining=411.77s.\n",
      "Batch 2600: loss=2.588286, elapsed=104.07s, remaining=407.81s.\n",
      "Batch 2600: loss=2.588286, elapsed=104.07s, remaining=407.81s.\n",
      "Batch 2700: loss=2.551319, elapsed=108.02s, remaining=403.63s.\n",
      "Batch 2700: loss=2.551319, elapsed=108.02s, remaining=403.63s.\n",
      "Batch 2800: loss=2.513087, elapsed=112.17s, remaining=400.18s.\n",
      "Batch 2800: loss=2.513087, elapsed=112.17s, remaining=400.18s.\n",
      "Batch 2900: loss=2.477501, elapsed=116.29s, remaining=396.59s.\n",
      "Batch 2900: loss=2.477501, elapsed=116.29s, remaining=396.59s.\n",
      "Batch 3000: loss=2.445685, elapsed=120.41s, remaining=392.97s.\n",
      "Batch 3000: loss=2.445685, elapsed=120.41s, remaining=392.97s.\n",
      "Batch 3100: loss=2.414451, elapsed=124.46s, remaining=389.09s.\n",
      "Batch 3100: loss=2.414451, elapsed=124.46s, remaining=389.09s.\n",
      "Batch 3200: loss=2.386020, elapsed=128.57s, remaining=385.33s.\n",
      "Batch 3200: loss=2.386020, elapsed=128.57s, remaining=385.33s.\n",
      "Batch 3300: loss=2.357791, elapsed=132.51s, remaining=381.10s.\n",
      "Batch 3300: loss=2.357791, elapsed=132.51s, remaining=381.10s.\n",
      "Batch 3400: loss=2.330273, elapsed=136.48s, remaining=376.96s.\n",
      "Batch 3400: loss=2.330273, elapsed=136.48s, remaining=376.96s.\n",
      "Batch 3500: loss=2.305057, elapsed=140.44s, remaining=372.80s.\n",
      "Batch 3500: loss=2.305057, elapsed=140.44s, remaining=372.80s.\n",
      "Batch 3600: loss=2.282607, elapsed=144.45s, remaining=368.77s.\n",
      "Batch 3600: loss=2.282607, elapsed=144.45s, remaining=368.77s.\n",
      "Batch 3700: loss=2.258784, elapsed=148.45s, remaining=364.75s.\n",
      "Batch 3700: loss=2.258784, elapsed=148.45s, remaining=364.75s.\n",
      "Batch 3800: loss=2.235285, elapsed=152.55s, remaining=360.95s.\n",
      "Batch 3800: loss=2.235285, elapsed=152.55s, remaining=360.95s.\n",
      "Batch 3900: loss=2.216272, elapsed=156.59s, remaining=356.99s.\n",
      "Batch 3900: loss=2.216272, elapsed=156.59s, remaining=356.99s.\n",
      "Batch 4000: loss=2.195268, elapsed=160.66s, remaining=353.10s.\n",
      "Batch 4000: loss=2.195268, elapsed=160.66s, remaining=353.10s.\n",
      "Batch 4100: loss=2.175701, elapsed=164.70s, remaining=349.15s.\n",
      "Batch 4100: loss=2.175701, elapsed=164.70s, remaining=349.15s.\n",
      "Batch 4200: loss=2.157061, elapsed=168.67s, remaining=345.04s.\n",
      "Batch 4200: loss=2.157061, elapsed=168.67s, remaining=345.04s.\n",
      "Batch 4300: loss=2.139109, elapsed=172.76s, remaining=341.17s.\n",
      "Batch 4300: loss=2.139109, elapsed=172.76s, remaining=341.17s.\n",
      "Batch 4400: loss=2.122521, elapsed=176.75s, remaining=337.11s.\n",
      "Batch 4400: loss=2.122521, elapsed=176.75s, remaining=337.11s.\n",
      "Batch 4500: loss=2.105232, elapsed=180.72s, remaining=333.01s.\n",
      "Batch 4500: loss=2.105232, elapsed=180.72s, remaining=333.01s.\n",
      "Batch 4600: loss=2.089497, elapsed=184.85s, remaining=329.22s.\n",
      "Batch 4600: loss=2.089497, elapsed=184.85s, remaining=329.22s.\n",
      "Batch 4700: loss=2.073702, elapsed=189.07s, remaining=325.55s.\n",
      "Batch 4700: loss=2.073702, elapsed=189.07s, remaining=325.55s.\n",
      "Batch 4800: loss=2.058335, elapsed=193.25s, remaining=321.74s.\n",
      "Batch 4800: loss=2.058335, elapsed=193.25s, remaining=321.74s.\n",
      "Batch 4900: loss=2.043209, elapsed=197.23s, remaining=317.64s.\n",
      "Batch 4900: loss=2.043209, elapsed=197.23s, remaining=317.64s.\n",
      "Batch 5000: loss=2.029334, elapsed=201.23s, remaining=313.57s.\n",
      "Batch 5000: loss=2.029334, elapsed=201.23s, remaining=313.57s.\n",
      "Batch 5100: loss=2.016928, elapsed=205.28s, remaining=309.60s.\n",
      "Batch 5100: loss=2.016928, elapsed=205.28s, remaining=309.60s.\n",
      "Batch 5200: loss=2.004267, elapsed=209.35s, remaining=305.65s.\n",
      "Batch 5200: loss=2.004267, elapsed=209.35s, remaining=305.65s.\n",
      "Batch 5300: loss=1.991245, elapsed=213.43s, remaining=301.71s.\n",
      "Batch 5300: loss=1.991245, elapsed=213.43s, remaining=301.71s.\n",
      "Batch 5400: loss=1.979558, elapsed=217.47s, remaining=297.70s.\n",
      "Batch 5400: loss=1.979558, elapsed=217.47s, remaining=297.70s.\n",
      "Batch 5500: loss=1.966664, elapsed=221.58s, remaining=293.79s.\n",
      "Batch 5500: loss=1.966664, elapsed=221.58s, remaining=293.79s.\n",
      "Batch 5600: loss=1.955150, elapsed=225.65s, remaining=289.83s.\n",
      "Batch 5600: loss=1.955150, elapsed=225.65s, remaining=289.83s.\n",
      "Batch 5700: loss=1.943859, elapsed=229.64s, remaining=285.76s.\n",
      "Batch 5700: loss=1.943859, elapsed=229.64s, remaining=285.76s.\n",
      "Batch 5800: loss=1.934363, elapsed=233.71s, remaining=281.78s.\n",
      "Batch 5800: loss=1.934363, elapsed=233.71s, remaining=281.78s.\n",
      "Batch 5900: loss=1.922808, elapsed=237.78s, remaining=277.81s.\n",
      "Batch 5900: loss=1.922808, elapsed=237.78s, remaining=277.81s.\n",
      "Batch 6000: loss=1.912966, elapsed=241.69s, remaining=273.64s.\n",
      "Batch 6000: loss=1.912966, elapsed=241.69s, remaining=273.64s.\n",
      "Batch 6100: loss=1.902191, elapsed=245.66s, remaining=269.55s.\n",
      "Batch 6100: loss=1.902191, elapsed=245.66s, remaining=269.55s.\n",
      "Batch 6200: loss=1.892948, elapsed=249.78s, remaining=265.62s.\n",
      "Batch 6200: loss=1.892948, elapsed=249.78s, remaining=265.62s.\n",
      "Batch 6300: loss=1.883177, elapsed=253.83s, remaining=261.59s.\n",
      "Batch 6300: loss=1.883177, elapsed=253.83s, remaining=261.59s.\n",
      "Batch 6400: loss=1.873241, elapsed=257.85s, remaining=257.55s.\n",
      "Batch 6400: loss=1.873241, elapsed=257.85s, remaining=257.55s.\n",
      "Batch 6500: loss=1.863047, elapsed=261.84s, remaining=253.49s.\n",
      "Batch 6500: loss=1.863047, elapsed=261.84s, remaining=253.49s.\n",
      "Batch 6600: loss=1.854518, elapsed=265.93s, remaining=249.52s.\n",
      "Batch 6600: loss=1.854518, elapsed=265.93s, remaining=249.52s.\n",
      "Batch 6700: loss=1.846159, elapsed=270.03s, remaining=245.54s.\n",
      "Batch 6700: loss=1.846159, elapsed=270.03s, remaining=245.54s.\n",
      "Batch 6800: loss=1.837212, elapsed=274.20s, remaining=241.64s.\n",
      "Batch 6800: loss=1.837212, elapsed=274.20s, remaining=241.64s.\n",
      "Batch 6900: loss=1.828694, elapsed=278.22s, remaining=237.60s.\n",
      "Batch 6900: loss=1.828694, elapsed=278.22s, remaining=237.60s.\n",
      "Batch 7000: loss=1.819521, elapsed=282.31s, remaining=233.62s.\n",
      "Batch 7000: loss=1.819521, elapsed=282.31s, remaining=233.62s.\n",
      "Batch 7100: loss=1.811164, elapsed=286.33s, remaining=229.58s.\n",
      "Batch 7100: loss=1.811164, elapsed=286.33s, remaining=229.58s.\n",
      "Batch 7200: loss=1.803798, elapsed=290.34s, remaining=225.53s.\n",
      "Batch 7200: loss=1.803798, elapsed=290.34s, remaining=225.53s.\n",
      "Batch 7300: loss=1.797035, elapsed=294.35s, remaining=221.49s.\n",
      "Batch 7300: loss=1.797035, elapsed=294.35s, remaining=221.49s.\n",
      "Batch 7400: loss=1.789926, elapsed=298.37s, remaining=217.44s.\n",
      "Batch 7400: loss=1.789926, elapsed=298.37s, remaining=217.44s.\n",
      "Batch 7500: loss=1.782902, elapsed=302.32s, remaining=213.35s.\n",
      "Batch 7500: loss=1.782902, elapsed=302.32s, remaining=213.35s.\n",
      "Batch 7600: loss=1.775939, elapsed=306.37s, remaining=209.34s.\n",
      "Batch 7600: loss=1.775939, elapsed=306.37s, remaining=209.34s.\n",
      "Batch 7700: loss=1.769535, elapsed=310.39s, remaining=205.28s.\n",
      "Batch 7700: loss=1.769535, elapsed=310.39s, remaining=205.28s.\n",
      "Batch 7800: loss=1.762663, elapsed=314.52s, remaining=201.32s.\n",
      "Batch 7800: loss=1.762663, elapsed=314.52s, remaining=201.32s.\n",
      "Batch 7900: loss=1.755739, elapsed=318.57s, remaining=197.30s.\n",
      "Batch 7900: loss=1.755739, elapsed=318.57s, remaining=197.30s.\n",
      "Batch 8000: loss=1.749088, elapsed=322.77s, remaining=193.38s.\n",
      "Batch 8000: loss=1.749088, elapsed=322.77s, remaining=193.38s.\n",
      "Batch 8100: loss=1.742166, elapsed=326.69s, remaining=189.28s.\n",
      "Batch 8100: loss=1.742166, elapsed=326.69s, remaining=189.28s.\n",
      "Batch 8200: loss=1.734990, elapsed=330.71s, remaining=185.24s.\n",
      "Batch 8200: loss=1.734990, elapsed=330.71s, remaining=185.24s.\n",
      "Batch 8300: loss=1.729525, elapsed=334.89s, remaining=181.29s.\n",
      "Batch 8300: loss=1.729525, elapsed=334.89s, remaining=181.29s.\n",
      "Batch 8400: loss=1.724173, elapsed=338.91s, remaining=177.25s.\n",
      "Batch 8400: loss=1.724173, elapsed=338.91s, remaining=177.25s.\n",
      "Batch 8500: loss=1.717951, elapsed=342.97s, remaining=173.23s.\n",
      "Batch 8500: loss=1.717951, elapsed=342.97s, remaining=173.23s.\n",
      "Batch 8600: loss=1.711966, elapsed=347.16s, remaining=169.26s.\n",
      "Batch 8600: loss=1.711966, elapsed=347.16s, remaining=169.26s.\n",
      "Batch 8700: loss=1.706039, elapsed=351.18s, remaining=165.20s.\n",
      "Batch 8700: loss=1.706039, elapsed=351.18s, remaining=165.20s.\n",
      "Batch 8800: loss=1.700597, elapsed=355.36s, remaining=161.20s.\n",
      "Batch 8800: loss=1.700597, elapsed=355.36s, remaining=161.20s.\n",
      "Batch 8900: loss=1.695048, elapsed=359.31s, remaining=157.11s.\n",
      "Batch 8900: loss=1.695048, elapsed=359.31s, remaining=157.11s.\n",
      "Batch 9000: loss=1.690396, elapsed=363.30s, remaining=153.06s.\n",
      "Batch 9000: loss=1.690396, elapsed=363.30s, remaining=153.06s.\n",
      "Batch 9100: loss=1.684689, elapsed=367.29s, remaining=149.01s.\n",
      "Batch 9100: loss=1.684689, elapsed=367.29s, remaining=149.01s.\n",
      "Batch 9200: loss=1.679277, elapsed=371.28s, remaining=144.95s.\n",
      "Batch 9200: loss=1.679277, elapsed=371.28s, remaining=144.95s.\n",
      "Batch 9300: loss=1.673979, elapsed=375.34s, remaining=140.93s.\n",
      "Batch 9300: loss=1.673979, elapsed=375.34s, remaining=140.93s.\n",
      "Batch 9400: loss=1.668127, elapsed=379.30s, remaining=136.87s.\n",
      "Batch 9400: loss=1.668127, elapsed=379.30s, remaining=136.87s.\n",
      "Batch 9500: loss=1.662654, elapsed=383.37s, remaining=132.85s.\n",
      "Batch 9500: loss=1.662654, elapsed=383.37s, remaining=132.85s.\n",
      "Batch 9600: loss=1.657473, elapsed=387.48s, remaining=128.85s.\n",
      "Batch 9600: loss=1.657473, elapsed=387.48s, remaining=128.85s.\n",
      "Batch 9700: loss=1.653148, elapsed=391.46s, remaining=124.79s.\n",
      "Batch 9700: loss=1.653148, elapsed=391.46s, remaining=124.79s.\n",
      "Batch 9800: loss=1.648245, elapsed=395.51s, remaining=120.75s.\n",
      "Batch 9800: loss=1.648245, elapsed=395.51s, remaining=120.75s.\n",
      "Batch 9900: loss=1.644157, elapsed=399.75s, remaining=116.75s.\n",
      "Batch 9900: loss=1.644157, elapsed=399.75s, remaining=116.75s.\n",
      "Batch 10000: loss=1.639057, elapsed=403.80s, remaining=112.69s.\n",
      "Batch 10000: loss=1.639057, elapsed=403.80s, remaining=112.69s.\n",
      "Batch 10100: loss=1.635571, elapsed=407.77s, remaining=108.63s.\n",
      "Batch 10100: loss=1.635571, elapsed=407.77s, remaining=108.63s.\n",
      "Batch 10200: loss=1.630379, elapsed=411.96s, remaining=104.61s.\n",
      "Batch 10200: loss=1.630379, elapsed=411.96s, remaining=104.61s.\n",
      "Batch 10300: loss=1.625967, elapsed=415.99s, remaining=100.57s.\n",
      "Batch 10300: loss=1.625967, elapsed=415.99s, remaining=100.57s.\n",
      "Batch 10400: loss=1.621788, elapsed=420.07s, remaining=96.54s.\n",
      "Batch 10400: loss=1.621788, elapsed=420.07s, remaining=96.54s.\n",
      "Batch 10500: loss=1.616811, elapsed=424.31s, remaining=92.54s.\n",
      "Batch 10500: loss=1.616811, elapsed=424.31s, remaining=92.54s.\n",
      "Batch 10600: loss=1.612773, elapsed=428.33s, remaining=88.49s.\n",
      "Batch 10600: loss=1.612773, elapsed=428.33s, remaining=88.49s.\n",
      "Batch 10700: loss=1.608765, elapsed=432.35s, remaining=84.45s.\n",
      "Batch 10700: loss=1.608765, elapsed=432.35s, remaining=84.45s.\n",
      "Batch 10800: loss=1.604355, elapsed=436.35s, remaining=80.41s.\n",
      "Batch 10800: loss=1.604355, elapsed=436.35s, remaining=80.41s.\n",
      "Batch 10900: loss=1.600138, elapsed=440.41s, remaining=76.37s.\n",
      "Batch 10900: loss=1.600138, elapsed=440.41s, remaining=76.37s.\n",
      "Batch 11000: loss=1.595888, elapsed=444.40s, remaining=72.32s.\n",
      "Batch 11000: loss=1.595888, elapsed=444.40s, remaining=72.32s.\n",
      "Batch 11100: loss=1.591968, elapsed=448.44s, remaining=68.29s.\n",
      "Batch 11100: loss=1.591968, elapsed=448.44s, remaining=68.29s.\n",
      "Batch 11200: loss=1.588567, elapsed=452.49s, remaining=64.25s.\n",
      "Batch 11200: loss=1.588567, elapsed=452.49s, remaining=64.25s.\n",
      "Batch 11300: loss=1.585023, elapsed=456.63s, remaining=60.23s.\n",
      "Batch 11300: loss=1.585023, elapsed=456.63s, remaining=60.23s.\n",
      "Batch 11400: loss=1.581489, elapsed=460.67s, remaining=56.19s.\n",
      "Batch 11400: loss=1.581489, elapsed=460.67s, remaining=56.19s.\n",
      "Batch 11500: loss=1.577847, elapsed=464.51s, remaining=52.13s.\n",
      "Batch 11500: loss=1.577847, elapsed=464.51s, remaining=52.13s.\n",
      "Batch 11600: loss=1.573752, elapsed=468.49s, remaining=48.09s.\n",
      "Batch 11600: loss=1.573752, elapsed=468.49s, remaining=48.09s.\n",
      "Batch 11700: loss=1.569801, elapsed=472.61s, remaining=44.06s.\n",
      "Batch 11700: loss=1.569801, elapsed=472.61s, remaining=44.06s.\n",
      "Batch 11800: loss=1.566127, elapsed=476.70s, remaining=40.03s.\n",
      "Batch 11800: loss=1.566127, elapsed=476.70s, remaining=40.03s.\n",
      "Batch 11900: loss=1.562671, elapsed=480.64s, remaining=35.98s.\n",
      "Batch 11900: loss=1.562671, elapsed=480.64s, remaining=35.98s.\n",
      "Batch 12000: loss=1.558772, elapsed=484.80s, remaining=31.95s.\n",
      "Batch 12000: loss=1.558772, elapsed=484.80s, remaining=31.95s.\n",
      "Batch 12100: loss=1.555329, elapsed=488.85s, remaining=27.92s.\n",
      "Batch 12100: loss=1.555329, elapsed=488.85s, remaining=27.92s.\n",
      "Batch 12200: loss=1.551882, elapsed=493.04s, remaining=23.89s.\n",
      "Batch 12200: loss=1.551882, elapsed=493.04s, remaining=23.89s.\n",
      "Batch 12300: loss=1.548560, elapsed=496.95s, remaining=19.84s.\n",
      "Batch 12300: loss=1.548560, elapsed=496.95s, remaining=19.84s.\n",
      "Batch 12400: loss=1.544867, elapsed=500.94s, remaining=15.80s.\n",
      "Batch 12400: loss=1.544867, elapsed=500.94s, remaining=15.80s.\n",
      "Batch 12500: loss=1.541070, elapsed=504.99s, remaining=11.77s.\n",
      "Batch 12500: loss=1.541070, elapsed=504.99s, remaining=11.77s.\n",
      "Batch 12600: loss=1.538325, elapsed=509.04s, remaining=7.73s.\n",
      "Batch 12600: loss=1.538325, elapsed=509.04s, remaining=7.73s.\n",
      "Batch 12700: loss=1.534953, elapsed=513.10s, remaining=3.70s.\n",
      "Batch 12700: loss=1.534953, elapsed=513.10s, remaining=3.70s.\n",
      "Batch 12800: loss=1.531359, elapsed=517.09s, remaining=-0.34s.\n",
      "Batch 12800: loss=1.531359, elapsed=517.09s, remaining=-0.34s.\n",
      "Batch 12900: loss=1.528415, elapsed=521.09s, remaining=-4.38s.\n",
      "Batch 12900: loss=1.528415, elapsed=521.09s, remaining=-4.38s.\n",
      "Batch 13000: loss=1.525392, elapsed=525.12s, remaining=-8.42s.\n",
      "Batch 13000: loss=1.525392, elapsed=525.12s, remaining=-8.42s.\n",
      "Batch 13100: loss=1.522032, elapsed=529.20s, remaining=-12.45s.\n",
      "Batch 13100: loss=1.522032, elapsed=529.20s, remaining=-12.45s.\n",
      "Batch 13200: loss=1.519295, elapsed=533.14s, remaining=-16.49s.\n",
      "Batch 13200: loss=1.519295, elapsed=533.14s, remaining=-16.49s.\n",
      "\n",
      "Training epoch took: 536.52s\n",
      "\n",
      "Training epoch took: 536.52s\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 1.071625 \n",
      "\n",
      "Accuracy - Sentiment: 80.8%, Avg loss: 1.071625 \n",
      "\n",
      "Accuracy - Topic: 79.9%, Avg loss: 1.071625 \n",
      "\n",
      "Epoch: 2/5\n",
      "\n",
      "Training...\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 1.071625 \n",
      "\n",
      "Accuracy - Sentiment: 80.8%, Avg loss: 1.071625 \n",
      "\n",
      "Accuracy - Topic: 79.9%, Avg loss: 1.071625 \n",
      "\n",
      "Epoch: 2/5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=1.112232, elapsed=4.05s, remaining=508.83s.\n",
      "Batch 100: loss=1.112232, elapsed=4.05s, remaining=508.83s.\n",
      "Batch 200: loss=1.098048, elapsed=8.22s, remaining=515.19s.\n",
      "Batch 200: loss=1.098048, elapsed=8.22s, remaining=515.19s.\n",
      "Batch 300: loss=1.066580, elapsed=12.36s, remaining=513.28s.\n",
      "Batch 300: loss=1.066580, elapsed=12.36s, remaining=513.28s.\n",
      "Batch 400: loss=1.061502, elapsed=16.37s, remaining=506.17s.\n",
      "Batch 400: loss=1.061502, elapsed=16.37s, remaining=506.17s.\n",
      "Batch 500: loss=1.072452, elapsed=20.24s, remaining=496.76s.\n",
      "Batch 500: loss=1.072452, elapsed=20.24s, remaining=496.76s.\n",
      "Batch 600: loss=1.079306, elapsed=24.40s, remaining=495.20s.\n",
      "Batch 600: loss=1.079306, elapsed=24.40s, remaining=495.20s.\n",
      "Batch 700: loss=1.086047, elapsed=28.40s, remaining=490.13s.\n",
      "Batch 700: loss=1.086047, elapsed=28.40s, remaining=490.13s.\n",
      "Batch 800: loss=1.088644, elapsed=32.39s, remaining=485.23s.\n",
      "Batch 800: loss=1.088644, elapsed=32.39s, remaining=485.23s.\n",
      "Batch 900: loss=1.086099, elapsed=36.40s, remaining=480.73s.\n",
      "Batch 900: loss=1.086099, elapsed=36.40s, remaining=480.73s.\n",
      "Batch 1000: loss=1.084166, elapsed=40.42s, remaining=476.46s.\n",
      "Batch 1000: loss=1.084166, elapsed=40.42s, remaining=476.46s.\n",
      "Batch 1100: loss=1.092843, elapsed=44.45s, remaining=472.32s.\n",
      "Batch 1100: loss=1.092843, elapsed=44.45s, remaining=472.32s.\n",
      "Batch 1200: loss=1.095696, elapsed=48.47s, remaining=468.12s.\n",
      "Batch 1200: loss=1.095696, elapsed=48.47s, remaining=468.12s.\n",
      "Batch 1300: loss=1.091242, elapsed=52.51s, remaining=464.13s.\n",
      "Batch 1300: loss=1.091242, elapsed=52.51s, remaining=464.13s.\n",
      "Batch 1400: loss=1.089802, elapsed=56.54s, remaining=460.02s.\n",
      "Batch 1400: loss=1.089802, elapsed=56.54s, remaining=460.02s.\n",
      "Batch 1500: loss=1.087861, elapsed=60.53s, remaining=455.68s.\n",
      "Batch 1500: loss=1.087861, elapsed=60.53s, remaining=455.68s.\n",
      "Batch 1600: loss=1.087578, elapsed=64.51s, remaining=451.28s.\n",
      "Batch 1600: loss=1.087578, elapsed=64.51s, remaining=451.28s.\n",
      "Batch 1700: loss=1.086356, elapsed=68.56s, remaining=447.37s.\n",
      "Batch 1700: loss=1.086356, elapsed=68.56s, remaining=447.37s.\n",
      "Batch 1800: loss=1.087394, elapsed=72.42s, remaining=442.32s.\n",
      "Batch 1800: loss=1.087394, elapsed=72.42s, remaining=442.32s.\n",
      "Batch 1900: loss=1.085676, elapsed=76.48s, remaining=438.52s.\n",
      "Batch 1900: loss=1.085676, elapsed=76.48s, remaining=438.52s.\n",
      "Batch 2000: loss=1.086539, elapsed=80.55s, remaining=434.77s.\n",
      "Batch 2000: loss=1.086539, elapsed=80.55s, remaining=434.77s.\n",
      "Batch 2100: loss=1.088818, elapsed=84.53s, remaining=430.53s.\n",
      "Batch 2100: loss=1.088818, elapsed=84.53s, remaining=430.53s.\n",
      "Batch 2200: loss=1.090323, elapsed=88.68s, remaining=427.12s.\n",
      "Batch 2200: loss=1.090323, elapsed=88.68s, remaining=427.12s.\n",
      "Batch 2300: loss=1.089247, elapsed=92.79s, remaining=423.46s.\n",
      "Batch 2300: loss=1.089247, elapsed=92.79s, remaining=423.46s.\n",
      "Batch 2400: loss=1.088006, elapsed=96.73s, remaining=419.00s.\n",
      "Batch 2400: loss=1.088006, elapsed=96.73s, remaining=419.00s.\n",
      "Batch 2500: loss=1.085788, elapsed=100.72s, remaining=414.82s.\n",
      "Batch 2500: loss=1.085788, elapsed=100.72s, remaining=414.82s.\n",
      "Batch 2600: loss=1.084532, elapsed=104.82s, remaining=411.10s.\n",
      "Batch 2600: loss=1.084532, elapsed=104.82s, remaining=411.10s.\n",
      "Batch 2700: loss=1.085379, elapsed=108.85s, remaining=407.06s.\n",
      "Batch 2700: loss=1.085379, elapsed=108.85s, remaining=407.06s.\n",
      "Batch 2800: loss=1.084606, elapsed=112.71s, remaining=402.39s.\n",
      "Batch 2800: loss=1.084606, elapsed=112.71s, remaining=402.39s.\n",
      "Batch 2900: loss=1.084090, elapsed=116.72s, remaining=398.34s.\n",
      "Batch 2900: loss=1.084090, elapsed=116.72s, remaining=398.34s.\n",
      "Batch 3000: loss=1.082844, elapsed=120.78s, remaining=394.42s.\n",
      "Batch 3000: loss=1.082844, elapsed=120.78s, remaining=394.42s.\n",
      "Batch 3100: loss=1.082209, elapsed=124.86s, remaining=390.60s.\n",
      "Batch 3100: loss=1.082209, elapsed=124.86s, remaining=390.60s.\n",
      "Batch 3200: loss=1.081422, elapsed=128.94s, remaining=386.72s.\n",
      "Batch 3200: loss=1.081422, elapsed=128.94s, remaining=386.72s.\n",
      "Batch 3300: loss=1.079379, elapsed=132.98s, remaining=382.75s.\n",
      "Batch 3300: loss=1.079379, elapsed=132.98s, remaining=382.75s.\n",
      "Batch 3400: loss=1.079825, elapsed=137.02s, remaining=378.75s.\n",
      "Batch 3400: loss=1.079825, elapsed=137.02s, remaining=378.75s.\n",
      "Batch 3500: loss=1.080672, elapsed=141.11s, remaining=374.90s.\n",
      "Batch 3500: loss=1.080672, elapsed=141.11s, remaining=374.90s.\n",
      "Batch 3600: loss=1.079185, elapsed=145.07s, remaining=370.67s.\n",
      "Batch 3600: loss=1.079185, elapsed=145.07s, remaining=370.67s.\n",
      "Batch 3700: loss=1.077150, elapsed=149.03s, remaining=366.47s.\n",
      "Batch 3700: loss=1.077150, elapsed=149.03s, remaining=366.47s.\n",
      "Batch 3800: loss=1.076128, elapsed=153.06s, remaining=362.46s.\n",
      "Batch 3800: loss=1.076128, elapsed=153.06s, remaining=362.46s.\n",
      "Batch 3900: loss=1.075507, elapsed=156.99s, remaining=358.21s.\n",
      "Batch 3900: loss=1.075507, elapsed=156.99s, remaining=358.21s.\n",
      "Batch 4000: loss=1.074413, elapsed=160.93s, remaining=354.00s.\n",
      "Batch 4000: loss=1.074413, elapsed=160.93s, remaining=354.00s.\n",
      "Batch 4100: loss=1.074826, elapsed=164.99s, remaining=350.05s.\n",
      "Batch 4100: loss=1.074826, elapsed=164.99s, remaining=350.05s.\n",
      "Batch 4200: loss=1.075440, elapsed=169.03s, remaining=346.07s.\n",
      "Batch 4200: loss=1.075440, elapsed=169.03s, remaining=346.07s.\n",
      "Batch 4300: loss=1.076388, elapsed=173.12s, remaining=342.16s.\n",
      "Batch 4300: loss=1.076388, elapsed=173.12s, remaining=342.16s.\n",
      "Batch 4400: loss=1.076678, elapsed=177.16s, remaining=338.17s.\n",
      "Batch 4400: loss=1.076678, elapsed=177.16s, remaining=338.17s.\n",
      "Batch 4500: loss=1.076253, elapsed=181.16s, remaining=334.12s.\n",
      "Batch 4500: loss=1.076253, elapsed=181.16s, remaining=334.12s.\n",
      "Batch 4600: loss=1.076754, elapsed=185.19s, remaining=330.09s.\n",
      "Batch 4600: loss=1.076754, elapsed=185.19s, remaining=330.09s.\n",
      "Batch 4700: loss=1.076453, elapsed=189.20s, remaining=326.04s.\n",
      "Batch 4700: loss=1.076453, elapsed=189.20s, remaining=326.04s.\n",
      "Batch 4800: loss=1.076160, elapsed=193.23s, remaining=322.02s.\n",
      "Batch 4800: loss=1.076160, elapsed=193.23s, remaining=322.02s.\n",
      "Batch 4900: loss=1.077298, elapsed=197.36s, remaining=318.18s.\n",
      "Batch 4900: loss=1.077298, elapsed=197.36s, remaining=318.18s.\n",
      "Batch 5000: loss=1.076729, elapsed=201.34s, remaining=314.07s.\n",
      "Batch 5000: loss=1.076729, elapsed=201.34s, remaining=314.07s.\n",
      "Batch 5100: loss=1.076898, elapsed=205.31s, remaining=309.96s.\n",
      "Batch 5100: loss=1.076898, elapsed=205.31s, remaining=309.96s.\n",
      "Batch 5200: loss=1.075918, elapsed=209.40s, remaining=306.03s.\n",
      "Batch 5200: loss=1.075918, elapsed=209.40s, remaining=306.03s.\n",
      "Batch 5300: loss=1.075133, elapsed=213.49s, remaining=302.09s.\n",
      "Batch 5300: loss=1.075133, elapsed=213.49s, remaining=302.09s.\n",
      "Batch 5400: loss=1.074603, elapsed=217.60s, remaining=298.19s.\n",
      "Batch 5400: loss=1.074603, elapsed=217.60s, remaining=298.19s.\n",
      "Batch 5500: loss=1.073784, elapsed=221.62s, remaining=294.13s.\n",
      "Batch 5500: loss=1.073784, elapsed=221.62s, remaining=294.13s.\n",
      "Batch 5600: loss=1.073180, elapsed=225.59s, remaining=290.03s.\n",
      "Batch 5600: loss=1.073180, elapsed=225.59s, remaining=290.03s.\n",
      "Batch 5700: loss=1.072129, elapsed=229.63s, remaining=286.03s.\n",
      "Batch 5700: loss=1.072129, elapsed=229.63s, remaining=286.03s.\n",
      "Batch 5800: loss=1.071944, elapsed=233.76s, remaining=282.12s.\n",
      "Batch 5800: loss=1.071944, elapsed=233.76s, remaining=282.12s.\n",
      "Batch 5900: loss=1.070252, elapsed=237.79s, remaining=278.10s.\n",
      "Batch 5900: loss=1.070252, elapsed=237.79s, remaining=278.10s.\n",
      "Batch 6000: loss=1.068679, elapsed=241.82s, remaining=274.07s.\n",
      "Batch 6000: loss=1.068679, elapsed=241.82s, remaining=274.07s.\n",
      "Batch 6100: loss=1.067433, elapsed=245.73s, remaining=269.90s.\n",
      "Batch 6100: loss=1.067433, elapsed=245.73s, remaining=269.90s.\n",
      "Batch 6200: loss=1.066671, elapsed=249.71s, remaining=265.83s.\n",
      "Batch 6200: loss=1.066671, elapsed=249.71s, remaining=265.83s.\n",
      "Batch 6300: loss=1.065862, elapsed=253.81s, remaining=261.87s.\n",
      "Batch 6300: loss=1.065862, elapsed=253.81s, remaining=261.87s.\n",
      "Batch 6400: loss=1.065346, elapsed=257.67s, remaining=257.68s.\n",
      "Batch 6400: loss=1.065346, elapsed=257.67s, remaining=257.68s.\n",
      "Batch 6500: loss=1.064564, elapsed=261.73s, remaining=253.68s.\n",
      "Batch 6500: loss=1.064564, elapsed=261.73s, remaining=253.68s.\n",
      "Batch 6600: loss=1.064304, elapsed=265.80s, remaining=249.70s.\n",
      "Batch 6600: loss=1.064304, elapsed=265.80s, remaining=249.70s.\n",
      "Batch 6700: loss=1.063444, elapsed=269.87s, remaining=245.71s.\n",
      "Batch 6700: loss=1.063444, elapsed=269.87s, remaining=245.71s.\n",
      "Batch 6800: loss=1.062832, elapsed=273.90s, remaining=241.69s.\n",
      "Batch 6800: loss=1.062832, elapsed=273.90s, remaining=241.69s.\n",
      "Batch 6900: loss=1.062697, elapsed=277.99s, remaining=237.72s.\n",
      "Batch 6900: loss=1.062697, elapsed=277.99s, remaining=237.72s.\n",
      "Batch 7000: loss=1.062366, elapsed=282.06s, remaining=233.73s.\n",
      "Batch 7000: loss=1.062366, elapsed=282.06s, remaining=233.73s.\n",
      "Batch 7100: loss=1.061823, elapsed=286.02s, remaining=229.64s.\n",
      "Batch 7100: loss=1.061823, elapsed=286.02s, remaining=229.64s.\n",
      "Batch 7200: loss=1.061274, elapsed=290.04s, remaining=225.61s.\n",
      "Batch 7200: loss=1.061274, elapsed=290.04s, remaining=225.61s.\n",
      "Batch 7300: loss=1.061188, elapsed=293.95s, remaining=221.49s.\n",
      "Batch 7300: loss=1.061188, elapsed=293.95s, remaining=221.49s.\n",
      "Batch 7400: loss=1.060831, elapsed=297.97s, remaining=217.45s.\n",
      "Batch 7400: loss=1.060831, elapsed=297.97s, remaining=217.45s.\n",
      "Batch 7500: loss=1.059903, elapsed=302.00s, remaining=213.43s.\n",
      "Batch 7500: loss=1.059903, elapsed=302.00s, remaining=213.43s.\n",
      "Batch 7600: loss=1.058550, elapsed=306.01s, remaining=209.39s.\n",
      "Batch 7600: loss=1.058550, elapsed=306.01s, remaining=209.39s.\n",
      "Batch 7700: loss=1.058238, elapsed=309.90s, remaining=205.28s.\n",
      "Batch 7700: loss=1.058238, elapsed=309.90s, remaining=205.28s.\n",
      "Batch 7800: loss=1.057828, elapsed=313.97s, remaining=201.28s.\n",
      "Batch 7800: loss=1.057828, elapsed=313.97s, remaining=201.28s.\n",
      "Batch 7900: loss=1.057681, elapsed=317.99s, remaining=197.25s.\n",
      "Batch 7900: loss=1.057681, elapsed=317.99s, remaining=197.25s.\n",
      "Batch 8000: loss=1.057612, elapsed=322.05s, remaining=193.25s.\n",
      "Batch 8000: loss=1.057612, elapsed=322.05s, remaining=193.25s.\n",
      "Batch 8100: loss=1.057292, elapsed=326.16s, remaining=189.27s.\n",
      "Batch 8100: loss=1.057292, elapsed=326.16s, remaining=189.27s.\n",
      "Batch 8200: loss=1.056981, elapsed=330.21s, remaining=185.26s.\n",
      "Batch 8200: loss=1.056981, elapsed=330.21s, remaining=185.26s.\n",
      "Batch 8300: loss=1.057041, elapsed=334.41s, remaining=181.34s.\n",
      "Batch 8300: loss=1.057041, elapsed=334.41s, remaining=181.34s.\n",
      "Batch 8400: loss=1.057061, elapsed=338.42s, remaining=177.30s.\n",
      "Batch 8400: loss=1.057061, elapsed=338.42s, remaining=177.30s.\n",
      "Batch 8500: loss=1.056807, elapsed=342.52s, remaining=173.31s.\n",
      "Batch 8500: loss=1.056807, elapsed=342.52s, remaining=173.31s.\n",
      "Batch 8600: loss=1.056035, elapsed=346.51s, remaining=169.26s.\n",
      "Batch 8600: loss=1.056035, elapsed=346.51s, remaining=169.26s.\n",
      "Batch 8700: loss=1.055986, elapsed=350.61s, remaining=165.27s.\n",
      "Batch 8700: loss=1.055986, elapsed=350.61s, remaining=165.27s.\n",
      "Batch 8800: loss=1.055757, elapsed=354.81s, remaining=161.32s.\n",
      "Batch 8800: loss=1.055757, elapsed=354.81s, remaining=161.32s.\n",
      "Batch 8900: loss=1.055530, elapsed=358.81s, remaining=157.28s.\n",
      "Batch 8900: loss=1.055530, elapsed=358.81s, remaining=157.28s.\n",
      "Batch 9000: loss=1.055349, elapsed=362.93s, remaining=153.29s.\n",
      "Batch 9000: loss=1.055349, elapsed=362.93s, remaining=153.29s.\n",
      "Batch 9100: loss=1.054740, elapsed=366.95s, remaining=149.25s.\n",
      "Batch 9100: loss=1.054740, elapsed=366.95s, remaining=149.25s.\n",
      "Batch 9200: loss=1.054992, elapsed=370.97s, remaining=145.21s.\n",
      "Batch 9200: loss=1.054992, elapsed=370.97s, remaining=145.21s.\n",
      "Batch 9300: loss=1.054996, elapsed=375.03s, remaining=141.20s.\n",
      "Batch 9300: loss=1.054996, elapsed=375.03s, remaining=141.20s.\n",
      "Batch 9400: loss=1.054232, elapsed=379.08s, remaining=137.17s.\n",
      "Batch 9400: loss=1.054232, elapsed=379.08s, remaining=137.17s.\n",
      "Batch 9500: loss=1.053545, elapsed=383.19s, remaining=133.17s.\n",
      "Batch 9500: loss=1.053545, elapsed=383.19s, remaining=133.17s.\n",
      "Batch 9600: loss=1.053431, elapsed=387.21s, remaining=129.13s.\n",
      "Batch 9600: loss=1.053431, elapsed=387.21s, remaining=129.13s.\n",
      "Batch 9700: loss=1.053417, elapsed=391.36s, remaining=125.13s.\n",
      "Batch 9700: loss=1.053417, elapsed=391.36s, remaining=125.13s.\n",
      "Batch 9800: loss=1.052480, elapsed=395.45s, remaining=121.12s.\n",
      "Batch 9800: loss=1.052480, elapsed=395.45s, remaining=121.12s.\n",
      "Batch 9900: loss=1.052510, elapsed=399.50s, remaining=117.08s.\n",
      "Batch 9900: loss=1.052510, elapsed=399.50s, remaining=117.08s.\n",
      "Batch 10000: loss=1.051798, elapsed=403.52s, remaining=113.05s.\n",
      "Batch 10000: loss=1.051798, elapsed=403.52s, remaining=113.05s.\n",
      "Batch 10100: loss=1.051306, elapsed=407.61s, remaining=109.02s.\n",
      "Batch 10100: loss=1.051306, elapsed=407.61s, remaining=109.02s.\n",
      "Batch 10200: loss=1.050757, elapsed=411.60s, remaining=104.98s.\n",
      "Batch 10200: loss=1.050757, elapsed=411.60s, remaining=104.98s.\n",
      "Batch 10300: loss=1.050236, elapsed=415.71s, remaining=100.96s.\n",
      "Batch 10300: loss=1.050236, elapsed=415.71s, remaining=100.96s.\n",
      "Batch 10400: loss=1.050074, elapsed=419.80s, remaining=96.94s.\n",
      "Batch 10400: loss=1.050074, elapsed=419.80s, remaining=96.94s.\n",
      "Batch 10500: loss=1.049706, elapsed=423.79s, remaining=92.89s.\n",
      "Batch 10500: loss=1.049706, elapsed=423.79s, remaining=92.89s.\n",
      "Batch 10600: loss=1.049077, elapsed=427.83s, remaining=88.85s.\n",
      "Batch 10600: loss=1.049077, elapsed=427.83s, remaining=88.85s.\n",
      "Batch 10700: loss=1.048580, elapsed=431.91s, remaining=84.83s.\n",
      "Batch 10700: loss=1.048580, elapsed=431.91s, remaining=84.83s.\n",
      "Batch 10800: loss=1.048066, elapsed=435.94s, remaining=80.79s.\n",
      "Batch 10800: loss=1.048066, elapsed=435.94s, remaining=80.79s.\n",
      "Batch 10900: loss=1.047877, elapsed=439.96s, remaining=76.75s.\n",
      "Batch 10900: loss=1.047877, elapsed=439.96s, remaining=76.75s.\n",
      "Batch 11000: loss=1.047824, elapsed=443.96s, remaining=72.71s.\n",
      "Batch 11000: loss=1.047824, elapsed=443.96s, remaining=72.71s.\n",
      "Batch 11100: loss=1.047759, elapsed=447.97s, remaining=68.67s.\n",
      "Batch 11100: loss=1.047759, elapsed=447.97s, remaining=68.67s.\n",
      "Batch 11200: loss=1.047496, elapsed=452.05s, remaining=64.64s.\n",
      "Batch 11200: loss=1.047496, elapsed=452.05s, remaining=64.64s.\n",
      "Batch 11300: loss=1.047321, elapsed=456.12s, remaining=60.61s.\n",
      "Batch 11300: loss=1.047321, elapsed=456.12s, remaining=60.61s.\n",
      "Batch 11400: loss=1.046421, elapsed=460.08s, remaining=56.57s.\n",
      "Batch 11400: loss=1.046421, elapsed=460.08s, remaining=56.57s.\n",
      "Batch 11500: loss=1.045693, elapsed=464.12s, remaining=52.53s.\n",
      "Batch 11500: loss=1.045693, elapsed=464.12s, remaining=52.53s.\n",
      "Batch 11600: loss=1.045652, elapsed=468.05s, remaining=48.48s.\n",
      "Batch 11600: loss=1.045652, elapsed=468.05s, remaining=48.48s.\n",
      "Batch 11700: loss=1.045634, elapsed=472.05s, remaining=44.45s.\n",
      "Batch 11700: loss=1.045634, elapsed=472.05s, remaining=44.45s.\n",
      "Batch 11800: loss=1.045677, elapsed=476.12s, remaining=40.41s.\n",
      "Batch 11800: loss=1.045677, elapsed=476.12s, remaining=40.41s.\n",
      "Batch 11900: loss=1.044798, elapsed=480.19s, remaining=36.38s.\n",
      "Batch 11900: loss=1.044798, elapsed=480.19s, remaining=36.38s.\n",
      "Batch 12000: loss=1.044503, elapsed=484.19s, remaining=32.35s.\n",
      "Batch 12000: loss=1.044503, elapsed=484.19s, remaining=32.35s.\n",
      "Batch 12100: loss=1.044509, elapsed=488.41s, remaining=28.33s.\n",
      "Batch 12100: loss=1.044509, elapsed=488.41s, remaining=28.33s.\n",
      "Batch 12200: loss=1.044943, elapsed=492.39s, remaining=24.29s.\n",
      "Batch 12200: loss=1.044943, elapsed=492.39s, remaining=24.29s.\n",
      "Batch 12300: loss=1.043784, elapsed=496.34s, remaining=20.25s.\n",
      "Batch 12300: loss=1.043784, elapsed=496.34s, remaining=20.25s.\n",
      "Batch 12400: loss=1.043485, elapsed=500.54s, remaining=16.22s.\n",
      "Batch 12400: loss=1.043485, elapsed=500.54s, remaining=16.22s.\n",
      "Batch 12500: loss=1.043127, elapsed=504.70s, remaining=12.19s.\n",
      "Batch 12500: loss=1.043127, elapsed=504.70s, remaining=12.19s.\n",
      "Batch 12600: loss=1.042532, elapsed=508.65s, remaining=8.15s.\n",
      "Batch 12600: loss=1.042532, elapsed=508.65s, remaining=8.15s.\n",
      "Batch 12700: loss=1.041850, elapsed=512.61s, remaining=4.11s.\n",
      "Batch 12700: loss=1.041850, elapsed=512.61s, remaining=4.11s.\n",
      "Batch 12800: loss=1.041526, elapsed=516.87s, remaining=0.08s.\n",
      "Batch 12800: loss=1.041526, elapsed=516.87s, remaining=0.08s.\n",
      "Batch 12900: loss=1.041310, elapsed=520.88s, remaining=-3.96s.\n",
      "Batch 12900: loss=1.041310, elapsed=520.88s, remaining=-3.96s.\n",
      "Batch 13000: loss=1.040924, elapsed=524.96s, remaining=-7.99s.\n",
      "Batch 13000: loss=1.040924, elapsed=524.96s, remaining=-7.99s.\n",
      "Batch 13100: loss=1.040896, elapsed=529.01s, remaining=-12.03s.\n",
      "Batch 13100: loss=1.040896, elapsed=529.01s, remaining=-12.03s.\n",
      "Batch 13200: loss=1.040617, elapsed=533.12s, remaining=-16.07s.\n",
      "Batch 13200: loss=1.040617, elapsed=533.12s, remaining=-16.07s.\n",
      "\n",
      "Training epoch took: 536.60s\n",
      "\n",
      "Training epoch took: 536.60s\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.971222 \n",
      "\n",
      "Accuracy - Sentiment: 82.6%, Avg loss: 0.971222 \n",
      "\n",
      "Accuracy - Topic: 81.7%, Avg loss: 0.971222 \n",
      "\n",
      "Epoch: 3/5\n",
      "\n",
      "Training...\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.971222 \n",
      "\n",
      "Accuracy - Sentiment: 82.6%, Avg loss: 0.971222 \n",
      "\n",
      "Accuracy - Topic: 81.7%, Avg loss: 0.971222 \n",
      "\n",
      "Epoch: 3/5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.948231, elapsed=4.10s, remaining=515.42s.\n",
      "Batch 100: loss=0.948231, elapsed=4.10s, remaining=515.42s.\n",
      "Batch 200: loss=0.967168, elapsed=8.02s, remaining=502.55s.\n",
      "Batch 200: loss=0.967168, elapsed=8.02s, remaining=502.55s.\n",
      "Batch 300: loss=0.987041, elapsed=12.09s, remaining=501.76s.\n",
      "Batch 300: loss=0.987041, elapsed=12.09s, remaining=501.76s.\n",
      "Batch 400: loss=0.983996, elapsed=16.18s, remaining=500.35s.\n",
      "Batch 400: loss=0.983996, elapsed=16.18s, remaining=500.35s.\n",
      "Batch 500: loss=0.965427, elapsed=20.06s, remaining=492.37s.\n",
      "Batch 500: loss=0.965427, elapsed=20.06s, remaining=492.37s.\n",
      "Batch 600: loss=0.970602, elapsed=24.06s, remaining=488.42s.\n",
      "Batch 600: loss=0.970602, elapsed=24.06s, remaining=488.42s.\n",
      "Batch 700: loss=0.966531, elapsed=28.02s, remaining=483.54s.\n",
      "Batch 700: loss=0.966531, elapsed=28.02s, remaining=483.54s.\n",
      "Batch 800: loss=0.967280, elapsed=31.95s, remaining=478.65s.\n",
      "Batch 800: loss=0.967280, elapsed=31.95s, remaining=478.65s.\n",
      "Batch 900: loss=0.963667, elapsed=36.00s, remaining=475.47s.\n",
      "Batch 900: loss=0.963667, elapsed=36.00s, remaining=475.47s.\n",
      "Batch 1000: loss=0.964772, elapsed=39.97s, remaining=471.19s.\n",
      "Batch 1000: loss=0.964772, elapsed=39.97s, remaining=471.19s.\n",
      "Batch 1100: loss=0.962823, elapsed=44.20s, remaining=469.70s.\n",
      "Batch 1100: loss=0.962823, elapsed=44.20s, remaining=469.70s.\n",
      "Batch 1200: loss=0.958389, elapsed=48.16s, remaining=465.18s.\n",
      "Batch 1200: loss=0.958389, elapsed=48.16s, remaining=465.18s.\n",
      "Batch 1300: loss=0.959414, elapsed=52.21s, remaining=461.52s.\n",
      "Batch 1300: loss=0.959414, elapsed=52.21s, remaining=461.52s.\n",
      "Batch 1400: loss=0.957252, elapsed=56.10s, remaining=456.51s.\n",
      "Batch 1400: loss=0.957252, elapsed=56.10s, remaining=456.51s.\n",
      "Batch 1500: loss=0.956952, elapsed=60.16s, remaining=452.96s.\n",
      "Batch 1500: loss=0.956952, elapsed=60.16s, remaining=452.96s.\n",
      "Batch 1600: loss=0.955997, elapsed=64.09s, remaining=448.38s.\n",
      "Batch 1600: loss=0.955997, elapsed=64.09s, remaining=448.38s.\n",
      "Batch 1700: loss=0.955390, elapsed=68.13s, remaining=444.56s.\n",
      "Batch 1700: loss=0.955390, elapsed=68.13s, remaining=444.56s.\n",
      "Batch 1800: loss=0.956865, elapsed=72.19s, remaining=440.91s.\n",
      "Batch 1800: loss=0.956865, elapsed=72.19s, remaining=440.91s.\n",
      "Batch 1900: loss=0.956704, elapsed=76.25s, remaining=437.24s.\n",
      "Batch 1900: loss=0.956704, elapsed=76.25s, remaining=437.24s.\n",
      "Batch 2000: loss=0.955004, elapsed=80.34s, remaining=433.62s.\n",
      "Batch 2000: loss=0.955004, elapsed=80.34s, remaining=433.62s.\n",
      "Batch 2100: loss=0.952125, elapsed=84.41s, remaining=429.90s.\n",
      "Batch 2100: loss=0.952125, elapsed=84.41s, remaining=429.90s.\n",
      "Batch 2200: loss=0.951800, elapsed=88.48s, remaining=426.17s.\n",
      "Batch 2200: loss=0.951800, elapsed=88.48s, remaining=426.17s.\n",
      "Batch 2300: loss=0.950392, elapsed=92.56s, remaining=422.43s.\n",
      "Batch 2300: loss=0.950392, elapsed=92.56s, remaining=422.43s.\n",
      "Batch 2400: loss=0.948714, elapsed=96.62s, remaining=418.53s.\n",
      "Batch 2400: loss=0.948714, elapsed=96.62s, remaining=418.53s.\n",
      "Batch 2500: loss=0.948073, elapsed=100.67s, remaining=414.62s.\n",
      "Batch 2500: loss=0.948073, elapsed=100.67s, remaining=414.62s.\n",
      "Batch 2600: loss=0.951248, elapsed=104.62s, remaining=410.29s.\n",
      "Batch 2600: loss=0.951248, elapsed=104.62s, remaining=410.29s.\n",
      "Batch 2700: loss=0.951866, elapsed=108.69s, remaining=406.46s.\n",
      "Batch 2700: loss=0.951866, elapsed=108.69s, remaining=406.46s.\n",
      "Batch 2800: loss=0.952290, elapsed=112.76s, remaining=402.64s.\n",
      "Batch 2800: loss=0.952290, elapsed=112.76s, remaining=402.64s.\n",
      "Batch 2900: loss=0.951676, elapsed=116.85s, remaining=398.83s.\n",
      "Batch 2900: loss=0.951676, elapsed=116.85s, remaining=398.83s.\n",
      "Batch 3000: loss=0.954874, elapsed=120.83s, remaining=394.63s.\n",
      "Batch 3000: loss=0.954874, elapsed=120.83s, remaining=394.63s.\n",
      "Batch 3100: loss=0.956508, elapsed=124.91s, remaining=390.77s.\n",
      "Batch 3100: loss=0.956508, elapsed=124.91s, remaining=390.77s.\n",
      "Batch 3200: loss=0.957863, elapsed=128.86s, remaining=386.51s.\n",
      "Batch 3200: loss=0.957863, elapsed=128.86s, remaining=386.51s.\n",
      "Batch 3300: loss=0.960558, elapsed=132.99s, remaining=382.79s.\n",
      "Batch 3300: loss=0.960558, elapsed=132.99s, remaining=382.79s.\n",
      "Batch 3400: loss=0.960802, elapsed=137.07s, remaining=378.89s.\n",
      "Batch 3400: loss=0.960802, elapsed=137.07s, remaining=378.89s.\n",
      "Batch 3500: loss=0.959885, elapsed=141.17s, remaining=375.06s.\n",
      "Batch 3500: loss=0.959885, elapsed=141.17s, remaining=375.06s.\n",
      "Batch 3600: loss=0.960753, elapsed=145.20s, remaining=371.03s.\n",
      "Batch 3600: loss=0.960753, elapsed=145.20s, remaining=371.03s.\n",
      "Batch 3700: loss=0.960154, elapsed=149.27s, remaining=367.09s.\n",
      "Batch 3700: loss=0.960154, elapsed=149.27s, remaining=367.09s.\n",
      "Batch 3800: loss=0.959402, elapsed=153.39s, remaining=363.26s.\n",
      "Batch 3800: loss=0.959402, elapsed=153.39s, remaining=363.26s.\n",
      "Batch 3900: loss=0.959235, elapsed=157.40s, remaining=359.16s.\n",
      "Batch 3900: loss=0.959235, elapsed=157.40s, remaining=359.16s.\n",
      "Batch 4000: loss=0.959466, elapsed=161.50s, remaining=355.30s.\n",
      "Batch 4000: loss=0.959466, elapsed=161.50s, remaining=355.30s.\n",
      "Batch 4100: loss=0.959174, elapsed=165.49s, remaining=351.15s.\n",
      "Batch 4100: loss=0.959174, elapsed=165.49s, remaining=351.15s.\n",
      "Batch 4200: loss=0.959471, elapsed=169.49s, remaining=347.05s.\n",
      "Batch 4200: loss=0.959471, elapsed=169.49s, remaining=347.05s.\n",
      "Batch 4300: loss=0.959763, elapsed=173.59s, remaining=343.13s.\n",
      "Batch 4300: loss=0.959763, elapsed=173.59s, remaining=343.13s.\n",
      "Batch 4400: loss=0.960182, elapsed=177.60s, remaining=339.06s.\n",
      "Batch 4400: loss=0.960182, elapsed=177.60s, remaining=339.06s.\n",
      "Batch 4500: loss=0.959336, elapsed=181.58s, remaining=334.92s.\n",
      "Batch 4500: loss=0.959336, elapsed=181.58s, remaining=334.92s.\n",
      "Batch 4600: loss=0.959045, elapsed=185.66s, remaining=330.96s.\n",
      "Batch 4600: loss=0.959045, elapsed=185.66s, remaining=330.96s.\n",
      "Batch 4700: loss=0.958380, elapsed=189.65s, remaining=326.86s.\n",
      "Batch 4700: loss=0.958380, elapsed=189.65s, remaining=326.86s.\n",
      "Batch 4800: loss=0.957893, elapsed=193.72s, remaining=322.87s.\n",
      "Batch 4800: loss=0.957893, elapsed=193.72s, remaining=322.87s.\n",
      "Batch 4900: loss=0.960030, elapsed=197.82s, remaining=318.94s.\n",
      "Batch 4900: loss=0.960030, elapsed=197.82s, remaining=318.94s.\n",
      "Batch 5000: loss=0.960195, elapsed=201.86s, remaining=314.91s.\n",
      "Batch 5000: loss=0.960195, elapsed=201.86s, remaining=314.91s.\n",
      "Batch 5100: loss=0.959735, elapsed=205.89s, remaining=310.87s.\n",
      "Batch 5100: loss=0.959735, elapsed=205.89s, remaining=310.87s.\n",
      "Batch 5200: loss=0.960323, elapsed=209.90s, remaining=306.79s.\n",
      "Batch 5200: loss=0.960323, elapsed=209.90s, remaining=306.79s.\n",
      "Batch 5300: loss=0.960438, elapsed=213.89s, remaining=302.70s.\n",
      "Batch 5300: loss=0.960438, elapsed=213.89s, remaining=302.70s.\n",
      "Batch 5400: loss=0.958719, elapsed=217.94s, remaining=298.67s.\n",
      "Batch 5400: loss=0.958719, elapsed=217.94s, remaining=298.67s.\n",
      "Batch 5500: loss=0.958773, elapsed=221.89s, remaining=294.53s.\n",
      "Batch 5500: loss=0.958773, elapsed=221.89s, remaining=294.53s.\n",
      "Batch 5600: loss=0.959070, elapsed=225.91s, remaining=290.47s.\n",
      "Batch 5600: loss=0.959070, elapsed=225.91s, remaining=290.47s.\n",
      "Batch 5700: loss=0.960703, elapsed=229.98s, remaining=286.49s.\n",
      "Batch 5700: loss=0.960703, elapsed=229.98s, remaining=286.49s.\n",
      "Batch 5800: loss=0.960103, elapsed=234.02s, remaining=282.47s.\n",
      "Batch 5800: loss=0.960103, elapsed=234.02s, remaining=282.47s.\n",
      "Batch 5900: loss=0.960029, elapsed=238.24s, remaining=278.66s.\n",
      "Batch 5900: loss=0.960029, elapsed=238.24s, remaining=278.66s.\n",
      "Batch 6000: loss=0.960036, elapsed=242.15s, remaining=274.47s.\n",
      "Batch 6000: loss=0.960036, elapsed=242.15s, remaining=274.47s.\n",
      "Batch 6100: loss=0.959742, elapsed=246.25s, remaining=270.52s.\n",
      "Batch 6100: loss=0.959742, elapsed=246.25s, remaining=270.52s.\n",
      "Batch 6200: loss=0.958762, elapsed=250.15s, remaining=266.32s.\n",
      "Batch 6200: loss=0.958762, elapsed=250.15s, remaining=266.32s.\n",
      "Batch 6300: loss=0.958304, elapsed=254.18s, remaining=262.28s.\n",
      "Batch 6300: loss=0.958304, elapsed=254.18s, remaining=262.28s.\n",
      "Batch 6400: loss=0.957863, elapsed=258.23s, remaining=258.26s.\n",
      "Batch 6400: loss=0.957863, elapsed=258.23s, remaining=258.26s.\n",
      "Batch 6500: loss=0.958761, elapsed=262.29s, remaining=254.27s.\n",
      "Batch 6500: loss=0.958761, elapsed=262.29s, remaining=254.27s.\n",
      "Batch 6600: loss=0.958962, elapsed=266.16s, remaining=250.07s.\n",
      "Batch 6600: loss=0.958962, elapsed=266.16s, remaining=250.07s.\n",
      "Batch 6700: loss=0.958414, elapsed=270.19s, remaining=246.03s.\n",
      "Batch 6700: loss=0.958414, elapsed=270.19s, remaining=246.03s.\n",
      "Batch 6800: loss=0.958283, elapsed=274.27s, remaining=242.05s.\n",
      "Batch 6800: loss=0.958283, elapsed=274.27s, remaining=242.05s.\n",
      "Batch 6900: loss=0.957331, elapsed=278.31s, remaining=238.03s.\n",
      "Batch 6900: loss=0.957331, elapsed=278.31s, remaining=238.03s.\n",
      "Batch 7000: loss=0.956223, elapsed=282.20s, remaining=233.87s.\n",
      "Batch 7000: loss=0.956223, elapsed=282.20s, remaining=233.87s.\n",
      "Batch 7100: loss=0.956589, elapsed=286.16s, remaining=229.79s.\n",
      "Batch 7100: loss=0.956589, elapsed=286.16s, remaining=229.79s.\n",
      "Batch 7200: loss=0.955925, elapsed=290.15s, remaining=225.73s.\n",
      "Batch 7200: loss=0.955925, elapsed=290.15s, remaining=225.73s.\n",
      "Batch 7300: loss=0.955145, elapsed=294.19s, remaining=221.71s.\n",
      "Batch 7300: loss=0.955145, elapsed=294.19s, remaining=221.71s.\n",
      "Batch 7400: loss=0.955323, elapsed=298.33s, remaining=217.76s.\n",
      "Batch 7400: loss=0.955323, elapsed=298.33s, remaining=217.76s.\n",
      "Batch 7500: loss=0.955272, elapsed=302.38s, remaining=213.74s.\n",
      "Batch 7500: loss=0.955272, elapsed=302.38s, remaining=213.74s.\n",
      "Batch 7600: loss=0.954132, elapsed=306.41s, remaining=209.71s.\n",
      "Batch 7600: loss=0.954132, elapsed=306.41s, remaining=209.71s.\n",
      "Batch 7700: loss=0.953561, elapsed=310.51s, remaining=205.73s.\n",
      "Batch 7700: loss=0.953561, elapsed=310.51s, remaining=205.73s.\n",
      "Batch 7800: loss=0.952793, elapsed=314.56s, remaining=201.71s.\n",
      "Batch 7800: loss=0.952793, elapsed=314.56s, remaining=201.71s.\n",
      "Batch 7900: loss=0.953465, elapsed=318.61s, remaining=197.69s.\n",
      "Batch 7900: loss=0.953465, elapsed=318.61s, remaining=197.69s.\n",
      "Batch 8000: loss=0.953054, elapsed=322.49s, remaining=193.56s.\n",
      "Batch 8000: loss=0.953054, elapsed=322.49s, remaining=193.56s.\n",
      "Batch 8100: loss=0.953016, elapsed=326.52s, remaining=189.53s.\n",
      "Batch 8100: loss=0.953016, elapsed=326.52s, remaining=189.53s.\n",
      "Batch 8200: loss=0.953020, elapsed=330.58s, remaining=185.51s.\n",
      "Batch 8200: loss=0.953020, elapsed=330.58s, remaining=185.51s.\n",
      "Batch 8300: loss=0.953022, elapsed=334.66s, remaining=181.51s.\n",
      "Batch 8300: loss=0.953022, elapsed=334.66s, remaining=181.51s.\n",
      "Batch 8400: loss=0.952278, elapsed=338.82s, remaining=177.55s.\n",
      "Batch 8400: loss=0.952278, elapsed=338.82s, remaining=177.55s.\n",
      "Batch 8500: loss=0.952355, elapsed=342.90s, remaining=173.55s.\n",
      "Batch 8500: loss=0.952355, elapsed=342.90s, remaining=173.55s.\n",
      "Batch 8600: loss=0.952452, elapsed=346.97s, remaining=169.53s.\n",
      "Batch 8600: loss=0.952452, elapsed=346.97s, remaining=169.53s.\n",
      "Batch 8700: loss=0.952441, elapsed=351.05s, remaining=165.52s.\n",
      "Batch 8700: loss=0.952441, elapsed=351.05s, remaining=165.52s.\n",
      "Batch 8800: loss=0.952357, elapsed=355.11s, remaining=161.50s.\n",
      "Batch 8800: loss=0.952357, elapsed=355.11s, remaining=161.50s.\n",
      "Batch 8900: loss=0.951729, elapsed=359.20s, remaining=157.49s.\n",
      "Batch 8900: loss=0.951729, elapsed=359.20s, remaining=157.49s.\n",
      "Batch 9000: loss=0.951271, elapsed=363.23s, remaining=153.45s.\n",
      "Batch 9000: loss=0.951271, elapsed=363.23s, remaining=153.45s.\n",
      "Batch 9100: loss=0.950910, elapsed=367.24s, remaining=149.41s.\n",
      "Batch 9100: loss=0.950910, elapsed=367.24s, remaining=149.41s.\n",
      "Batch 9200: loss=0.951046, elapsed=371.32s, remaining=145.39s.\n",
      "Batch 9200: loss=0.951046, elapsed=371.32s, remaining=145.39s.\n",
      "Batch 9300: loss=0.950931, elapsed=375.31s, remaining=141.34s.\n",
      "Batch 9300: loss=0.950931, elapsed=375.31s, remaining=141.34s.\n",
      "Batch 9400: loss=0.950814, elapsed=379.36s, remaining=137.31s.\n",
      "Batch 9400: loss=0.950814, elapsed=379.36s, remaining=137.31s.\n",
      "Batch 9500: loss=0.951075, elapsed=383.46s, remaining=133.30s.\n",
      "Batch 9500: loss=0.951075, elapsed=383.46s, remaining=133.30s.\n",
      "Batch 9600: loss=0.950717, elapsed=387.52s, remaining=129.27s.\n",
      "Batch 9600: loss=0.950717, elapsed=387.52s, remaining=129.27s.\n",
      "Batch 9700: loss=0.951044, elapsed=391.70s, remaining=125.28s.\n",
      "Batch 9700: loss=0.951044, elapsed=391.70s, remaining=125.28s.\n",
      "Batch 9800: loss=0.950515, elapsed=395.83s, remaining=121.28s.\n",
      "Batch 9800: loss=0.950515, elapsed=395.83s, remaining=121.28s.\n",
      "Batch 9900: loss=0.950127, elapsed=399.79s, remaining=117.21s.\n",
      "Batch 9900: loss=0.950127, elapsed=399.79s, remaining=117.21s.\n",
      "Batch 10000: loss=0.950743, elapsed=404.01s, remaining=113.23s.\n",
      "Batch 10000: loss=0.950743, elapsed=404.01s, remaining=113.23s.\n",
      "Batch 10100: loss=0.950525, elapsed=407.96s, remaining=109.17s.\n",
      "Batch 10100: loss=0.950525, elapsed=407.96s, remaining=109.17s.\n",
      "Batch 10200: loss=0.950502, elapsed=411.96s, remaining=105.12s.\n",
      "Batch 10200: loss=0.950502, elapsed=411.96s, remaining=105.12s.\n",
      "Batch 10300: loss=0.948947, elapsed=416.00s, remaining=101.08s.\n",
      "Batch 10300: loss=0.948947, elapsed=416.00s, remaining=101.08s.\n",
      "Batch 10400: loss=0.948500, elapsed=420.10s, remaining=97.05s.\n",
      "Batch 10400: loss=0.948500, elapsed=420.10s, remaining=97.05s.\n",
      "Batch 10500: loss=0.948561, elapsed=424.05s, remaining=92.99s.\n",
      "Batch 10500: loss=0.948561, elapsed=424.05s, remaining=92.99s.\n",
      "Batch 10600: loss=0.948827, elapsed=428.11s, remaining=88.96s.\n",
      "Batch 10600: loss=0.948827, elapsed=428.11s, remaining=88.96s.\n",
      "Batch 10700: loss=0.948359, elapsed=432.25s, remaining=84.94s.\n",
      "Batch 10700: loss=0.948359, elapsed=432.25s, remaining=84.94s.\n",
      "Batch 10800: loss=0.948152, elapsed=436.38s, remaining=80.92s.\n",
      "Batch 10800: loss=0.948152, elapsed=436.38s, remaining=80.92s.\n",
      "Batch 10900: loss=0.947810, elapsed=440.41s, remaining=76.88s.\n",
      "Batch 10900: loss=0.947810, elapsed=440.41s, remaining=76.88s.\n",
      "Batch 11000: loss=0.947861, elapsed=444.47s, remaining=72.85s.\n",
      "Batch 11000: loss=0.947861, elapsed=444.47s, remaining=72.85s.\n",
      "Batch 11100: loss=0.947696, elapsed=448.57s, remaining=68.82s.\n",
      "Batch 11100: loss=0.947696, elapsed=448.57s, remaining=68.82s.\n",
      "Batch 11200: loss=0.947263, elapsed=452.61s, remaining=64.77s.\n",
      "Batch 11200: loss=0.947263, elapsed=452.61s, remaining=64.77s.\n",
      "Batch 11300: loss=0.947009, elapsed=456.76s, remaining=60.75s.\n",
      "Batch 11300: loss=0.947009, elapsed=456.76s, remaining=60.75s.\n",
      "Batch 11400: loss=0.947193, elapsed=460.69s, remaining=56.69s.\n",
      "Batch 11400: loss=0.947193, elapsed=460.69s, remaining=56.69s.\n",
      "Batch 11500: loss=0.946746, elapsed=464.79s, remaining=52.66s.\n",
      "Batch 11500: loss=0.946746, elapsed=464.79s, remaining=52.66s.\n",
      "Batch 11600: loss=0.946753, elapsed=468.97s, remaining=48.63s.\n",
      "Batch 11600: loss=0.946753, elapsed=468.97s, remaining=48.63s.\n",
      "Batch 11700: loss=0.947294, elapsed=473.06s, remaining=44.60s.\n",
      "Batch 11700: loss=0.947294, elapsed=473.06s, remaining=44.60s.\n",
      "Batch 11800: loss=0.947051, elapsed=477.17s, remaining=40.56s.\n",
      "Batch 11800: loss=0.947051, elapsed=477.17s, remaining=40.56s.\n",
      "Batch 11900: loss=0.946720, elapsed=481.18s, remaining=36.51s.\n",
      "Batch 11900: loss=0.946720, elapsed=481.18s, remaining=36.51s.\n",
      "Batch 12000: loss=0.946059, elapsed=485.28s, remaining=32.48s.\n",
      "Batch 12000: loss=0.946059, elapsed=485.28s, remaining=32.48s.\n",
      "Batch 12100: loss=0.945742, elapsed=489.27s, remaining=28.43s.\n",
      "Batch 12100: loss=0.945742, elapsed=489.27s, remaining=28.43s.\n",
      "Batch 12200: loss=0.945566, elapsed=493.27s, remaining=24.38s.\n",
      "Batch 12200: loss=0.945566, elapsed=493.27s, remaining=24.38s.\n",
      "Batch 12300: loss=0.945113, elapsed=497.30s, remaining=20.34s.\n",
      "Batch 12300: loss=0.945113, elapsed=497.30s, remaining=20.34s.\n",
      "Batch 12400: loss=0.944781, elapsed=501.28s, remaining=16.29s.\n",
      "Batch 12400: loss=0.944781, elapsed=501.28s, remaining=16.29s.\n",
      "Batch 12500: loss=0.944171, elapsed=505.38s, remaining=12.25s.\n",
      "Batch 12500: loss=0.944171, elapsed=505.38s, remaining=12.25s.\n",
      "Batch 12600: loss=0.944397, elapsed=509.51s, remaining=8.21s.\n",
      "Batch 12600: loss=0.944397, elapsed=509.51s, remaining=8.21s.\n",
      "Batch 12700: loss=0.944308, elapsed=513.56s, remaining=4.17s.\n",
      "Batch 12700: loss=0.944308, elapsed=513.56s, remaining=4.17s.\n",
      "Batch 12800: loss=0.944277, elapsed=517.63s, remaining=0.12s.\n",
      "Batch 12800: loss=0.944277, elapsed=517.63s, remaining=0.12s.\n",
      "Batch 12900: loss=0.943816, elapsed=521.70s, remaining=-3.92s.\n",
      "Batch 12900: loss=0.943816, elapsed=521.70s, remaining=-3.92s.\n",
      "Batch 13000: loss=0.944096, elapsed=525.74s, remaining=-7.96s.\n",
      "Batch 13000: loss=0.944096, elapsed=525.74s, remaining=-7.96s.\n",
      "Batch 13100: loss=0.944069, elapsed=529.76s, remaining=-12.01s.\n",
      "Batch 13100: loss=0.944069, elapsed=529.76s, remaining=-12.01s.\n",
      "Batch 13200: loss=0.943209, elapsed=533.86s, remaining=-16.05s.\n",
      "Batch 13200: loss=0.943209, elapsed=533.86s, remaining=-16.05s.\n",
      "\n",
      "Training epoch took: 537.26s\n",
      "\n",
      "Training epoch took: 537.26s\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.933338 \n",
      "\n",
      "Accuracy - Sentiment: 83.2%, Avg loss: 0.933338 \n",
      "\n",
      "Accuracy - Topic: 82.5%, Avg loss: 0.933338 \n",
      "\n",
      "Epoch: 4/5\n",
      "\n",
      "Training...\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.933338 \n",
      "\n",
      "Accuracy - Sentiment: 83.2%, Avg loss: 0.933338 \n",
      "\n",
      "Accuracy - Topic: 82.5%, Avg loss: 0.933338 \n",
      "\n",
      "Epoch: 4/5\n",
      "\n",
      "Training...\n",
      "Batch 100: loss=0.935107, elapsed=4.13s, remaining=518.52s.\n",
      "Batch 100: loss=0.935107, elapsed=4.13s, remaining=518.52s.\n",
      "Batch 200: loss=0.917064, elapsed=8.10s, remaining=507.64s.\n",
      "Batch 200: loss=0.917064, elapsed=8.10s, remaining=507.64s.\n",
      "Batch 300: loss=0.923559, elapsed=12.20s, remaining=506.71s.\n",
      "Batch 300: loss=0.923559, elapsed=12.20s, remaining=506.71s.\n",
      "Batch 400: loss=0.904793, elapsed=16.20s, remaining=500.94s.\n",
      "Batch 400: loss=0.904793, elapsed=16.20s, remaining=500.94s.\n",
      "Batch 500: loss=0.903417, elapsed=20.20s, remaining=495.99s.\n",
      "Batch 500: loss=0.903417, elapsed=20.20s, remaining=495.99s.\n",
      "Batch 600: loss=0.899966, elapsed=24.26s, remaining=492.43s.\n",
      "Batch 600: loss=0.899966, elapsed=24.26s, remaining=492.43s.\n",
      "Batch 700: loss=0.896538, elapsed=28.22s, remaining=487.11s.\n",
      "Batch 700: loss=0.896538, elapsed=28.22s, remaining=487.11s.\n",
      "Batch 800: loss=0.896123, elapsed=32.34s, remaining=484.47s.\n",
      "Batch 800: loss=0.896123, elapsed=32.34s, remaining=484.47s.\n",
      "Batch 900: loss=0.894683, elapsed=36.35s, remaining=480.04s.\n",
      "Batch 900: loss=0.894683, elapsed=36.35s, remaining=480.04s.\n",
      "Batch 1000: loss=0.891752, elapsed=40.37s, remaining=475.95s.\n",
      "Batch 1000: loss=0.891752, elapsed=40.37s, remaining=475.95s.\n",
      "Batch 1100: loss=0.894898, elapsed=44.36s, remaining=471.40s.\n",
      "Batch 1100: loss=0.894898, elapsed=44.36s, remaining=471.40s.\n",
      "Batch 1200: loss=0.892834, elapsed=48.31s, remaining=466.58s.\n",
      "Batch 1200: loss=0.892834, elapsed=48.31s, remaining=466.58s.\n",
      "Batch 1300: loss=0.895355, elapsed=52.22s, remaining=461.58s.\n",
      "Batch 1300: loss=0.895355, elapsed=52.22s, remaining=461.58s.\n",
      "Batch 1400: loss=0.896871, elapsed=56.38s, remaining=458.79s.\n",
      "Batch 1400: loss=0.896871, elapsed=56.38s, remaining=458.79s.\n",
      "Batch 1500: loss=0.897707, elapsed=60.48s, remaining=455.31s.\n",
      "Batch 1500: loss=0.897707, elapsed=60.48s, remaining=455.31s.\n",
      "Batch 1600: loss=0.897701, elapsed=64.49s, remaining=451.17s.\n",
      "Batch 1600: loss=0.897701, elapsed=64.49s, remaining=451.17s.\n",
      "Batch 1700: loss=0.896337, elapsed=68.57s, remaining=447.50s.\n",
      "Batch 1700: loss=0.896337, elapsed=68.57s, remaining=447.50s.\n",
      "Batch 1800: loss=0.894047, elapsed=72.58s, remaining=443.35s.\n",
      "Batch 1800: loss=0.894047, elapsed=72.58s, remaining=443.35s.\n",
      "Batch 1900: loss=0.893515, elapsed=76.62s, remaining=439.40s.\n",
      "Batch 1900: loss=0.893515, elapsed=76.62s, remaining=439.40s.\n",
      "Batch 2000: loss=0.890988, elapsed=80.52s, remaining=434.65s.\n",
      "Batch 2000: loss=0.890988, elapsed=80.52s, remaining=434.65s.\n",
      "Batch 2100: loss=0.892990, elapsed=84.52s, remaining=430.51s.\n",
      "Batch 2100: loss=0.892990, elapsed=84.52s, remaining=430.51s.\n",
      "Batch 2200: loss=0.893649, elapsed=88.57s, remaining=426.63s.\n",
      "Batch 2200: loss=0.893649, elapsed=88.57s, remaining=426.63s.\n",
      "Batch 2300: loss=0.895477, elapsed=92.65s, remaining=422.86s.\n",
      "Batch 2300: loss=0.895477, elapsed=92.65s, remaining=422.86s.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEach model will be trained for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m epochs\u001b[39m\u001b[33m\"\u001b[39m.format(ENSEMBLE_CONFIG[\u001b[33m'\u001b[39m\u001b[33mn_epochs\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis approach provides much better diversity than shuffling the same split\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m ensemble_info_splits = \u001b[43mtrain_deep_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mENSEMBLE_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mENSEMBLE_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentiment_var\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic_var\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mENSEMBLE_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msave_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_splits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Different directory\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mENSEMBLE_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_prefix\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_splits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43morg_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed_val\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnsemble training with different splits completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal models trained: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ensemble_info_splits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ContextScale/utils/uncertainty.py:415\u001b[39m, in \u001b[36mtrain_deep_ensemble\u001b[39m\u001b[34m(model_factory, data_splits, tokenizer, data_collator, device, batch_size, n_epochs, lr, sentiment_var, topic_var, save_dir, model_prefix, org_seed)\u001b[39m\n\u001b[32m    412\u001b[39m save_path = os.path.join(save_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Train this ensemble member\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m info = \u001b[43mtrain_ensemble_member\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentiment_var\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentiment_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic_var\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopic_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreshuffle_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No need to shuffle since we have different splits\u001b[39;49;00m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43morg_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43morg_seed\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Add split information\u001b[39;00m\n\u001b[32m    431\u001b[39m info[\u001b[33m'\u001b[39m\u001b[33mdata_split_seed\u001b[39m\u001b[33m'\u001b[39m] = split_data[\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ContextScale/utils/uncertainty.py:279\u001b[39m, in \u001b[36mtrain_ensemble_member\u001b[39m\u001b[34m(model, train_dataloader, eval_dataloader, device, n_epochs, lr, sentiment_var, topic_var, model_id, save_path, reshuffle_dataloader, org_seed)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m timing_log = \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactual_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion_sent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion_topic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentiment_var\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentiment_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic_var\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopic_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtiming_log\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    290\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    293\u001b[39m eval_loop(\n\u001b[32m    294\u001b[39m     eval_dataloader, \n\u001b[32m    295\u001b[39m     model, \n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     topic_var=topic_var\n\u001b[32m    301\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ContextScale/utils/functions.py:495\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(dataloader, model, optimizer, scheduler, device, criterion_sent, criterion_topic, sentiment_var, topic_var, timing_log)\u001b[39m\n\u001b[32m    492\u001b[39m     train_loss += loss.item()\n\u001b[32m    494\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m scaler.unscale_(optimizer)\n\u001b[32m    497\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/y/envs/main-dev/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/y/envs/main-dev/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/y/envs/main-dev/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the deep ensemble with different data splits\n",
    "print(\"Starting Deep Ensemble Training with Different Data Splits\")\n",
    "print(\"This will train {} models using completely different train/eval/test splits\".format(len(data_splits)))\n",
    "print(\"Each model will be trained for {} epochs\".format(ENSEMBLE_CONFIG['n_epochs']))\n",
    "print(\"This approach provides much better diversity than shuffling the same split\")\n",
    "\n",
    "ensemble_info_splits = train_deep_ensemble(\n",
    "    model_factory=create_model,\n",
    "    data_splits=data_splits,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    device=device,\n",
    "    batch_size=16,\n",
    "    n_epochs=ENSEMBLE_CONFIG['n_epochs'],\n",
    "    lr=ENSEMBLE_CONFIG['lr'],\n",
    "    sentiment_var='sentiment',\n",
    "    topic_var='topic',\n",
    "    save_dir=ENSEMBLE_CONFIG['save_dir'] + '_splits',  # Different directory\n",
    "    model_prefix=ENSEMBLE_CONFIG['model_prefix'] + '_splits',\n",
    "    org_seed=seed_val\n",
    ")\n",
    "\n",
    "print(\"Ensemble training with different splits completed!\")\n",
    "print(f\"Total models trained: {len(ensemble_info_splits)}\")\n",
    "print(\"Each model was trained with:\")\n",
    "print(\"  - Different random seed (42 + model_id)\")\n",
    "print(\"  - Completely different train/eval/test data split\") \n",
    "print(\"  - Same model architecture and hyperparameters\")\n",
    "for i, info in enumerate(ensemble_info_splits):\n",
    "    total_time = sum(info['train_times'])\n",
    "    print(f\"Model {i}: Total training time = {total_time:.1f}s, Split seed = {info['data_split_seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Inference with Different Splits\n",
    "\n",
    "Let's perform inference using the ensemble trained with different data splits and compare with the regular ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate checkpoint paths for the ensemble models trained with different splits\n",
    "checkpoint_paths_splits = [\n",
    "    os.path.join(ENSEMBLE_CONFIG['save_dir'] + '_splits', f\"{ENSEMBLE_CONFIG['model_prefix']}_splits_{i}.safetensors\")\n",
    "    for i in range(len(data_splits))\n",
    "]\n",
    "\n",
    "print(\"Checkpoint paths for split-based ensemble:\")\n",
    "for i, path in enumerate(checkpoint_paths_splits):\n",
    "    print(f\"  Model {i}: {path}\")\n",
    "\n",
    "# Load the ensemble models\n",
    "models_splits = load_ensemble_models(\n",
    "    model_factory=create_model,\n",
    "    checkpoint_paths=checkpoint_paths_splits,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded {len(models_splits)} ensemble models trained with different splits\")\n",
    "\n",
    "# Create test dataloader using the first split's test set (or use a common test set)\n",
    "# For fair comparison, let's use the original test split\n",
    "test_tokenized = train_test['test'].map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=['text', 'topic_sentiment']\n",
    ")\n",
    "\n",
    "test_dataloader_splits = DataLoader(\n",
    "    test_tokenized,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Test set size: {len(test_tokenized)}\")\n",
    "\n",
    "# Perform ensemble inference\n",
    "ensemble_results_splits = ensemble_inference(\n",
    "    models=models_splits,\n",
    "    dataloader=test_dataloader_splits,\n",
    "    device=device,\n",
    "    beta=ENSEMBLE_CONFIG['beta'],\n",
    "    topic_label='topic',\n",
    "    sentiment_label='sentiment',\n",
    "    use_ground_truth_topic=False,\n",
    "    timing_log=True\n",
    ")\n",
    "\n",
    "print(\"Ensemble inference with different splits completed!\")\n",
    "print(f\"Results shape: {ensemble_results_splits['mean_position_scores'].shape}\")\n",
    "print(f\"Number of models used: {ensemble_results_splits['num_models']}\")\n",
    "print(f\"Beta parameter: {ensemble_results_splits['beta']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Different Ensemble Approaches\n",
    "\n",
    "Let's compare the uncertainty estimates from the two different ensemble approaches:\n",
    "1. **Same Split + Different Shuffles**: Traditional approach using same train/eval/test split with different random shuffles\n",
    "2. **Different Splits**: New approach using completely different train/eval/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrames for both ensemble approaches\n",
    "uncertainty_df_splits = create_ensemble_summary_dataframe(ensemble_results_splits)\n",
    "\n",
    "print(\"COMPARISON: Ensemble with Different Data Splits vs Same Split + Different Shuffles\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For fair comparison, we need to train the traditional ensemble too\n",
    "# Let's assume ensemble_results from the original approach exists\n",
    "# If not, you would need to run the original ensemble training first\n",
    "\n",
    "print(\"\\n1. DIFFERENT DATA SPLITS APPROACH:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Position Score Statistics (Mean of {ensemble_results_splits['num_models']} models):\")\n",
    "print(f\"  Mean: {uncertainty_df_splits['mean_position_score'].mean():.4f}\")\n",
    "print(f\"  Std:  {uncertainty_df_splits['mean_position_score'].std():.4f}\")\n",
    "print(f\"  Range: [{uncertainty_df_splits['mean_position_score'].min():.4f}, {uncertainty_df_splits['mean_position_score'].max():.4f}]\")\n",
    "\n",
    "print(f\"\\nEpistemic Uncertainty (model disagreement):\")\n",
    "print(f\"  Mean: {uncertainty_df_splits['epistemic_variance'].mean():.6f}\")\n",
    "print(f\"  Std:  {uncertainty_df_splits['epistemic_variance'].std():.6f}\")\n",
    "print(f\"  Range: [{uncertainty_df_splits['epistemic_variance'].min():.6f}, {uncertainty_df_splits['epistemic_variance'].max():.6f}]\")\n",
    "\n",
    "print(f\"\\nAleatoric Uncertainty (inherent data uncertainty):\")\n",
    "print(f\"  Mean: {uncertainty_df_splits['aleatoric_variance'].mean():.6f}\")\n",
    "print(f\"  Std:  {uncertainty_df_splits['aleatoric_variance'].std():.6f}\")\n",
    "print(f\"  Range: [{uncertainty_df_splits['aleatoric_variance'].min():.6f}, {uncertainty_df_splits['aleatoric_variance'].max():.6f}]\")\n",
    "\n",
    "print(f\"\\nTotal Uncertainty (epistemic + aleatoric):\")\n",
    "print(f\"  Mean: {uncertainty_df_splits['total_variance'].mean():.6f}\")\n",
    "print(f\"  Std:  {uncertainty_df_splits['total_variance'].std():.6f}\")\n",
    "print(f\"  Range: [{uncertainty_df_splits['total_variance'].min():.6f}, {uncertainty_df_splits['total_variance'].max():.6f}]\")\n",
    "\n",
    "# Add a column to distinguish the approach\n",
    "uncertainty_df_splits['ensemble_approach'] = 'Different_Splits'\n",
    "\n",
    "print(f\"\\nFirst 5 rows of uncertainty analysis (Different Splits):\")\n",
    "print(uncertainty_df_splits[['mean_position_score', 'epistemic_variance', 'aleatoric_variance', 'total_variance']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for both approaches\n",
    "save_ensemble_results(\n",
    "    results=ensemble_results_splits,\n",
    "    save_path='results/ensemble_results_different_splits.pkl',\n",
    "    include_individual_predictions=True\n",
    ")\n",
    "\n",
    "# Create a comparison plot if we have both approaches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Epistemic Uncertainty Distribution\n",
    "axes[0, 0].hist(uncertainty_df_splits['epistemic_variance'], bins=50, alpha=0.7, \n",
    "                label='Different Splits', color='blue')\n",
    "axes[0, 0].set_xlabel('Epistemic Variance')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Epistemic Uncertainty Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Aleatoric Uncertainty Distribution  \n",
    "axes[0, 1].hist(uncertainty_df_splits['aleatoric_variance'], bins=50, alpha=0.7,\n",
    "                label='Different Splits', color='green')\n",
    "axes[0, 1].set_xlabel('Aleatoric Variance')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Aleatoric Uncertainty Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Total Uncertainty Distribution\n",
    "axes[1, 0].hist(uncertainty_df_splits['total_variance'], bins=50, alpha=0.7,\n",
    "                label='Different Splits', color='red')\n",
    "axes[1, 0].set_xlabel('Total Variance')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Total Uncertainty Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 4: Position Score vs Total Uncertainty\n",
    "scatter = axes[1, 1].scatter(uncertainty_df_splits['mean_position_score'], \n",
    "                           uncertainty_df_splits['total_variance'],\n",
    "                           alpha=0.5, s=1)\n",
    "axes[1, 1].set_xlabel('Mean Position Score')\n",
    "axes[1, 1].set_ylabel('Total Variance')\n",
    "axes[1, 1].set_title('Position Score vs Total Uncertainty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Advantages of Different Splits Approach:\")\n",
    "print(\"1. Better captures epistemic uncertainty due to true data diversity\")\n",
    "print(\"2. More robust position estimates across different data samples\")\n",
    "print(\"3. Better generalization to unseen data\")\n",
    "print(\"4. More realistic uncertainty estimates for real-world deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Improved Ensemble Approach\n",
    "\n",
    "### What Was Changed\n",
    "\n",
    "**Previous Approach (Traditional):**\n",
    "- Used the same train/eval/test split (from `seed_val`)\n",
    "- Created ensemble diversity through different random seeds and data shuffling \n",
    "- Each model saw the same underlying data samples, just in different orders\n",
    "\n",
    "**New Approach (Different Splits):**\n",
    "- Creates 5 completely different train/eval/test splits from the original dataset\n",
    "- Each split uses a different seed (base_seed + split_id * 1000)\n",
    "- Each model is trained on fundamentally different data subsets\n",
    "- Maintains stratification to ensure balanced topic/sentiment distributions\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Better Epistemic Uncertainty**: Different data splits capture true model uncertainty about which samples are informative\n",
    "2. **More Robust Position Estimates**: Position scores are averaged across different training data subsets\n",
    "3. **Improved Generalization**: Ensemble members have seen different aspects of the data distribution\n",
    "4. **Realistic Uncertainty Quantification**: Better reflects real-world deployment scenarios\n",
    "\n",
    "### Usage\n",
    "\n",
    "To use the new approach instead of the traditional one:\n",
    "\n",
    "```python\n",
    "# Create multiple splits\n",
    "data_splits = create_multiple_data_splits(\n",
    "    original_dataset=manifesto_dataset,\n",
    "    num_splits=5,\n",
    "    test_size=0.1,\n",
    "    eval_size=0.3,\n",
    "    stratify_column='topic_sentiment',\n",
    "    base_seed=seed_val\n",
    ")\n",
    "\n",
    "# Train ensemble with different splits\n",
    "ensemble_info = train_deep_ensemble_with_different_splits(\n",
    "    model_factory=create_model,\n",
    "    data_splits=data_splits,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    device=device,\n",
    "    # ... other parameters\n",
    ")\n",
    "```\n",
    "\n",
    "This approach provides significantly better ensemble diversity and more reliable uncertainty estimates for political position scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Inference with Uncertainty Estimation\n",
    "\n",
    "Now let's load the trained ensemble models and perform inference with uncertainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate checkpoint paths for the ensemble models\n",
    "checkpoint_paths = [\n",
    "    os.path.join(ENSEMBLE_CONFIG['save_dir'], f\"{ENSEMBLE_CONFIG['model_prefix']}_{i}.safetensors\")\n",
    "    for i in range(ENSEMBLE_CONFIG['num_models'])\n",
    "]\n",
    "\n",
    "print(\"Checkpoint paths:\")\n",
    "for i, path in enumerate(checkpoint_paths):\n",
    "    print(f\"  Model {i}: {path}\")\n",
    "\n",
    "# Load the ensemble models\n",
    "print(\"\\nLoading ensemble models...\")\n",
    "ensemble_models = load_ensemble_models(\n",
    "    model_factory=create_model,\n",
    "    checkpoint_paths=checkpoint_paths,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {len(ensemble_models)} ensemble models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ensemble inference with uncertainty estimation\n",
    "print(\"Performing ensemble inference with uncertainty estimation...\")\n",
    "print(f\"Using beta = {ENSEMBLE_CONFIG['beta']} for exponential position score computation\")\n",
    "print(\"The ensemble will compute:\")\n",
    "print(\"  - Mean position scores across all 5 models\")\n",
    "print(\"  - Position score variance for each text sequence\")\n",
    "print(\"  - Epistemic uncertainty (model disagreement)\")\n",
    "print(\"  - Aleatoric uncertainty (inherent data uncertainty)\")\n",
    "\n",
    "ensemble_results = ensemble_inference(\n",
    "    models=ensemble_models,\n",
    "    dataloader=test_dataloader,\n",
    "    device=device,\n",
    "    beta=ENSEMBLE_CONFIG['beta'],\n",
    "    topic_label='topic',\n",
    "    sentiment_label='sentiment',\n",
    "    use_ground_truth_topic=True,\n",
    "    timing_log=True\n",
    ")\n",
    "\n",
    "print(f\"\\nEnsemble inference completed!\")\n",
    "print(f\"Final position scores are the mean of {len(ensemble_models)} models\")\n",
    "print(f\"Position score variance included for each sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame with uncertainty estimates\n",
    "uncertainty_df = create_ensemble_summary_dataframe(ensemble_results)\n",
    "\n",
    "print(\"Ensemble Deep Learning with Different Data Shuffles - Uncertainty Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Position Score Statistics (Mean of {ensemble_results['num_models']} models):\")\n",
    "print(f\"  Mean: {uncertainty_df['mean_position_score'].mean():.4f}\")\n",
    "print(f\"  Std:  {uncertainty_df['mean_position_score'].std():.4f}\")\n",
    "print(f\"  Range: [{uncertainty_df['mean_position_score'].min():.4f}, {uncertainty_df['mean_position_score'].max():.4f}]\")\n",
    "\n",
    "print(f\"\\nPosition Score Variance (per sequence across models):\")\n",
    "print(f\"  Mean: {uncertainty_df['position_score_variance'].mean():.6f}\")\n",
    "print(f\"  Std:  {uncertainty_df['position_score_variance'].std():.6f}\")\n",
    "print(f\"  Range: [{uncertainty_df['position_score_variance'].min():.6f}, {uncertainty_df['position_score_variance'].max():.6f}]\")\n",
    "\n",
    "print(f\"\\nEpistemic Uncertainty (model disagreement):\")\n",
    "print(f\"  Mean: {uncertainty_df['epistemic_variance'].mean():.6f}\")\n",
    "print(f\"  Std:  {uncertainty_df['epistemic_variance'].std():.6f}\")\n",
    "print(f\"  Range: [{uncertainty_df['epistemic_variance'].min():.6f}, {uncertainty_df['epistemic_variance'].max():.6f}]\")\n",
    "\n",
    "print(f\"\\nAleatoric Uncertainty (inherent data uncertainty):\")\n",
    "print(f\"  Mean: {uncertainty_df['aleatoric_variance'].mean():.6f}\")\n",
    "print(f\"  Std:  {uncertainty_df['aleatoric_variance'].std():.6f}\")\n",
    "print(f\"  Range: [{uncertainty_df['aleatoric_variance'].min():.6f}, {uncertainty_df['aleatoric_variance'].max():.6f}]\")\n",
    "\n",
    "print(f\"\\nTotal Uncertainty (epistemic + aleatoric):\")\n",
    "print(f\"  Mean: {uncertainty_df['total_variance'].mean():.6f}\")\n",
    "print(f\"  Std:  {uncertainty_df['total_variance'].std():.6f}\")\n",
    "print(f\"  Range: [{uncertainty_df['total_variance'].min():.6f}, {uncertainty_df['total_variance'].max():.6f}]\")\n",
    "\n",
    "# Show which columns are available\n",
    "print(f\"\\nDataFrame columns: {list(uncertainty_df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 10 rows of uncertainty analysis:\")\n",
    "print(uncertainty_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the impact of different data shuffles\n",
    "print(\"Impact of Different Data Shuffles and Seeds:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show variance in individual model predictions\n",
    "individual_scores = ensemble_results['individual_position_scores']\n",
    "print(f\"Individual model predictions shape: {np.array(individual_scores).shape}\")\n",
    "print(f\"(models, sequences) = ({len(individual_scores)}, {len(individual_scores[0])})\")\n",
    "\n",
    "# Calculate some example statistics for first 10 sequences\n",
    "print(f\"\\nExample: Position scores for first 10 sequences across {len(individual_scores)} models:\")\n",
    "for seq_idx in range(min(10, len(individual_scores[0]))):\n",
    "    scores_for_seq = [individual_scores[model_idx][seq_idx] for model_idx in range(len(individual_scores))]\n",
    "    mean_score = np.mean(scores_for_seq)\n",
    "    variance = np.var(scores_for_seq)\n",
    "    print(f\"Sequence {seq_idx:2d}: Mean={mean_score:6.3f}, Variance={variance:.6f}, Scores={[f'{s:.3f}' for s in scores_for_seq]}\")\n",
    "\n",
    "# Show overall statistics\n",
    "all_individual_scores = np.array(individual_scores)\n",
    "print(f\"\\nOverall ensemble statistics:\")\n",
    "print(f\"Mean variance across sequences: {np.var(all_individual_scores, axis=0).mean():.6f}\")\n",
    "print(f\"Range of individual model means: {[f'{np.mean(model_scores):.3f}' for model_scores in individual_scores]}\")\n",
    "print(f\"This shows how different data shuffles and seeds create model diversity!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainties with improved ensemble approach\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Position scores with uncertainty bands\n",
    "axes[0, 0].scatter(range(len(uncertainty_df)), uncertainty_df['mean_position_score'], \n",
    "                  alpha=0.6, s=1)\n",
    "axes[0, 0].fill_between(range(len(uncertainty_df)), \n",
    "                       uncertainty_df['position_score_lower_95'],\n",
    "                       uncertainty_df['position_score_upper_95'],\n",
    "                       alpha=0.2, color='red', label='95% CI')\n",
    "axes[0, 0].set_title('Position Scores with 95% Confidence Intervals')\n",
    "axes[0, 0].set_xlabel('Sample Index')\n",
    "axes[0, 0].set_ylabel('Position Score')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Position Score Variance (NEW - from different data shuffles)\n",
    "axes[0, 1].scatter(range(len(uncertainty_df)), uncertainty_df['position_score_variance'], \n",
    "                  alpha=0.6, s=1, color='green')\n",
    "axes[0, 1].set_title('Position Score Variance per Sequence\\n(From Different Data Shuffles)')\n",
    "axes[0, 1].set_xlabel('Sample Index')\n",
    "axes[0, 1].set_ylabel('Position Score Variance')\n",
    "\n",
    "# Epistemic vs Aleatoric uncertainty\n",
    "axes[0, 2].scatter(uncertainty_df['epistemic_variance'], uncertainty_df['aleatoric_variance'], \n",
    "                  alpha=0.6, s=2)\n",
    "axes[0, 2].set_xlabel('Epistemic Variance (Model Uncertainty)')\n",
    "axes[0, 2].set_ylabel('Aleatoric Variance (Data Uncertainty)')\n",
    "axes[0, 2].set_title('Epistemic vs Aleatoric Uncertainty')\n",
    "\n",
    "# Distribution of all uncertainties\n",
    "axes[1, 0].hist(uncertainty_df['position_score_variance'], bins=50, alpha=0.7, label='Position Score Variance', density=True)\n",
    "axes[1, 0].hist(uncertainty_df['epistemic_variance'], bins=50, alpha=0.7, label='Epistemic', density=True)\n",
    "axes[1, 0].hist(uncertainty_df['aleatoric_variance'], bins=50, alpha=0.7, label='Aleatoric', density=True)\n",
    "axes[1, 0].set_xlabel('Variance')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title('Distribution of All Uncertainties')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Position score variance vs other uncertainties\n",
    "axes[1, 1].scatter(uncertainty_df['position_score_variance'], uncertainty_df['epistemic_variance'], \n",
    "                  alpha=0.6, s=2, color='blue')\n",
    "axes[1, 1].set_xlabel('Position Score Variance')\n",
    "axes[1, 1].set_ylabel('Epistemic Variance')\n",
    "axes[1, 1].set_title('Position Score Variance vs Epistemic Uncertainty')\n",
    "\n",
    "# Uncertainty vs Position Score magnitude\n",
    "axes[1, 2].scatter(np.abs(uncertainty_df['mean_position_score']), \n",
    "                  uncertainty_df['position_score_variance'], alpha=0.6, s=2, color='purple')\n",
    "axes[1, 2].set_xlabel('|Position Score|')\n",
    "axes[1, 2].set_ylabel('Position Score Variance')\n",
    "axes[1, 2].set_title('Position Score Variance vs Score Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Enhanced correlation analysis\n",
    "print(\"\\nEnhanced Correlation Analysis (with Position Score Variance):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Correlation between position score variance and epistemic uncertainty: {uncertainty_df['position_score_variance'].corr(uncertainty_df['epistemic_variance']):.3f}\")\n",
    "print(f\"Correlation between position score variance and aleatoric uncertainty: {uncertainty_df['position_score_variance'].corr(uncertainty_df['aleatoric_variance']):.3f}\")\n",
    "print(f\"Correlation between epistemic and aleatoric uncertainty: {uncertainty_df['epistemic_variance'].corr(uncertainty_df['aleatoric_variance']):.3f}\")\n",
    "print(f\"Correlation between |position score| and position score variance: {np.abs(uncertainty_df['mean_position_score']).corr(uncertainty_df['position_score_variance']):.3f}\")\n",
    "print(f\"Correlation between |position score| and total uncertainty: {np.abs(uncertainty_df['mean_position_score']).corr(uncertainty_df['total_variance']):.3f}\")\n",
    "print(f\"Correlation between |position score| and epistemic uncertainty: {np.abs(uncertainty_df['mean_position_score']).corr(uncertainty_df['epistemic_variance']):.3f}\")\n",
    "print(f\"Correlation between |position score| and aleatoric uncertainty: {np.abs(uncertainty_df['mean_position_score']).corr(uncertainty_df['aleatoric_variance']):.3f}\")\n",
    "\n",
    "print(f\"\\nSummary: Different data shuffles create position score variance of {uncertainty_df['position_score_variance'].mean():.6f} on average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample analysis of high and low uncertainty predictions\n",
    "print(\"Sample Analysis: High vs Low Uncertainty Predictions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by total variance to find high and low uncertainty cases\n",
    "sorted_by_uncertainty = uncertainty_df.sort_values('total_variance')\n",
    "\n",
    "print(\"\\nLOW UNCERTAINTY EXAMPLES (most confident predictions):\")\n",
    "print(\"-\" * 50)\n",
    "low_uncertainty = sorted_by_uncertainty.head(3)\n",
    "for idx, row in low_uncertainty.iterrows():\n",
    "    print(f\"\\nText: {row['text'][:100]}...\")\n",
    "    print(f\"Position Score: {row['mean_position_score']:.3f}\")\n",
    "    print(f\"Position Score Variance: {row['position_score_variance']:.6f}\")\n",
    "    print(f\"Total Uncertainty: {row['total_variance']:.6f}\")\n",
    "    print(f\"Epistemic: {row['epistemic_variance']:.6f}, Aleatoric: {row['aleatoric_variance']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HIGH UNCERTAINTY EXAMPLES (least confident predictions):\")\n",
    "print(\"-\" * 50)\n",
    "high_uncertainty = sorted_by_uncertainty.tail(3)\n",
    "for idx, row in high_uncertainty.iterrows():\n",
    "    print(f\"\\nText: {row['text'][:100]}...\")\n",
    "    print(f\"Position Score: {row['mean_position_score']:.3f}\")\n",
    "    print(f\"Position Score Variance: {row['position_score_variance']:.6f}\")\n",
    "    print(f\"Total Uncertainty: {row['total_variance']:.6f}\")\n",
    "    print(f\"Epistemic: {row['epistemic_variance']:.6f}, Aleatoric: {row['aleatoric_variance']:.6f}\")\n",
    "\n",
    "# Additional analysis by position score variance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS BY POSITION SCORE VARIANCE (from different data shuffles):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sorted_by_pos_var = uncertainty_df.sort_values('position_score_variance')\n",
    "\n",
    "print(\"\\nMOST STABLE ACROSS SHUFFLES (low position score variance):\")\n",
    "stable_preds = sorted_by_pos_var.head(2)\n",
    "for idx, row in stable_preds.iterrows():\n",
    "    print(f\"Text: {row['text'][:80]}...\")\n",
    "    print(f\"Position Score: {row['mean_position_score']:.3f} (variance: {row['position_score_variance']:.6f})\")\n",
    "\n",
    "print(\"\\nMOST VARIABLE ACROSS SHUFFLES (high position score variance):\")\n",
    "variable_preds = sorted_by_pos_var.tail(2)\n",
    "for idx, row in variable_preds.iterrows():\n",
    "    print(f\"Text: {row['text'][:80]}...\")\n",
    "    print(f\"Position Score: {row['mean_position_score']:.3f} (variance: {row['position_score_variance']:.6f})\")\n",
    "\n",
    "print(f\"\\nSummary: {len(uncertainty_df)} predictions analyzed with enhanced ensemble (5 shuffles + seeds)\")\n",
    "print(f\"Average position score variance from data shuffling: {uncertainty_df['position_score_variance'].mean():.6f}\")\n",
    "print(f\"Max position score variance: {uncertainty_df['position_score_variance'].max():.6f}\")\n",
    "print(f\"Predictions with high shuffle sensitivity (top 10%): {int(len(uncertainty_df) * 0.1)}\")\n",
    "\n",
    "# Save ensemble results and uncertainty analysis\n",
    "save_dir = 'results/ensemble_uncertainty'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save ensemble results (without individual predictions to save space)\n",
    "ensemble_results_path = os.path.join(save_dir, 'ensemble_results.pkl')\n",
    "save_ensemble_results(\n",
    "    ensemble_results, \n",
    "    ensemble_results_path, \n",
    "    include_individual_predictions=False\n",
    ")\n",
    "\n",
    "# Save uncertainty DataFrame\n",
    "uncertainty_csv_path = os.path.join(save_dir, 'uncertainty_analysis.csv')\n",
    "uncertainty_df.to_csv(uncertainty_csv_path, index=False)\n",
    "print(f\"\\nUncertainty analysis saved to: {uncertainty_csv_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(save_dir, 'ensemble_config.pkl')\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(ENSEMBLE_CONFIG, f)\n",
    "print(f\"Ensemble configuration saved to: {config_path}\")\n",
    "\n",
    "# Save ensemble training info\n",
    "training_info_path = os.path.join(save_dir, 'ensemble_training_info.pkl')\n",
    "with open(training_info_path, 'wb') as f:\n",
    "    pickle.dump(ensemble_info, f)\n",
    "print(f\"Training info saved to: {training_info_path}\")\n",
    "\n",
    "print(\"\\nAll ensemble and uncertainty results have been saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Parameter Configuration (Optional)\n",
    "\n",
    "For easy experimentation with different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Interactive widgets for parameter tuning\n",
    "# Uncomment the following code to use interactive widgets\n",
    "\n",
    "# from ipywidgets import interact, FloatSlider, IntSlider\n",
    "# \n",
    "# @interact(\n",
    "#     beta=FloatSlider(value=1.0, min=0.1, max=3.0, step=0.1, description='Beta:'),\n",
    "#     num_models=IntSlider(value=5, min=2, max=10, step=1, description='Models:')\n",
    "# )\n",
    "# def interactive_ensemble_config(beta, num_models):\n",
    "#     \"\"\"Interactive configuration for ensemble parameters.\"\"\"\n",
    "#     print(f\"Updated configuration:\")\n",
    "#     print(f\"  Beta (exponential decay): {beta}\")\n",
    "#     print(f\"  Number of ensemble models: {num_models}\")\n",
    "#     print(f\"  To apply these changes, update ENSEMBLE_CONFIG manually and re-run ensemble training\")\n",
    "#     \n",
    "#     return {'beta': beta, 'num_models': num_models}\n",
    "\n",
    "# Alternative: Command-line style argument parsing\n",
    "# You can also set parameters via environment variables or command-line arguments:\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_ensemble_config():\n",
    "    \"\"\"Get ensemble configuration from environment variables or command line.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Ensemble Configuration')\n",
    "    parser.add_argument('--beta', type=float, default=float(os.getenv('ENSEMBLE_BETA', 1.0)),\n",
    "                       help='Beta parameter for exponential position score')\n",
    "    parser.add_argument('--num_models', type=int, default=int(os.getenv('ENSEMBLE_NUM_MODELS', 5)),\n",
    "                       help='Number of ensemble models')\n",
    "    parser.add_argument('--epochs', type=int, default=int(os.getenv('ENSEMBLE_EPOCHS', 5)),\n",
    "                       help='Number of training epochs per model')\n",
    "    \n",
    "    # In Jupyter, we'll use default values\n",
    "    args = parser.parse_args(args=[])  # Empty args for Jupyter\n",
    "    \n",
    "    return {\n",
    "        'beta': args.beta,\n",
    "        'num_models': args.num_models,\n",
    "        'epochs': args.epochs\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "# export ENSEMBLE_BETA=1.5\n",
    "# export ENSEMBLE_NUM_MODELS=7\n",
    "# Then restart notebook kernel\n",
    "\n",
    "cli_config = get_ensemble_config()\n",
    "print(\"Configuration from environment/CLI:\")\n",
    "print(f\"  Beta: {cli_config['beta']}\")\n",
    "print(f\"  Num models: {cli_config['num_models']}\")\n",
    "print(f\"  Epochs: {cli_config['epochs']}\")\n",
    "\n",
    "print(\"\\nTo use these values, update ENSEMBLE_CONFIG before training:\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "main-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
